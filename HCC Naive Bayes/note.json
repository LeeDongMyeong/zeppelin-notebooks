{"paragraphs":[{"text":"%md\n\n## HCC Questions Auto-Tagging\n#### with Naive Bayes\n\n**Level**: Intermediate\n**Language**: Python\n**Requirements**: \n\n- Spark 2.x\n\n**Author**: Robert Hryniewicz\n**Follow** [@RobertH8z](https://twitter.com/RobH8z)","dateUpdated":"2016-12-12T21:25:09-0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481592848023_925922074","id":"20161123-122037_1386074437","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>HCC Questions Auto-Tagging</h2>\n<h4>with Naive Bayes</h4>\n<p><strong>Level</strong>: Intermediate\n<br  /><strong>Language</strong>: Python\n<br  /><strong>Requirements</strong>:</p>\n<ul>\n<li>Spark 2.x</li>\n</ul>\n<p><strong>Author</strong>: Robert Hryniewicz\n<br  /><strong>Follow</strong> <a href=\"https://twitter.com/RobH8z\">@RobertH8z</a></p>\n"},"dateCreated":"2016-12-12T05:34:08-0800","dateStarted":"2016-12-12T21:25:08-0800","dateFinished":"2016-12-12T21:25:08-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:514"},{"text":"%md\n\n## Part 1\n#### Download and Extract relevant data","dateUpdated":"2016-12-12T20:09:38-0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481592848024_923998329","id":"20161123-145000_634213211","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Part 1</h2>\n<h4>Download and Extract relevant data</h4>\n"},"dateCreated":"2016-12-12T05:34:08-0800","dateStarted":"2016-12-12T05:34:33-0800","dateFinished":"2016-12-12T05:34:33-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:515"},{"title":"Download dataset","text":"%sh\n\nmkdir /tmp/hcc-public-dataset\n\nwget https://raw.githubusercontent.com/roberthryniewicz/datasets/master/hcc-pulic-dataset/hcc-part0.json -O /tmp/hcc-public-dataset/part0.json\nwget https://raw.githubusercontent.com/roberthryniewicz/datasets/master/hcc-pulic-dataset/hcc-part1.json -O /tmp/hcc-public-dataset/part1.json\nwget https://raw.githubusercontent.com/roberthryniewicz/datasets/master/hcc-pulic-dataset/hcc-part2.json -O /tmp/hcc-public-dataset/part2.json\nwget https://raw.githubusercontent.com/roberthryniewicz/datasets/master/hcc-pulic-dataset/hcc-part3.json -O /tmp/hcc-public-dataset/part3.json","dateUpdated":"2016-12-12T06:07:09-0800","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481592901443_1966960653","id":"20161212-173501_848249815","result":{"code":"SUCCESS","type":"TEXT","msg":"--2016-12-12 18:07:09--  https://raw.githubusercontent.com/roberthryniewicz/datasets/master/hcc-pulic-dataset/hcc-part0.json\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.40.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.40.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7999441 (7,6M) [text/plain]\nSaving to: ‘/tmp/hcc-public-dataset/part0.json’\n\n     0K .......... .......... .......... .......... ..........  0% 1,25M 6s\n    50K .......... .......... .......... .......... ..........  1% 3,21M 4s\n   100K .......... .......... .......... .......... ..........  1% 3,14M 4s\n   150K .......... .......... .......... .......... ..........  2% 4,39M 3s\n   200K .......... .......... .......... .......... ..........  3% 5,16M 3s\n   250K .......... .......... .......... .......... ..........  3% 3,33M 3s\n   300K .......... .......... .......... .......... ..........  4% 6,22M 2s\n   350K .......... .......... .......... .......... ..........  5% 4,62M 2s\n   400K .......... .......... .......... .......... ..........  5% 2,80M 2s\n   450K .......... .......... .......... .......... ..........  6% 4,48M 2s\n   500K .......... .......... .......... .......... ..........  7% 5,89M 2s\n   550K .......... .......... .......... .......... ..........  7% 6,46M 2s\n   600K .......... .......... .......... .......... ..........  8% 5,67M 2s\n   650K .......... .......... .......... .......... ..........  8% 3,09M 2s\n   700K .......... .......... .......... .......... ..........  9% 11,8M 2s\n   750K .......... .......... .......... .......... .......... 10% 5,80M 2s\n   800K .......... .......... .......... .......... .......... 10% 7,15M 2s\n   850K .......... .......... .......... .......... .......... 11% 6,34M 2s\n   900K .......... .......... .......... .......... .......... 12% 6,65M 2s\n   950K .......... .......... .......... .......... .......... 12% 5,33M 2s\n  1000K .......... .......... .......... .......... .......... 13% 15,2M 2s\n  1050K .......... .......... .......... .......... .......... 14% 2,50M 2s\n  1100K .......... .......... .......... .......... .......... 14% 39,1M 2s\n  1150K .......... .......... .......... .......... .......... 15% 33,3M 1s\n  1200K .......... .......... .......... .......... .......... 16% 4,31M 1s\n  1250K .......... .......... .......... .......... .......... 16% 11,2M 1s\n  1300K .......... .......... .......... .......... .......... 17% 7,45M 1s\n  1350K .......... .......... .......... .......... .......... 17% 5,09M 1s\n  1400K .......... .......... .......... .......... .......... 18% 15,3M 1s\n  1450K .......... .......... .......... .......... .......... 19% 4,23M 1s\n  1500K .......... .......... .......... .......... .......... 19% 3,59M 1s\n  1550K .......... .......... .......... .......... .......... 20% 3,72M 1s\n  1600K .......... .......... .......... .......... .......... 21% 30,7M 1s\n  1650K .......... .......... .......... .......... .......... 21% 44,6M 1s\n  1700K .......... .......... .......... .......... .......... 22% 9,82M 1s\n  1750K .......... .......... .......... .......... .......... 23% 3,54M 1s\n  1800K .......... .......... .......... .......... .......... 23% 10,2M 1s\n  1850K .......... .......... .......... .......... .......... 24% 53,3M 1s\n  1900K .......... .......... .......... .......... .......... 24% 3,33M 1s\n  1950K .......... .......... .......... .......... .......... 25% 9,09M 1s\n  2000K .......... .......... .......... .......... .......... 26% 51,4M 1s\n  2050K .......... .......... .......... .......... .......... 26% 3,95M 1s\n  2100K .......... .......... .......... .......... .......... 27% 7,46M 1s\n  2150K .......... .......... .......... .......... .......... 28% 4,27M 1s\n  2200K .......... .......... .......... .......... .......... 28% 3,62M 1s\n  2250K .......... .......... .......... .......... .......... 29% 5,93M 1s\n  2300K .......... .......... .......... .......... .......... 30% 55,9M 1s\n  2350K .......... .......... .......... .......... .......... 30% 5,09M 1s\n  2400K .......... .......... .......... .......... .......... 31% 65,1M 1s\n  2450K .......... .......... .......... .......... .......... 32% 6,48M 1s\n  2500K .......... .......... .......... .......... .......... 32% 9,59M 1s\n  2550K .......... .......... .......... .......... .......... 33% 6,19M 1s\n  2600K .......... .......... .......... .......... .......... 33% 6,21M 1s\n  2650K .......... .......... .......... .......... .......... 34% 6,78M 1s\n  2700K .......... .......... .......... .......... .......... 35% 3,69M 1s\n  2750K .......... .......... .......... .......... .......... 35% 5,84M 1s\n  2800K .......... .......... .......... .......... .......... 36% 57,2M 1s\n  2850K .......... .......... .......... .......... .......... 37% 9,10M 1s\n  2900K .......... .......... .......... .......... .......... 37% 28,8M 1s\n  2950K .......... .......... .......... .......... .......... 38% 3,58M 1s\n  3000K .......... .......... .......... .......... .......... 39% 11,7M 1s\n  3050K .......... .......... .......... .......... .......... 39% 4,32M 1s\n  3100K .......... .......... .......... .......... .......... 40% 7,01M 1s\n  3150K .......... .......... .......... .......... .......... 40% 58,4M 1s\n  3200K .......... .......... .......... .......... .......... 41% 3,13M 1s\n  3250K .......... .......... .......... .......... .......... 42% 9,28M 1s\n  3300K .......... .......... .......... .......... .......... 42% 13,6M 1s\n  3350K .......... .......... .......... .......... .......... 43% 13,2M 1s\n  3400K .......... .......... .......... .......... .......... 44% 3,11M 1s\n  3450K .......... .......... .......... .......... .......... 44% 6,63M 1s\n  3500K .......... .......... .......... .......... .......... 45% 5,89M 1s\n  3550K .......... .......... .......... .......... .......... 46% 5,35M 1s\n  3600K .......... .......... .......... .......... .......... 46% 6,09M 1s\n  3650K .......... .......... .......... .......... .......... 47% 43,6M 1s\n  3700K .......... .......... .......... .......... .......... 48% 7,18M 1s\n  3750K .......... .......... .......... .......... .......... 48% 8,15M 1s\n  3800K .......... .......... .......... .......... .......... 49% 10,9M 1s\n  3850K .......... .......... .......... .......... .......... 49% 8,94M 1s\n  3900K .......... .......... .......... .......... .......... 50% 4,00M 1s\n  3950K .......... .......... .......... .......... .......... 51% 6,53M 1s\n  4000K .......... .......... .......... .......... .......... 51% 5,91M 1s\n  4050K .......... .......... .......... .......... .......... 52% 5,75M 1s\n  4100K .......... .......... .......... .......... .......... 53% 7,40M 1s\n  4150K .......... .......... .......... .......... .......... 53% 47,4M 1s\n  4200K .......... .......... .......... .......... .......... 54% 7,29M 1s\n  4250K .......... .......... .......... .......... .......... 55% 7,06M 1s\n  4300K .......... .......... .......... .......... .......... 55% 5,04M 1s\n  4350K .......... .......... .......... .......... .......... 56% 6,40M 1s\n  4400K .......... .......... .......... .......... .......... 56% 5,69M 1s\n  4450K .......... .......... .......... .......... .......... 57% 2,69M 1s\n  4500K .......... .......... .......... .......... .......... 58% 65,8M 1s\n  4550K .......... .......... .......... .......... .......... 58% 9,89M 1s\n  4600K .......... .......... .......... .......... .......... 59% 19,2M 1s\n  4650K .......... .......... .......... .......... .......... 60% 14,1M 1s\n  4700K .......... .......... .......... .......... .......... 60% 91,0M 0s\n  4750K .......... .......... .......... .......... .......... 61% 4,78M 0s\n  4800K .......... .......... .......... .......... .......... 62% 6,94M 0s\n  4850K .......... .......... .......... .......... .......... 62% 21,8M 0s\n  4900K .......... .......... .......... .......... .......... 63% 4,59M 0s\n  4950K .......... .......... .......... .......... .......... 64% 3,61M 0s\n  5000K .......... .......... .......... .......... .......... 64% 55,8M 0s\n  5050K .......... .......... .......... .......... .......... 65% 3,39M 0s\n  5100K .......... .......... .......... .......... .......... 65% 3,16M 0s\n  5150K .......... .......... .......... .......... .......... 66% 46,1M 0s\n  5200K .......... .......... .......... .......... .......... 67% 6,38M 0s\n  5250K .......... .......... .......... .......... .......... 67% 7,78M 0s\n  5300K .......... .......... .......... .......... .......... 68% 40,5M 0s\n  5350K .......... .......... .......... .......... .......... 69% 4,59M 0s\n  5400K .......... .......... .......... .......... .......... 69% 6,53M 0s\n  5450K .......... .......... .......... .......... .......... 70% 37,4M 0s\n  5500K .......... .......... .......... .......... .......... 71% 61,7M 0s\n  5550K .......... .......... .......... .......... .......... 71% 2,65M 0s\n  5600K .......... .......... .......... .......... .......... 72% 4,38M 0s\n  5650K .......... .......... .......... .......... .......... 72% 3,26M 0s\n  5700K .......... .......... .......... .......... .......... 73% 56,1M 0s\n  5750K .......... .......... .......... .......... .......... 74% 6,03M 0s\n  5800K .......... .......... .......... .......... .......... 74% 5,99M 0s\n  5850K .......... .......... .......... .......... .......... 75% 6,01M 0s\n  5900K .......... .......... .......... .......... .......... 76% 40,3M 0s\n  5950K .......... .......... .......... .......... .......... 76% 6,50M 0s\n  6000K .......... .......... .......... .......... .......... 77% 5,66M 0s\n  6050K .......... .......... .......... .......... .......... 78% 4,99M 0s\n  6100K .......... .......... .......... .......... .......... 78% 2,74M 0s\n  6150K .......... .......... .......... .......... .......... 79% 56,9M 0s\n  6200K .......... .......... .......... .......... .......... 80% 8,27M 0s\n  6250K .......... .......... .......... .......... .......... 80% 71,3M 0s\n  6300K .......... .......... .......... .......... .......... 81% 2,79M 0s\n  6350K .......... .......... .......... .......... .......... 81% 32,8M 0s\n  6400K .......... .......... .......... .......... .......... 82% 8,04M 0s\n  6450K .......... .......... .......... .......... .......... 83% 63,6M 0s\n  6500K .......... .......... .......... .......... .......... 83% 12,4M 0s\n  6550K .......... .......... .......... .......... .......... 84% 14,3M 0s\n  6600K .......... .......... .......... .......... .......... 85% 5,21M 0s\n  6650K .......... .......... .......... .......... .......... 85% 7,25M 0s\n  6700K .......... .......... .......... .......... .......... 86% 3,68M 0s\n  6750K .......... .......... .......... .......... .......... 87% 6,16M 0s\n  6800K .......... .......... .......... .......... .......... 87% 68,8M 0s\n  6850K .......... .......... .......... .......... .......... 88% 2,84M 0s\n  6900K .......... .......... .......... .......... .......... 88% 60,4M 0s\n  6950K .......... .......... .......... .......... .......... 89% 7,09M 0s\n  7000K .......... .......... .......... .......... .......... 90% 93,0M 0s\n  7050K .......... .......... .......... .......... .......... 90% 2,80M 0s\n  7100K .......... .......... .......... .......... .......... 91% 6,96M 0s\n  7150K .......... .......... .......... .......... .......... 92% 55,0M 0s\n  7200K .......... .......... .......... .......... .......... 92% 2,78M 0s\n  7250K .......... .......... .......... .......... .......... 93% 65,8M 0s\n  7300K .......... .......... .......... .......... .......... 94% 7,33M 0s\n  7350K .......... .......... .......... .......... .......... 94% 7,32M 0s\n  7400K .......... .......... .......... .......... .......... 95% 44,3M 0s\n  7450K .......... .......... .......... .......... .......... 96% 4,94M 0s\n  7500K .......... .......... .......... .......... .......... 96% 58,8M 0s\n  7550K .......... .......... .......... .......... .......... 97% 5,65M 0s\n  7600K .......... .......... .......... .......... .......... 97% 3,28M 0s\n  7650K .......... .......... .......... .......... .......... 98% 69,9M 0s\n  7700K .......... .......... .......... .......... .......... 99% 29,3M 0s\n  7750K .......... .......... .......... .......... .......... 99% 85,8M 0s\n  7800K .......... .                                          100% 78,8M=1,2s\n\n2016-12-12 18:07:11 (6,42 MB/s) - ‘/tmp/hcc-public-dataset/part0.json’ saved [7999441/7999441]\n\n--2016-12-12 18:07:11--  https://raw.githubusercontent.com/roberthryniewicz/datasets/master/hcc-pulic-dataset/hcc-part1.json\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.40.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.40.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 5605307 (5,3M) [text/plain]\nSaving to: ‘/tmp/hcc-public-dataset/part1.json’\n\n     0K .......... .......... .......... .......... ..........  0% 1,27M 4s\n    50K .......... .......... .......... .......... ..........  1% 2,95M 3s\n   100K .......... .......... .......... .......... ..........  2% 3,04M 3s\n   150K .......... .......... .......... .......... ..........  3% 12,7M 2s\n   200K .......... .......... .......... .......... ..........  4% 3,91M 2s\n   250K .......... .......... .......... .......... ..........  5% 2,83M 2s\n   300K .......... .......... .......... .......... ..........  6% 3,63M 2s\n   350K .......... .......... .......... .......... ..........  7% 4,48M 2s\n   400K .......... .......... .......... .......... ..........  8% 4,36M 2s\n   450K .......... .......... .......... .......... ..........  9% 3,61M 2s\n   500K .......... .......... .......... .......... .......... 10% 2,94M 2s\n   550K .......... .......... .......... .......... .......... 10% 5,00M 1s\n   600K .......... .......... .......... .......... .......... 11% 6,58M 1s\n   650K .......... .......... .......... .......... .......... 12% 4,05M 1s\n   700K .......... .......... .......... .......... .......... 13% 5,37M 1s\n   750K .......... .......... .......... .......... .......... 14% 5,13M 1s\n   800K .......... .......... .......... .......... .......... 15% 5,43M 1s\n   850K .......... .......... .......... .......... .......... 16% 6,25M 1s\n   900K .......... .......... .......... .......... .......... 17% 4,36M 1s\n   950K .......... .......... .......... .......... .......... 18% 5,12M 1s\n  1000K .......... .......... .......... .......... .......... 19% 7,44M 1s\n  1050K .......... .......... .......... .......... .......... 20% 8,43M 1s\n  1100K .......... .......... .......... .......... .......... 21% 2,99M 1s\n  1150K .......... .......... .......... .......... .......... 21% 6,58M 1s\n  1200K .......... .......... .......... .......... .......... 22% 3,97M 1s\n  1250K .......... .......... .......... .......... .......... 23% 8,03M 1s\n  1300K .......... .......... .......... .......... .......... 24% 8,95M 1s\n  1350K .......... .......... .......... .......... .......... 25% 3,02M 1s\n  1400K .......... .......... .......... .......... .......... 26% 10,0M 1s\n  1450K .......... .......... .......... .......... .......... 27% 5,18M 1s\n  1500K .......... .......... .......... .......... .......... 28% 6,05M 1s\n  1550K .......... .......... .......... .......... .......... 29% 5,66M 1s\n  1600K .......... .......... .......... .......... .......... 30% 17,5M 1s\n  1650K .......... .......... .......... .......... .......... 31% 7,56M 1s\n  1700K .......... .......... .......... .......... .......... 31% 4,38M 1s\n  1750K .......... .......... .......... .......... .......... 32% 3,51M 1s\n  1800K .......... .......... .......... .......... .......... 33% 9,57M 1s\n  1850K .......... .......... .......... .......... .......... 34% 98,3M 1s\n  1900K .......... .......... .......... .......... .......... 35% 6,22M 1s\n  1950K .......... .......... .......... .......... .......... 36% 8,07M 1s\n  2000K .......... .......... .......... .......... .......... 37% 4,91M 1s\n  2050K .......... .......... .......... .......... .......... 38% 5,89M 1s\n  2100K .......... .......... .......... .......... .......... 39% 5,29M 1s\n  2150K .......... .......... .......... .......... .......... 40% 5,02M 1s\n  2200K .......... .......... .......... .......... .......... 41% 5,37M 1s\n  2250K .......... .......... .......... .......... .......... 42% 65,3M 1s\n  2300K .......... .......... .......... .......... .......... 42% 5,04M 1s\n  2350K .......... .......... .......... .......... .......... 43% 5,19M 1s\n  2400K .......... .......... .......... .......... .......... 44% 13,0M 1s\n  2450K .......... .......... .......... .......... .......... 45% 5,88M 1s\n  2500K .......... .......... .......... .......... .......... 46% 4,14M 1s\n  2550K .......... .......... .......... .......... .......... 47% 64,1M 1s\n  2600K .......... .......... .......... .......... .......... 48% 4,72M 1s\n  2650K .......... .......... .......... .......... .......... 49% 10,3M 1s\n  2700K .......... .......... .......... .......... .......... 50% 5,96M 1s\n  2750K .......... .......... .......... .......... .......... 51% 5,94M 1s\n  2800K .......... .......... .......... .......... .......... 52% 8,87M 1s\n  2850K .......... .......... .......... .......... .......... 52% 9,23M 0s\n  2900K .......... .......... .......... .......... .......... 53% 3,51M 0s\n  2950K .......... .......... .......... .......... .......... 54% 6,83M 0s\n  3000K .......... .......... .......... .......... .......... 55% 65,4M 0s\n  3050K .......... .......... .......... .......... .......... 56% 4,54M 0s\n  3100K .......... .......... .......... .......... .......... 57% 7,91M 0s\n  3150K .......... .......... .......... .......... .......... 58% 7,04M 0s\n  3200K .......... .......... .......... .......... .......... 59% 5,84M 0s\n  3250K .......... .......... .......... .......... .......... 60% 3,33M 0s\n  3300K .......... .......... .......... .......... .......... 61% 14,4M 0s\n  3350K .......... .......... .......... .......... .......... 62% 9,67M 0s\n  3400K .......... .......... .......... .......... .......... 63% 11,8M 0s\n  3450K .......... .......... .......... .......... .......... 63% 9,66M 0s\n  3500K .......... .......... .......... .......... .......... 64% 5,89M 0s\n  3550K .......... .......... .......... .......... .......... 65% 5,61M 0s\n  3600K .......... .......... .......... .......... .......... 66% 6,88M 0s\n  3650K .......... .......... .......... .......... .......... 67% 8,41M 0s\n  3700K .......... .......... .......... .......... .......... 68% 3,29M 0s\n  3750K .......... .......... .......... .......... .......... 69% 6,51M 0s\n  3800K .......... .......... .......... .......... .......... 70% 9,78M 0s\n  3850K .......... .......... .......... .......... .......... 71% 54,2M 0s\n  3900K .......... .......... .......... .......... .......... 72% 6,31M 0s\n  3950K .......... .......... .......... .......... .......... 73% 10,8M 0s\n  4000K .......... .......... .......... .......... .......... 73% 5,41M 0s\n  4050K .......... .......... .......... .......... .......... 74% 4,96M 0s\n  4100K .......... .......... .......... .......... .......... 75% 8,21M 0s\n  4150K .......... .......... .......... .......... .......... 76% 48,4M 0s\n  4200K .......... .......... .......... .......... .......... 77% 3,17M 0s\n  4250K .......... .......... .......... .......... .......... 78% 12,7M 0s\n  4300K .......... .......... .......... .......... .......... 79% 52,9M 0s\n  4350K .......... .......... .......... .......... .......... 80% 3,79M 0s\n  4400K .......... .......... .......... .......... .......... 81% 6,92M 0s\n  4450K .......... .......... .......... .......... .......... 82% 9,44M 0s\n  4500K .......... .......... .......... .......... .......... 83% 2,32M 0s\n  4550K .......... .......... .......... .......... .......... 84% 86,1M 0s\n  4600K .......... .......... .......... .......... .......... 84% 7,63M 0s\n  4650K .......... .......... .......... .......... .......... 85% 4,66M 0s\n  4700K .......... .......... .......... .......... .......... 86% 5,38M 0s\n  4750K .......... .......... .......... .......... .......... 87% 45,6M 0s\n  4800K .......... .......... .......... .......... .......... 88% 35,4M 0s\n  4850K .......... .......... .......... .......... .......... 89% 5,45M 0s\n  4900K .......... .......... .......... .......... .......... 90% 6,93M 0s\n  4950K .......... .......... .......... .......... .......... 91% 4,56M 0s\n  5000K .......... .......... .......... .......... .......... 92% 6,88M 0s\n  5050K .......... .......... .......... .......... .......... 93% 2,91M 0s\n  5100K .......... .......... .......... .......... .......... 94% 46,1M 0s\n  5150K .......... .......... .......... .......... .......... 94% 7,00M 0s\n  5200K .......... .......... .......... .......... .......... 95% 80,3M 0s\n  5250K .......... .......... .......... .......... .......... 96% 19,1M 0s\n  5300K .......... .......... .......... .......... .......... 97% 3,23M 0s\n  5350K .......... .......... .......... .......... .......... 98% 12,5M 0s\n  5400K .......... .......... .......... .......... .......... 99% 25,2M 0s\n  5450K .......... .......... ...                             100% 56,7M=0,9s\n\n2016-12-12 18:07:13 (5,87 MB/s) - ‘/tmp/hcc-public-dataset/part1.json’ saved [5605307/5605307]\n\n--2016-12-12 18:07:13--  https://raw.githubusercontent.com/roberthryniewicz/datasets/master/hcc-pulic-dataset/hcc-part2.json\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.40.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.40.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4787693 (4,6M) [text/plain]\nSaving to: ‘/tmp/hcc-public-dataset/part2.json’\n\n     0K .......... .......... .......... .......... ..........  1%  663K 7s\n    50K .......... .......... .......... .......... ..........  2% 1,73M 5s\n   100K .......... .......... .......... .......... ..........  3% 2,50M 4s\n   150K .......... .......... .......... .......... ..........  4% 2,41M 3s\n   200K .......... .......... .......... .......... ..........  5% 2,50M 3s\n   250K .......... .......... .......... .......... ..........  6% 2,74M 3s\n   300K .......... .......... .......... .......... ..........  7% 2,94M 2s\n   350K .......... .......... .......... .......... ..........  8% 2,26M 2s\n   400K .......... .......... .......... .......... ..........  9% 3,04M 2s\n   450K .......... .......... .......... .......... .......... 10% 2,87M 2s\n   500K .......... .......... .......... .......... .......... 11% 3,72M 2s\n   550K .......... .......... .......... .......... .......... 12% 3,17M 2s\n   600K .......... .......... .......... .......... .......... 13% 3,20M 2s\n   650K .......... .......... .......... .......... .......... 14% 4,29M 2s\n   700K .......... .......... .......... .......... .......... 16% 3,27M 2s\n   750K .......... .......... .......... .......... .......... 17% 2,81M 2s\n   800K .......... .......... .......... .......... .......... 18% 3,48M 2s\n   850K .......... .......... .......... .......... .......... 19% 4,28M 2s\n   900K .......... .......... .......... .......... .......... 20% 3,84M 1s\n   950K .......... .......... .......... .......... .......... 21% 4,76M 1s\n  1000K .......... .......... .......... .......... .......... 22% 4,20M 1s\n  1050K .......... .......... .......... .......... .......... 23% 4,58M 1s\n  1100K .......... .......... .......... .......... .......... 24% 4,69M 1s\n  1150K .......... .......... .......... .......... .......... 25% 3,30M 1s\n  1200K .......... .......... .......... .......... .......... 26% 4,81M 1s\n  1250K .......... .......... .......... .......... .......... 27% 4,15M 1s\n  1300K .......... .......... .......... .......... .......... 28% 6,12M 1s\n  1350K .......... .......... .......... .......... .......... 29% 4,46M 1s\n  1400K .......... .......... .......... .......... .......... 31% 4,85M 1s\n  1450K .......... .......... .......... .......... .......... 32% 4,98M 1s\n  1500K .......... .......... .......... .......... .......... 33% 6,93M 1s\n  1550K .......... .......... .......... .......... .......... 34% 4,03M 1s\n  1600K .......... .......... .......... .......... .......... 35% 5,61M 1s\n  1650K .......... .......... .......... .......... .......... 36% 5,14M 1s\n  1700K .......... .......... .......... .......... .......... 37% 5,80M 1s\n  1750K .......... .......... .......... .......... .......... 38% 4,55M 1s\n  1800K .......... .......... .......... .......... .......... 39% 8,57M 1s\n  1850K .......... .......... .......... .......... .......... 40% 4,20M 1s\n  1900K .......... .......... .......... .......... .......... 41% 5,54M 1s\n  1950K .......... .......... .......... .......... .......... 42% 4,72M 1s\n  2000K .......... .......... .......... .......... .......... 43% 8,54M 1s\n  2050K .......... .......... .......... .......... .......... 44% 4,10M 1s\n  2100K .......... .......... .......... .......... .......... 45% 5,04M 1s\n  2150K .......... .......... .......... .......... .......... 47% 8,88M 1s\n  2200K .......... .......... .......... .......... .......... 48% 5,65M 1s\n  2250K .......... .......... .......... .......... .......... 49% 5,32M 1s\n  2300K .......... .......... .......... .......... .......... 50% 5,13M 1s\n  2350K .......... .......... .......... .......... .......... 51% 4,27M 1s\n  2400K .......... .......... .......... .......... .......... 52% 14,3M 1s\n  2450K .......... .......... .......... .......... .......... 53% 5,30M 1s\n  2500K .......... .......... .......... .......... .......... 54% 5,22M 1s\n  2550K .......... .......... .......... .......... .......... 55% 7,16M 1s\n  2600K .......... .......... .......... .......... .......... 56% 7,74M 1s\n  2650K .......... .......... .......... .......... .......... 57% 6,67M 1s\n  2700K .......... .......... .......... .......... .......... 58% 5,50M 0s\n  2750K .......... .......... .......... .......... .......... 59% 6,57M 0s\n  2800K .......... .......... .......... .......... .......... 60% 6,91M 0s\n  2850K .......... .......... .......... .......... .......... 62% 11,2M 0s\n  2900K .......... .......... .......... .......... .......... 63% 4,33M 0s\n  2950K .......... .......... .......... .......... .......... 64% 6,42M 0s\n  3000K .......... .......... .......... .......... .......... 65% 4,94M 0s\n  3050K .......... .......... .......... .......... .......... 66% 10,6M 0s\n  3100K .......... .......... .......... .......... .......... 67% 7,82M 0s\n  3150K .......... .......... .......... .......... .......... 68% 6,69M 0s\n  3200K .......... .......... .......... .......... .......... 69% 2,42M 0s\n  3250K .......... .......... .......... .......... .......... 70% 7,49M 0s\n  3300K .......... .......... .......... .......... .......... 71% 7,45M 0s\n  3350K .......... .......... .......... .......... .......... 72% 6,51M 0s\n  3400K .......... .......... .......... .......... .......... 73% 6,16M 0s\n  3450K .......... .......... .......... .......... .......... 74% 14,7M 0s\n  3500K .......... .......... .......... .......... .......... 75% 6,18M 0s\n  3550K .......... .......... .......... .......... .......... 76% 5,33M 0s\n  3600K .......... .......... .......... .......... .......... 78% 4,15M 0s\n  3650K .......... .......... .......... .......... .......... 79% 16,7M 0s\n  3700K .......... .......... .......... .......... .......... 80% 3,10M 0s\n  3750K .......... .......... .......... .......... .......... 81% 28,4M 0s\n  3800K .......... .......... .......... .......... .......... 82% 76,5M 0s\n  3850K .......... .......... .......... .......... .......... 83% 3,56M 0s\n  3900K .......... .......... .......... .......... .......... 84% 4,78M 0s\n  3950K .......... .......... .......... .......... .......... 85% 86,0M 0s\n  4000K .......... .......... .......... .......... .......... 86% 2,84M 0s\n  4050K .......... .......... .......... .......... .......... 87% 57,0M 0s\n  4100K .......... .......... .......... .......... .......... 88% 4,87M 0s\n  4150K .......... .......... .......... .......... .......... 89% 4,76M 0s\n  4200K .......... .......... .......... .......... .......... 90% 6,26M 0s\n  4250K .......... .......... .......... .......... .......... 91% 4,98M 0s\n  4300K .......... .......... .......... .......... .......... 93% 6,87M 0s\n  4350K .......... .......... .......... .......... .......... 94% 6,24M 0s\n  4400K .......... .......... .......... .......... .......... 95% 12,4M 0s\n  4450K .......... .......... .......... .......... .......... 96% 18,7M 0s\n  4500K .......... .......... .......... .......... .......... 97% 5,81M 0s\n  4550K .......... .......... .......... .......... .......... 98% 6,07M 0s\n  4600K .......... .......... .......... .......... .......... 99% 6,12M 0s\n  4650K .......... .......... .....                           100% 69,3M=1,0s\n\n2016-12-12 18:07:14 (4,54 MB/s) - ‘/tmp/hcc-public-dataset/part2.json’ saved [4787693/4787693]\n\n--2016-12-12 18:07:14--  https://raw.githubusercontent.com/roberthryniewicz/datasets/master/hcc-pulic-dataset/hcc-part3.json\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.40.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.40.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3201163 (3,1M) [text/plain]\nSaving to: ‘/tmp/hcc-public-dataset/part3.json’\n\n     0K .......... .......... .......... .......... ..........  1% 1,26M 2s\n    50K .......... .......... .......... .......... ..........  3% 4,20M 2s\n   100K .......... .......... .......... .......... ..........  4% 3,10M 1s\n   150K .......... .......... .......... .......... ..........  6% 8,77M 1s\n   200K .......... .......... .......... .......... ..........  7% 3,39M 1s\n   250K .......... .......... .......... .......... ..........  9% 3,67M 1s\n   300K .......... .......... .......... .......... .......... 11% 2,91M 1s\n   350K .......... .......... .......... .......... .......... 12% 7,09M 1s\n   400K .......... .......... .......... .......... .......... 14% 8,08M 1s\n   450K .......... .......... .......... .......... .......... 15% 2,43M 1s\n   500K .......... .......... .......... .......... .......... 17% 10,4M 1s\n   550K .......... .......... .......... .......... .......... 19% 2,70M 1s\n   600K .......... .......... .......... .......... .......... 20% 4,82M 1s\n   650K .......... .......... .......... .......... .......... 22% 7,43M 1s\n   700K .......... .......... .......... .......... .......... 23% 4,82M 1s\n   750K .......... .......... .......... .......... .......... 25% 4,35M 1s\n   800K .......... .......... .......... .......... .......... 27% 69,1M 1s\n   850K .......... .......... .......... .......... .......... 28% 5,71M 1s\n   900K .......... .......... .......... .......... .......... 30% 5,40M 1s\n   950K .......... .......... .......... .......... .......... 31% 4,34M 1s\n  1000K .......... .......... .......... .......... .......... 33% 7,03M 0s\n  1050K .......... .......... .......... .......... .......... 35% 8,08M 0s\n  1100K .......... .......... .......... .......... .......... 36% 2,96M 0s\n  1150K .......... .......... .......... .......... .......... 38% 19,0M 0s\n  1200K .......... .......... .......... .......... .......... 39% 51,0M 0s\n  1250K .......... .......... .......... .......... .......... 41% 4,81M 0s\n  1300K .......... .......... .......... .......... .......... 43% 4,59M 0s\n  1350K .......... .......... .......... .......... .......... 44% 27,8M 0s\n  1400K .......... .......... .......... .......... .......... 46% 4,81M 0s\n  1450K .......... .......... .......... .......... .......... 47% 5,39M 0s\n  1500K .......... .......... .......... .......... .......... 49% 6,30M 0s\n  1550K .......... .......... .......... .......... .......... 51% 3,22M 0s\n  1600K .......... .......... .......... .......... .......... 52% 70,5M 0s\n  1650K .......... .......... .......... .......... .......... 54% 22,8M 0s\n  1700K .......... .......... .......... .......... .......... 55% 3,97M 0s\n  1750K .......... .......... .......... .......... .......... 57% 5,13M 0s\n  1800K .......... .......... .......... .......... .......... 59% 6,95M 0s\n  1850K .......... .......... .......... .......... .......... 60% 11,1M 0s\n  1900K .......... .......... .......... .......... .......... 62% 3,43M 0s\n  1950K .......... .......... .......... .......... .......... 63% 5,89M 0s\n  2000K .......... .......... .......... .......... .......... 65%  324M 0s\n  2050K .......... .......... .......... .......... .......... 67% 10,9M 0s\n  2100K .......... .......... .......... .......... .......... 68% 5,09M 0s\n  2150K .......... .......... .......... .......... .......... 70% 5,10M 0s\n  2200K .......... .......... .......... .......... .......... 71% 4,15M 0s\n  2250K .......... .......... .......... .......... .......... 73% 5,00M 0s\n  2300K .......... .......... .......... .......... .......... 75% 5,48M 0s\n  2350K .......... .......... .......... .......... .......... 76% 68,3M 0s\n  2400K .......... .......... .......... .......... .......... 78% 25,9M 0s\n  2450K .......... .......... .......... .......... .......... 79% 5,47M 0s\n  2500K .......... .......... .......... .......... .......... 81% 10,9M 0s\n  2550K .......... .......... .......... .......... .......... 83% 4,56M 0s\n  2600K .......... .......... .......... .......... .......... 84% 7,47M 0s\n  2650K .......... .......... .......... .......... .......... 86% 2,80M 0s\n  2700K .......... .......... .......... .......... .......... 87% 62,6M 0s\n  2750K .......... .......... .......... .......... .......... 89% 8,89M 0s\n  2800K .......... .......... .......... .......... .......... 91% 76,7M 0s\n  2850K .......... .......... .......... .......... .......... 92% 9,44M 0s\n  2900K .......... .......... .......... .......... .......... 94% 12,7M 0s\n  2950K .......... .......... .......... .......... .......... 95% 3,54M 0s\n  3000K .......... .......... .......... .......... .......... 97% 6,39M 0s\n  3050K .......... .......... .......... .......... .......... 99% 13,3M 0s\n  3100K .......... .......... ......                          100% 43,5M=0,5s\n\n2016-12-12 18:07:15 (5,63 MB/s) - ‘/tmp/hcc-public-dataset/part3.json’ saved [3201163/3201163]\n\n"},"dateCreated":"2016-12-12T05:35:01-0800","dateStarted":"2016-12-12T06:07:09-0800","dateFinished":"2016-12-12T06:07:15-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:516"},{"title":"Load dataset","text":"%pyspark\n\npath = \"file:///tmp/hcc-public-dataset\"\ndata = spark.read.json(path)                        # Create a DataFrame from JSON (automatically infer schema and data types)","dateUpdated":"2016-12-12T19:37:00-0800","config":{"editorMode":"ace/mode/scala","colWidth":12,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481592848024_923998329","id":"20161111-012558_1529171858","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2016-12-12T05:34:08-0800","dateStarted":"2016-12-12T19:37:00-0800","dateFinished":"2016-12-12T19:37:00-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:517"},{"title":"Preview Schema","text":"%pyspark\n\nraw_data.printSchema()","dateUpdated":"2016-12-13T19:16:22-0800","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481592848024_923998329","id":"20161123-143210_112792408","result":{"code":"SUCCESS","type":"TEXT","msg":"root\n |-- body: string (nullable = true)\n |-- creation_date: string (nullable = true)\n |-- id: long (nullable = true)\n |-- tags: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- title: string (nullable = true)\n |-- track_id: long (nullable = true)\n |-- track_name: string (nullable = true)\n\n"},"dateCreated":"2016-12-12T05:34:08-0800","dateStarted":"2016-12-13T19:16:22-0800","dateFinished":"2016-12-13T19:16:22-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:518"},{"text":"%md\n\n## Part 2\n#### Prepare data for training","dateUpdated":"2016-12-12T05:34:08-0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481592848025_923613581","id":"20161123-145345_2039764571","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Part 2</h2>\n<h4>Prepare data for training</h4>\n"},"dateCreated":"2016-12-12T05:34:08-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:519"},{"title":"Define Helper Functions","text":"%pyspark\n\nimport re\n\ndef prepTags(tags):\n    \"\"\" Lowercase, remove numbers and non-word characters, and return tag names length > 1\n    \n    Sample input: ['s', 'Spark', 'atlas-oozie', 'built_in', 'atlas', 'atlas-2.4.5', 'hdp2.4', 'node-label']\n    Expected output: ['atlas', 'atlas-oozie', 'built_in', 'hdp', 'node-label', 'spark']\n    \n    \"\"\"\n    tag_str = ' '.join([tag.lower() for tag in tags])\n    cleanr = re.compile('(-[\\.0-9]+)|([\\.0-9]+)')           \n    clean_tags_str = re.sub(cleanr, '', tag_str)\n    sorted_tags_str = ' '.join( sorted( set( [tag for tag in clean_tags_str.split(' ') if len(tag) > 1] )))\n    return sorted_tags_str\n    \ndef prepText(txt):\n    \"\"\" Prepare text by lowercasing and removing HTML tags and non-word characters\n    \n        Sample input: \"<p>I've noticed I am NOT able to edit my new-elephant's (superuser) in Atlas-2.4.5 HDP2.4.</p>\"\n        Expected output: 'noticed am not able to edit my new elephant superuser in atlas hdp'\n   \n    \"\"\"\n    cleanr1 = re.compile('<.*?>')                               # HTML tag regex\n    no_html = re.sub(cleanr1, ' ', txt)\n    cleanr2 = re.compile(\"(-[\\.0-9]+)|([\\.0-9]+)\")              # Numbers regex\n    str1 = re.sub(cleanr2, ' ', no_html)\n    cleanr3 = re.compile(\"(\\'[A-Za-z]+)\")                       # Apostrophe endings\n    str2 = re.sub(cleanr3, '', str1)\n    split_regex = \"\\W+\"\n    clean_str = ' '.join(filter(lambda word: len(word) > 1,   # Words length > 1\n                      re.split(split_regex, str2.lower())))     # Split on words only and lowercase (removes other non-word characters)\n    return clean_str\n    \ndef createUrl(id):\n    \"\"\" Create an HCC url\n    \"\"\"\n    return \"https://community.hortonworks.com/questions/\"+str(id)+\"/\"","dateUpdated":"2016-12-12T19:36:40-0800","config":{"tableHide":false,"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481592848025_923613581","id":"20161111-020758_149168924","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2016-12-12T05:34:08-0800","dateStarted":"2016-12-12T19:36:40-0800","dateFinished":"2016-12-12T19:36:40-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:520"},{"title":"Create a DataFrame with a new field","text":"%pyspark\n\nfrom pyspark.sql.functions import concat_ws\n\n# create text field (see option 1,2,3 below) combining other fields\n\ncombo_data = data.select(\n              data.id,\n              data.title,\n              data.body,\n              data.tags,\n              #concat_ws(' ', data.title, data.body).alias('text'),              # Option 1: title + body => text\n              concat_ws(' ', data.tags, data.title).alias('text'),               # Option 2: tags + title => text\n              #concat_ws(' ', data.tags, data.title, data.body).alias('text'),   # Option 3: tags + title + body => text\n              data.track_id,\n              data.track_name).cache()\n\n# Preview\ncombo_data.take(1)","dateUpdated":"2016-12-12T20:11:15-0800","config":{"tableHide":false,"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481592848025_923613581","id":"20161111-012701_658207508","result":{"code":"SUCCESS","type":"TEXT","msg":"[Row(id=32454, title=u'phoenix not returning the versions', body=u\"<p>I created a phoenix table usig sqlline.py and i am in HDP 2.4.</p><p>CREATE TABLE IF NOT EXISTS test1\\n    ( id integer not null primary key, name varchar(10), address varchar(10))\\n    DATA_BLOCK_ENCODING='NONE',VERSIONS=5;</p><p>I updated a row with a new value to the column. But when i try a select statement on the table</p><p>select * from test1; </p><p>i dont see all the versions. i only see the latest version. Is there any other way to query all the versions or is my create table statement correct with respect to creating multiple versions.</p>\", tags=[u'Hbase', u'Phoenix'], text=u'Hbase Phoenix phoenix not returning the versions', track_id=66, track_name=u'Data Processing')]\n"},"dateCreated":"2016-12-12T05:34:08-0800","dateStarted":"2016-12-12T20:11:15-0800","dateFinished":"2016-12-12T20:11:15-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:521"},{"title":"Prepare a DataFrame for training a model","text":"%pyspark\n\n# Create url\n# Clean tags & text\n\nrdds = (combo_data.select(\"id\", \"id\", \"title\", \"tags\", \"text\", \"track_id\", \"track_name\")\n        .rdd.map(lambda t: (t[0], createUrl(t[1]), prepText(t[2]), prepTags(t[3]), prepText(t[4]), t[5], prepText(t[6])))\n        .cache())\n\n# make sure text not empty\nrdds_filtered = rdds.filter(lambda t: t[4] != \"\").cache()             \n\ndata_m = rdds_filtered.toDF([\"id\", \"url\", \"title\", \"tags\", \"text\", \"track_id\", \"track_name\"], \n                            ['string', 'string', 'string', 'string', 'string', 'string', 'string']).cache()\n\n# Preview\ndata_m.take(1)","dateUpdated":"2016-12-12T20:08:24-0800","config":{"tableHide":false,"editorMode":"ace/mode/scala","colWidth":12,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481592848025_923613581","id":"20161111-021148_1675792381","result":{"code":"SUCCESS","type":"TEXT","msg":"[Row(id=32454, url=u'https://community.hortonworks.com/questions/32454/', title=u'phoenix not returning the versions', tags=u'hbase phoenix', text=u'hbase phoenix phoenix not returning the versions', track_id=66, track_name=u'data processing')]\n"},"dateCreated":"2016-12-12T05:34:08-0800","dateStarted":"2016-12-12T20:08:24-0800","dateFinished":"2016-12-12T20:08:25-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:522"},{"text":"%md\n\n## Part 3\n#### Train a model","dateUpdated":"2016-12-12T05:34:08-0800","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481592848025_923613581","id":"20161123-180232_1368683895","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Part 3</h2>\n<h4>Train a model</h4>\n"},"dateCreated":"2016-12-12T05:34:08-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:523"},{"title":"Split Training & Test Data","text":"%pyspark\n\n# Split the data into training and test sets\n(trainingData, testData) = data_m.randomSplit([0.7, 0.3], 1234L)","dateUpdated":"2016-12-13T20:27:31-0800","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481592848025_923613581","id":"20161116-010716_797143862","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2016-12-12T05:34:08-0800","dateStarted":"2016-12-12T20:06:54-0800","dateFinished":"2016-12-12T20:06:54-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:524"},{"title":"Train Naive Bayes Model","text":"%pyspark\n\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer, StringIndexer, StringIndexerModel, StopWordsRemover, IndexToString\nfrom pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml import Pipeline\n\nindexer     = StringIndexer(inputCol=\"track_name\", outputCol=\"label\").setHandleInvalid(\"skip\").fit(trainingData)\n\ntokenizer   = Tokenizer(inputCol=\"text\", outputCol=\"rawWords\")                                         \n\nremover     = StopWordsRemover(inputCol=\"rawWords\", outputCol=\"filteredWords\")                      \n\nhashingTF   = HashingTF(inputCol=\"filteredWords\", outputCol=\"rawFeatures\", numFeatures=250000)\n\nidf         = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=2)                               \n\nnb          = NaiveBayes(featuresCol=\"features\", labelCol=\"label\", smoothing=1.0, modelType=\"multinomial\")  # implicit name of outputCol: \"prediction\"\n\nconverter   = IndexToString(inputCol=\"prediction\", outputCol=\"predictedTrack\", labels=indexer._call_java(\"labels\"))\n\npipeline = Pipeline(stages=[indexer, tokenizer, remover, hashingTF, idf, nb, converter])\n\nmodel = pipeline.fit(trainingData)","dateUpdated":"2016-12-13T19:23:20-0800","config":{"tableHide":false,"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481592848025_923613581","id":"20161115-002850_763238464","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2016-12-12T05:34:08-0800","dateStarted":"2016-12-13T19:23:20-0800","dateFinished":"2016-12-13T19:23:22-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:525"},{"title":"Optional: Load model (if available)","text":"%pyspark\n\nfrom pyspark.ml import PipelineModel\n\nmodel_path = \"/path/to/my/model\"\n\n#model = PipelineModel.load(model_path)","dateUpdated":"2016-12-12T21:30:51-0800","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481606824781_-667260098","id":"20161212-212704_70953211","dateCreated":"2016-12-12T21:27:04-0800","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:526"},{"title":"Predict","text":"%pyspark\n\nprediction = model.transform(testData)","dateUpdated":"2016-12-13T19:23:57-0800","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481592848025_923613581","id":"20161116-203016_796242716","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2016-12-12T05:34:08-0800","dateStarted":"2016-12-13T19:23:57-0800","dateFinished":"2016-12-13T19:23:57-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:527"},{"title":"Score Prediction","text":"%pyspark\n\ntotal = prediction.count()\ncorrect = prediction.select(\"label\", \"prediction\").rdd.filter(lambda (l,p): l == p).count()\n\nprint \"score: \", float(correct)/total","dateUpdated":"2016-12-13T19:24:02-0800","config":{"tableHide":false,"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481592848025_923613581","id":"20161115-012159_1806967721","result":{"code":"SUCCESS","type":"TEXT","msg":"score:  0.550321825629\n"},"dateCreated":"2016-12-12T05:34:08-0800","dateStarted":"2016-12-13T19:24:02-0800","dateFinished":"2016-12-13T19:24:02-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:528"},{"title":"Register Temp View","text":"%pyspark\nprediction.createOrReplaceTempView(\"predictions\")","dateUpdated":"2016-12-13T19:25:13-0800","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481592848026_924767827","id":"20161111-014425_1068726253","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2016-12-12T05:34:08-0800","dateStarted":"2016-12-13T19:25:13-0800","dateFinished":"2016-12-13T19:25:13-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:529"},{"title":"Show Results","text":"%sql\nSELECT track_name AS originalTrack, predictedTrack, title, tags --, text\nFROM predictions \nLIMIT 100","dateUpdated":"2016-12-13T19:25:32-0800","config":{"lineNumbers":true,"colWidth":12,"editorMode":"ace/mode/sql","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"originalTrack","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"originalTrack","index":0,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481592848026_924767827","id":"20161115-235400_1774340794","result":{"code":"SUCCESS","type":"TABLE","msg":"originalTrack\tpredictedTrack\ttitle\ttags\ndata ingestion streaming\tdata ingestion streaming\thow to connect storm to hbase\thbase storm\ndata processing\tdata processing\thive cli unresponsive\tcli hive\ncloud operations\tcloud operations\thbase regionservers won start after cloudbreak install in aws\tambari aws cloudbreak regionserver\nsandbox learning\tcloud operations\tusr hdp current hadoop client conf doesn exist error\tambari\ndata processing\thadoop core\tdatastage hadoop\tdatastage\nsandbox learning\tsandbox learning\tpermission denied when removing file from hdfs from sandbox\thdfs-permissions permission-denied rm\ndata processing\tdata processing\tam getting execution error return code from org apache hadoop hive ql exec tez teztask\terror help sandbox\ndesign architecture\tdata processing\tlucidworks hdpsearch solr installation\thdpsearch lucidworks solr\nsecurity\tds analytics spark\tzeppelin support for kerberos in hive interpreter\thive kerberos zeppelin zeppelin-notebook\ndata ingestion streaming\tdata ingestion streaming\tsqoop job too slow importing data from teradata to hive table\thive performance sqoop teradata\nsecurity\tdata processing\twhere to host api within hadoop for knox query\tapi hadoop knox server\nhadoop core\thadoop core\tcapacityscheduler job priority preemption\tyarn yarn-scheduler\ndata processing\tsecurity\twebhcat returns http error not found\thdfs http url webhcat webhdfs\nsandbox learning\tcloud operations\thi have installed sandbox and able to login to sandbox but when issue hadoop command it is giving me connectionexception can you please let me know is there any firewall setting or do include ip details any where\tsandbox\ndata processing\tdata processing\ttodate java lang illegalargumentexception invalid format is malformed\thive invalid-format pig\nhadoop core\tdata processing\trunning mapreduce program using oozie map reduce action\tmapreduce oozie\ndata processing\tdata processing\thow to store json data into hbase\tpig\nhadoop core\tdata processing\tpermission denied for user while creating hive table using sqoop in oozie workflow\thdfs-permissions hive oozie permission-denied yarn\ncloud operations\thadoop core\tpig joib failing slf failed to load class org slf impl staticloggerbinder\taws ec pig\ncloud operations\tcloud operations\thdp utils centos rpm dependency problems\tcentos dependency installation rpm\ncloud operations\tcloud operations\thive settings missing in ambari\tambari hive\nsolutions\thadoop core\thow to verify if namenode is good to leave safemode\tnamenode\ncloud operations\tcloud operations\terror with amabari install at conf select py it is expecting an int but receiving possible symlink issue\tambari installation\nsandbox learning\tcloud operations\thive queries in ambari stay in status\tambari help hive\ndata ingestion streaming\tdata ingestion streaming\twhat is the difference between google dataflow and hadoop data flow\tdataflow google hdf\ndata processing\tdata processing\thive query stalls on hive cli\thive\nsecurity\tsecurity\tenable kerberos error while checking admin admin principal\tambari kdc kerberos\ncloud operations\tcloud operations\thow to add datanode heap metric widget in ambari dashboard just like namenode heap\tambari ambari-metrics\ndesign architecture\tcloud operations\tcluster creation in progress issue with ambari\tambari cluster\ngovernance lifecycle\tdata processing\toozie job fails if the history server is down\thistoryserver mapreduce oozie\nsecurity\tsecurity\tcan not access hdfs\thdfs kerberos\ndesign architecture\tsecurity\tranger admin install fails with access denied error\tmysql ranger ranger-admin\nhadoop core\tgovernance lifecycle\thow replicate hadoop cluster to hadoop cluster dr using apache falcon\tdisaster-recovery falcon\nds analytics spark\tdata ingestion streaming\thdp and hana vora\tsap-hana\nds analytics spark\tcloud operations\tzeppelin not working on upgrading to hdp\tspark upgrade zeppelin\ncloud operations\tcloud operations\tambari agent disk usage alert remedy\tambari ambari-service\ndata processing\tdata processing\tphoenix storage in pig\thive phoenix pig\nhadoop core\thadoop core\twarning use yarn jar to launch yarn applications\tyarn\ndata processing\tdata processing\thive with orc data read from disk for queries same despite very different selectivities\tdisk hive orc\nhadoop core\thadoop core\thow do you get kerberos delegation token in high availability cluster\thigh-availability kerberos webhdfs\ncloud operations\tcloud operations\thow to change password for ambari db post install\tambari\ndesign architecture\tdesign architecture\tin production level ha cluster do we need dedicated servers for journalnodes and zookepers\tarchitecture hdfs\ngovernance lifecycle\tdata ingestion streaming\tstrategies for archiving raw input data\tarchive compression falcon\nsecurity\tsecurity\ton an unkerborized cluster how can get ranger solr to work can get solr ranger working without kerberos\taudit kerberos ranger solr\ndata processing\tdata processing\tcreated phoenix view to map existing hbase table but got different float value on row\thbase phoenix\ndesign architecture\tcloud operations\tneed list of component name groupid artifactid and version required to access each hdp artifactid\thdp maven repository\nhadoop core\tcloud operations\twhich oozie metrics should be monitored\tambari-metrics hdp monitoring oozie\ngovernance lifecycle\tcloud operations\toozie service check fails after moving oozie server\tambari oozie\nsandbox learning\tsandbox learning\tvmware workstation connectivity issue detected\tconnection vmware\ndata processing\tcloud operations\tdo hbase and hdfs need to be co located on the same machines if so how much\tarchitecture hbase hdfs\ndata processing\tdata ingestion streaming\thbase shell throws exception at startup\tedge hbase\ngovernance lifecycle\tdata ingestion streaming\tfree form query in sqoop import with where clause\tsqoop\ncloud operations\tcloud operations\tambari settings after database restore\tambari database\ndata processing\tdata ingestion streaming\tsplit csv and use columns as avro fields\tavro nifi nifi-processor\ndata processing\thadoop core\terror yarn\tjmx yarn\nsolutions\tdata processing\tranger for phoenix\thbase phoenix ranger\ndata processing\thadoop core\thow to enable namenode ha using ambari\tambari ha namenode\nsecurity\tcloud operations\tdoes phoenix allow role creation\thbase phoenix\nhadoop core\tdata processing\tnumber of distcp mappers is small why\tdiscp mapreduce\nsandbox learning\tdata processing\tam stupid or does anyone else have constant issues settings up projects in the hadoop ecosystem\thadoop-ecosystem hive training\ndata processing\tdata processing\tcreating indexes in hive\thive\ndata ingestion streaming\tdata ingestion streaming\tstream data from web site using nifi\tnifi windows\ndata ingestion streaming\tdata ingestion streaming\thow to add content type application json to invokehttp\tinvokehttp\ndata ingestion streaming\tdata ingestion streaming\tkafka spark streaming unable to get any messages\tkafka spark-streaming\ndata processing\tds analytics spark\thow to use with spark\tpython spark\ngovernance lifecycle\tcloud operations\tfalcon doesn start up hdp ambari\tfalcon\nds analytics spark\tdata processing\tsubmit spark applition to kerberised hdp but authentication failed\tspark yarn\ndata ingestion streaming\tdata ingestion streaming\tnot able to read tweets format never seen such behaviour flume\tflume\ndata processing\tdata processing\tsequence number generation in hive\thive\nhadoop core\tcloud operations\tstatus code received on get method for api api stacks hdp versions recommendations\tambari\ndata processing\tdata processing\tclasscastexception while loading decimal data from avro schema backed parquet file\tavro hive parquet\nsecurity\tsecurity\tranger hive policy question\tranger\nsandbox learning\tcloud operations\twhat are all the best possible ways to install single node cluster setup in laptop appreciate your inputs on this\tinstallation\nsandbox learning\tcloud operations\tyum install doesn work couldn resolve host centos org couldn know the problem exactly but need to install hue\tinstallation\ndata processing\tdata processing\tgenericudf not returning data\thive\ndesign architecture\tcloud operations\tambari dashboard logs\tambari dashboard logs\ndata processing\tdata processing\thbase replication identify and fix impacted records\thbase replication\ncloud operations\tcloud operations\twhile registering node with ambariserver am getting the following error can someone please help me to fix the issue\tambari\ndata ingestion streaming\tdata ingestion streaming\thow to extract data from mysql to oracle using apache nifi are there some demos\tnifi\nsecurity\tsecurity\thow the client can access kafka through knox\tknox-gateway\ndata processing\tdata ingestion streaming\toptions for decompressing hdfs data using pig\tcompression pig\ncloud operations\tcloud operations\tambari start service failed with error\tambari\ncloud operations\tcloud operations\tambari only install component through yum other method is have\tambari installer\nds analytics spark\tds analytics spark\twhen trying to run zeppelin for the first time get this error any help please\tspark zeppelin-notebook\ndata processing\tdata processing\thow to increment the hive external table\thive sqoop\nsolutions\tdata processing\thive hdfs for new records\thivecontext\ndata processing\tdata processing\thive insert getting failed\thiveserver\ndata processing\tdata processing\thive benchmarking error tpcds_kit zip issue\thelp hive\ndata processing\tdata processing\tpig map reduce permission denied after history server enable\tmapreduce pig\nhadoop core\tdata processing\thow to get the value in core site xml in oozie java action\toozie\nsandbox learning\tdata processing\tpig queries to analyse the data\ttutorial\ndata ingestion streaming\tdata processing\thadoop exposing sql interface on collection of csv files with variable columns\thadoop-ecosystem hive sql\ndata processing\tdata processing\thive query performance via jdbc java\thive\ndata ingestion streaming\tdata ingestion streaming\tputkafka getkafka nifi and avro format\tavro kafka nifi\nds analytics spark\tcloud operations\tzeppelin offline installation\tinstallation installer zeppelin\ncloud operations\thadoop core\twhat are best practices for patching worker nodes\tdatanode nodemanager regionserver\nds analytics spark\tdata processing\tcreate different schemas at run time for different data frames\tdataframe java schema spark spark-sql\ngovernance lifecycle\tgovernance lifecycle\thow to navigate within atlas how to create tags search etc\tanswerhub atlas hcc help\ncloud operations\tcloud operations\twhere can find the steps to modify configuration with ambari rest for cluster that is spinning several hosts\tambari\ndesign architecture\tcloud operations\thbase rootdir in ambari metrics configuration\tambari hdp\n","comment":"","msgTable":[[{"key":"predictedTrack","value":"data ingestion streaming"},{"key":"predictedTrack","value":"data ingestion streaming"},{"key":"predictedTrack","value":"how to connect storm to hbase"},{"key":"predictedTrack","value":"hbase storm"}],[{"key":"title","value":"data processing"},{"key":"title","value":"data processing"},{"key":"title","value":"hive cli unresponsive"},{"key":"title","value":"cli hive"}],[{"key":"tags","value":"cloud operations"},{"key":"tags","value":"cloud operations"},{"key":"tags","value":"hbase regionservers won start after cloudbreak install in aws"},{"key":"tags","value":"ambari aws cloudbreak regionserver"}],[{"value":"sandbox learning"},{"value":"cloud operations"},{"value":"usr hdp current hadoop client conf doesn exist error"},{"value":"ambari"}],[{"value":"data processing"},{"value":"hadoop core"},{"value":"datastage hadoop"},{"value":"datastage"}],[{"value":"sandbox learning"},{"value":"sandbox learning"},{"value":"permission denied when removing file from hdfs from sandbox"},{"value":"hdfs-permissions permission-denied rm"}],[{"value":"data processing"},{"value":"data processing"},{"value":"am getting execution error return code from org apache hadoop hive ql exec tez teztask"},{"value":"error help sandbox"}],[{"value":"design architecture"},{"value":"data processing"},{"value":"lucidworks hdpsearch solr installation"},{"value":"hdpsearch lucidworks solr"}],[{"value":"security"},{"value":"ds analytics spark"},{"value":"zeppelin support for kerberos in hive interpreter"},{"value":"hive kerberos zeppelin zeppelin-notebook"}],[{"value":"data ingestion streaming"},{"value":"data ingestion streaming"},{"value":"sqoop job too slow importing data from teradata to hive table"},{"value":"hive performance sqoop teradata"}],[{"value":"security"},{"value":"data processing"},{"value":"where to host api within hadoop for knox query"},{"value":"api hadoop knox server"}],[{"value":"hadoop core"},{"value":"hadoop core"},{"value":"capacityscheduler job priority preemption"},{"value":"yarn yarn-scheduler"}],[{"value":"data processing"},{"value":"security"},{"value":"webhcat returns http error not found"},{"value":"hdfs http url webhcat webhdfs"}],[{"value":"sandbox learning"},{"value":"cloud operations"},{"value":"hi have installed sandbox and able to login to sandbox but when issue hadoop command it is giving me connectionexception can you please let me know is there any firewall setting or do include ip details any where"},{"value":"sandbox"}],[{"value":"data processing"},{"value":"data processing"},{"value":"todate java lang illegalargumentexception invalid format is malformed"},{"value":"hive invalid-format pig"}],[{"value":"hadoop core"},{"value":"data processing"},{"value":"running mapreduce program using oozie map reduce action"},{"value":"mapreduce oozie"}],[{"value":"data processing"},{"value":"data processing"},{"value":"how to store json data into hbase"},{"value":"pig"}],[{"value":"hadoop core"},{"value":"data processing"},{"value":"permission denied for user while creating hive table using sqoop in oozie workflow"},{"value":"hdfs-permissions hive oozie permission-denied yarn"}],[{"value":"cloud operations"},{"value":"hadoop core"},{"value":"pig joib failing slf failed to load class org slf impl staticloggerbinder"},{"value":"aws ec pig"}],[{"value":"cloud operations"},{"value":"cloud operations"},{"value":"hdp utils centos rpm dependency problems"},{"value":"centos dependency installation rpm"}],[{"value":"cloud operations"},{"value":"cloud operations"},{"value":"hive settings missing in ambari"},{"value":"ambari hive"}],[{"value":"solutions"},{"value":"hadoop core"},{"value":"how to verify if namenode is good to leave safemode"},{"value":"namenode"}],[{"value":"cloud operations"},{"value":"cloud operations"},{"value":"error with amabari install at conf select py it is expecting an int but receiving possible symlink issue"},{"value":"ambari installation"}],[{"value":"sandbox learning"},{"value":"cloud operations"},{"value":"hive queries in ambari stay in status"},{"value":"ambari help hive"}],[{"value":"data ingestion streaming"},{"value":"data ingestion streaming"},{"value":"what is the difference between google dataflow and hadoop data flow"},{"value":"dataflow google hdf"}],[{"value":"data processing"},{"value":"data processing"},{"value":"hive query stalls on hive cli"},{"value":"hive"}],[{"value":"security"},{"value":"security"},{"value":"enable kerberos error while checking admin admin principal"},{"value":"ambari kdc kerberos"}],[{"value":"cloud operations"},{"value":"cloud operations"},{"value":"how to add datanode heap metric widget in ambari dashboard just like namenode heap"},{"value":"ambari ambari-metrics"}],[{"value":"design architecture"},{"value":"cloud operations"},{"value":"cluster creation in progress issue with ambari"},{"value":"ambari cluster"}],[{"value":"governance lifecycle"},{"value":"data processing"},{"value":"oozie job fails if the history server is down"},{"value":"historyserver mapreduce oozie"}],[{"value":"security"},{"value":"security"},{"value":"can not access hdfs"},{"value":"hdfs kerberos"}],[{"value":"design architecture"},{"value":"security"},{"value":"ranger admin install fails with access denied error"},{"value":"mysql ranger ranger-admin"}],[{"value":"hadoop core"},{"value":"governance lifecycle"},{"value":"how replicate hadoop cluster to hadoop cluster dr using apache falcon"},{"value":"disaster-recovery falcon"}],[{"value":"ds analytics spark"},{"value":"data ingestion streaming"},{"value":"hdp and hana vora"},{"value":"sap-hana"}],[{"value":"ds analytics spark"},{"value":"cloud operations"},{"value":"zeppelin not working on upgrading to hdp"},{"value":"spark upgrade zeppelin"}],[{"value":"cloud operations"},{"value":"cloud operations"},{"value":"ambari agent disk usage alert remedy"},{"value":"ambari ambari-service"}],[{"value":"data processing"},{"value":"data processing"},{"value":"phoenix storage in pig"},{"value":"hive phoenix pig"}],[{"value":"hadoop core"},{"value":"hadoop core"},{"value":"warning use yarn jar to launch yarn applications"},{"value":"yarn"}],[{"value":"data processing"},{"value":"data processing"},{"value":"hive with orc data read from disk for queries same despite very different selectivities"},{"value":"disk hive orc"}],[{"value":"hadoop core"},{"value":"hadoop core"},{"value":"how do you get kerberos delegation token in high availability cluster"},{"value":"high-availability kerberos webhdfs"}],[{"value":"cloud operations"},{"value":"cloud operations"},{"value":"how to change password for ambari db post install"},{"value":"ambari"}],[{"value":"design architecture"},{"value":"design architecture"},{"value":"in production level ha cluster do we need dedicated servers for journalnodes and zookepers"},{"value":"architecture hdfs"}],[{"value":"governance lifecycle"},{"value":"data ingestion streaming"},{"value":"strategies for archiving raw input data"},{"value":"archive compression falcon"}],[{"value":"security"},{"value":"security"},{"value":"on an unkerborized cluster how can get ranger solr to work can get solr ranger working without kerberos"},{"value":"audit kerberos ranger solr"}],[{"value":"data processing"},{"value":"data processing"},{"value":"created phoenix view to map existing hbase table but got different float value on row"},{"value":"hbase phoenix"}],[{"value":"design architecture"},{"value":"cloud operations"},{"value":"need list of component name groupid artifactid and version required to access each hdp artifactid"},{"value":"hdp maven repository"}],[{"value":"hadoop core"},{"value":"cloud operations"},{"value":"which oozie metrics should be monitored"},{"value":"ambari-metrics hdp monitoring oozie"}],[{"value":"governance lifecycle"},{"value":"cloud operations"},{"value":"oozie service check fails after moving oozie server"},{"value":"ambari oozie"}],[{"value":"sandbox learning"},{"value":"sandbox learning"},{"value":"vmware workstation connectivity issue detected"},{"value":"connection vmware"}],[{"value":"data processing"},{"value":"cloud operations"},{"value":"do hbase and hdfs need to be co located on the same machines if so how much"},{"value":"architecture hbase hdfs"}],[{"value":"data processing"},{"value":"data ingestion streaming"},{"value":"hbase shell throws exception at startup"},{"value":"edge hbase"}],[{"value":"governance lifecycle"},{"value":"data ingestion streaming"},{"value":"free form query in sqoop import with where clause"},{"value":"sqoop"}],[{"value":"cloud operations"},{"value":"cloud operations"},{"value":"ambari settings after database restore"},{"value":"ambari database"}],[{"value":"data processing"},{"value":"data ingestion streaming"},{"value":"split csv and use columns as avro fields"},{"value":"avro nifi nifi-processor"}],[{"value":"data processing"},{"value":"hadoop core"},{"value":"error yarn"},{"value":"jmx yarn"}],[{"value":"solutions"},{"value":"data processing"},{"value":"ranger for phoenix"},{"value":"hbase phoenix ranger"}],[{"value":"data processing"},{"value":"hadoop core"},{"value":"how to enable namenode ha using ambari"},{"value":"ambari ha namenode"}],[{"value":"security"},{"value":"cloud operations"},{"value":"does phoenix allow role creation"},{"value":"hbase phoenix"}],[{"value":"hadoop core"},{"value":"data processing"},{"value":"number of distcp mappers is small why"},{"value":"discp mapreduce"}],[{"value":"sandbox learning"},{"value":"data processing"},{"value":"am stupid or does anyone else have constant issues settings up projects in the hadoop ecosystem"},{"value":"hadoop-ecosystem hive training"}],[{"value":"data processing"},{"value":"data processing"},{"value":"creating indexes in hive"},{"value":"hive"}],[{"value":"data ingestion streaming"},{"value":"data ingestion streaming"},{"value":"stream data from web site using nifi"},{"value":"nifi windows"}],[{"value":"data ingestion streaming"},{"value":"data ingestion streaming"},{"value":"how to add content type application json to invokehttp"},{"value":"invokehttp"}],[{"value":"data ingestion streaming"},{"value":"data ingestion streaming"},{"value":"kafka spark streaming unable to get any messages"},{"value":"kafka spark-streaming"}],[{"value":"data processing"},{"value":"ds analytics spark"},{"value":"how to use with spark"},{"value":"python spark"}],[{"value":"governance lifecycle"},{"value":"cloud operations"},{"value":"falcon doesn start up hdp ambari"},{"value":"falcon"}],[{"value":"ds analytics spark"},{"value":"data processing"},{"value":"submit spark applition to kerberised hdp but authentication failed"},{"value":"spark yarn"}],[{"value":"data ingestion streaming"},{"value":"data ingestion streaming"},{"value":"not able to read tweets format never seen such behaviour flume"},{"value":"flume"}],[{"value":"data processing"},{"value":"data processing"},{"value":"sequence number generation in hive"},{"value":"hive"}],[{"value":"hadoop core"},{"value":"cloud operations"},{"value":"status code received on get method for api api stacks hdp versions recommendations"},{"value":"ambari"}],[{"value":"data processing"},{"value":"data processing"},{"value":"classcastexception while loading decimal data from avro schema backed parquet file"},{"value":"avro hive parquet"}],[{"value":"security"},{"value":"security"},{"value":"ranger hive policy question"},{"value":"ranger"}],[{"value":"sandbox learning"},{"value":"cloud operations"},{"value":"what are all the best possible ways to install single node cluster setup in laptop appreciate your inputs on this"},{"value":"installation"}],[{"value":"sandbox learning"},{"value":"cloud operations"},{"value":"yum install doesn work couldn resolve host centos org couldn know the problem exactly but need to install hue"},{"value":"installation"}],[{"value":"data processing"},{"value":"data processing"},{"value":"genericudf not returning data"},{"value":"hive"}],[{"value":"design architecture"},{"value":"cloud operations"},{"value":"ambari dashboard logs"},{"value":"ambari dashboard logs"}],[{"value":"data processing"},{"value":"data processing"},{"value":"hbase replication identify and fix impacted records"},{"value":"hbase replication"}],[{"value":"cloud operations"},{"value":"cloud operations"},{"value":"while registering node with ambariserver am getting the following error can someone please help me to fix the issue"},{"value":"ambari"}],[{"value":"data ingestion streaming"},{"value":"data ingestion streaming"},{"value":"how to extract data from mysql to oracle using apache nifi are there some demos"},{"value":"nifi"}],[{"value":"security"},{"value":"security"},{"value":"how the client can access kafka through knox"},{"value":"knox-gateway"}],[{"value":"data processing"},{"value":"data ingestion streaming"},{"value":"options for decompressing hdfs data using pig"},{"value":"compression pig"}],[{"value":"cloud operations"},{"value":"cloud operations"},{"value":"ambari start service failed with error"},{"value":"ambari"}],[{"value":"cloud operations"},{"value":"cloud operations"},{"value":"ambari only install component through yum other method is have"},{"value":"ambari installer"}],[{"value":"ds analytics spark"},{"value":"ds analytics spark"},{"value":"when trying to run zeppelin for the first time get this error any help please"},{"value":"spark zeppelin-notebook"}],[{"value":"data processing"},{"value":"data processing"},{"value":"how to increment the hive external table"},{"value":"hive sqoop"}],[{"value":"solutions"},{"value":"data processing"},{"value":"hive hdfs for new records"},{"value":"hivecontext"}],[{"value":"data processing"},{"value":"data processing"},{"value":"hive insert getting failed"},{"value":"hiveserver"}],[{"value":"data processing"},{"value":"data processing"},{"value":"hive benchmarking error tpcds_kit zip issue"},{"value":"help hive"}],[{"value":"data processing"},{"value":"data processing"},{"value":"pig map reduce permission denied after history server enable"},{"value":"mapreduce pig"}],[{"value":"hadoop core"},{"value":"data processing"},{"value":"how to get the value in core site xml in oozie java action"},{"value":"oozie"}],[{"value":"sandbox learning"},{"value":"data processing"},{"value":"pig queries to analyse the data"},{"value":"tutorial"}],[{"value":"data ingestion streaming"},{"value":"data processing"},{"value":"hadoop exposing sql interface on collection of csv files with variable columns"},{"value":"hadoop-ecosystem hive sql"}],[{"value":"data processing"},{"value":"data processing"},{"value":"hive query performance via jdbc java"},{"value":"hive"}],[{"value":"data ingestion streaming"},{"value":"data ingestion streaming"},{"value":"putkafka getkafka nifi and avro format"},{"value":"avro kafka nifi"}],[{"value":"ds analytics spark"},{"value":"cloud operations"},{"value":"zeppelin offline installation"},{"value":"installation installer zeppelin"}],[{"value":"cloud operations"},{"value":"hadoop core"},{"value":"what are best practices for patching worker nodes"},{"value":"datanode nodemanager regionserver"}],[{"value":"ds analytics spark"},{"value":"data processing"},{"value":"create different schemas at run time for different data frames"},{"value":"dataframe java schema spark spark-sql"}],[{"value":"governance lifecycle"},{"value":"governance lifecycle"},{"value":"how to navigate within atlas how to create tags search etc"},{"value":"answerhub atlas hcc help"}],[{"value":"cloud operations"},{"value":"cloud operations"},{"value":"where can find the steps to modify configuration with ambari rest for cluster that is spinning several hosts"},{"value":"ambari"}],[{"value":"design architecture"},{"value":"cloud operations"},{"value":"hbase rootdir in ambari metrics configuration"},{"value":"ambari hdp"}]],"columnNames":[{"name":"originalTrack","index":0,"aggr":"sum"},{"name":"predictedTrack","index":1,"aggr":"sum"},{"name":"title","index":2,"aggr":"sum"},{"name":"tags","index":3,"aggr":"sum"}],"rows":[["data ingestion streaming","data ingestion streaming","how to connect storm to hbase","hbase storm"],["data processing","data processing","hive cli unresponsive","cli hive"],["cloud operations","cloud operations","hbase regionservers won start after cloudbreak install in aws","ambari aws cloudbreak regionserver"],["sandbox learning","cloud operations","usr hdp current hadoop client conf doesn exist error","ambari"],["data processing","hadoop core","datastage hadoop","datastage"],["sandbox learning","sandbox learning","permission denied when removing file from hdfs from sandbox","hdfs-permissions permission-denied rm"],["data processing","data processing","am getting execution error return code from org apache hadoop hive ql exec tez teztask","error help sandbox"],["design architecture","data processing","lucidworks hdpsearch solr installation","hdpsearch lucidworks solr"],["security","ds analytics spark","zeppelin support for kerberos in hive interpreter","hive kerberos zeppelin zeppelin-notebook"],["data ingestion streaming","data ingestion streaming","sqoop job too slow importing data from teradata to hive table","hive performance sqoop teradata"],["security","data processing","where to host api within hadoop for knox query","api hadoop knox server"],["hadoop core","hadoop core","capacityscheduler job priority preemption","yarn yarn-scheduler"],["data processing","security","webhcat returns http error not found","hdfs http url webhcat webhdfs"],["sandbox learning","cloud operations","hi have installed sandbox and able to login to sandbox but when issue hadoop command it is giving me connectionexception can you please let me know is there any firewall setting or do include ip details any where","sandbox"],["data processing","data processing","todate java lang illegalargumentexception invalid format is malformed","hive invalid-format pig"],["hadoop core","data processing","running mapreduce program using oozie map reduce action","mapreduce oozie"],["data processing","data processing","how to store json data into hbase","pig"],["hadoop core","data processing","permission denied for user while creating hive table using sqoop in oozie workflow","hdfs-permissions hive oozie permission-denied yarn"],["cloud operations","hadoop core","pig joib failing slf failed to load class org slf impl staticloggerbinder","aws ec pig"],["cloud operations","cloud operations","hdp utils centos rpm dependency problems","centos dependency installation rpm"],["cloud operations","cloud operations","hive settings missing in ambari","ambari hive"],["solutions","hadoop core","how to verify if namenode is good to leave safemode","namenode"],["cloud operations","cloud operations","error with amabari install at conf select py it is expecting an int but receiving possible symlink issue","ambari installation"],["sandbox learning","cloud operations","hive queries in ambari stay in status","ambari help hive"],["data ingestion streaming","data ingestion streaming","what is the difference between google dataflow and hadoop data flow","dataflow google hdf"],["data processing","data processing","hive query stalls on hive cli","hive"],["security","security","enable kerberos error while checking admin admin principal","ambari kdc kerberos"],["cloud operations","cloud operations","how to add datanode heap metric widget in ambari dashboard just like namenode heap","ambari ambari-metrics"],["design architecture","cloud operations","cluster creation in progress issue with ambari","ambari cluster"],["governance lifecycle","data processing","oozie job fails if the history server is down","historyserver mapreduce oozie"],["security","security","can not access hdfs","hdfs kerberos"],["design architecture","security","ranger admin install fails with access denied error","mysql ranger ranger-admin"],["hadoop core","governance lifecycle","how replicate hadoop cluster to hadoop cluster dr using apache falcon","disaster-recovery falcon"],["ds analytics spark","data ingestion streaming","hdp and hana vora","sap-hana"],["ds analytics spark","cloud operations","zeppelin not working on upgrading to hdp","spark upgrade zeppelin"],["cloud operations","cloud operations","ambari agent disk usage alert remedy","ambari ambari-service"],["data processing","data processing","phoenix storage in pig","hive phoenix pig"],["hadoop core","hadoop core","warning use yarn jar to launch yarn applications","yarn"],["data processing","data processing","hive with orc data read from disk for queries same despite very different selectivities","disk hive orc"],["hadoop core","hadoop core","how do you get kerberos delegation token in high availability cluster","high-availability kerberos webhdfs"],["cloud operations","cloud operations","how to change password for ambari db post install","ambari"],["design architecture","design architecture","in production level ha cluster do we need dedicated servers for journalnodes and zookepers","architecture hdfs"],["governance lifecycle","data ingestion streaming","strategies for archiving raw input data","archive compression falcon"],["security","security","on an unkerborized cluster how can get ranger solr to work can get solr ranger working without kerberos","audit kerberos ranger solr"],["data processing","data processing","created phoenix view to map existing hbase table but got different float value on row","hbase phoenix"],["design architecture","cloud operations","need list of component name groupid artifactid and version required to access each hdp artifactid","hdp maven repository"],["hadoop core","cloud operations","which oozie metrics should be monitored","ambari-metrics hdp monitoring oozie"],["governance lifecycle","cloud operations","oozie service check fails after moving oozie server","ambari oozie"],["sandbox learning","sandbox learning","vmware workstation connectivity issue detected","connection vmware"],["data processing","cloud operations","do hbase and hdfs need to be co located on the same machines if so how much","architecture hbase hdfs"],["data processing","data ingestion streaming","hbase shell throws exception at startup","edge hbase"],["governance lifecycle","data ingestion streaming","free form query in sqoop import with where clause","sqoop"],["cloud operations","cloud operations","ambari settings after database restore","ambari database"],["data processing","data ingestion streaming","split csv and use columns as avro fields","avro nifi nifi-processor"],["data processing","hadoop core","error yarn","jmx yarn"],["solutions","data processing","ranger for phoenix","hbase phoenix ranger"],["data processing","hadoop core","how to enable namenode ha using ambari","ambari ha namenode"],["security","cloud operations","does phoenix allow role creation","hbase phoenix"],["hadoop core","data processing","number of distcp mappers is small why","discp mapreduce"],["sandbox learning","data processing","am stupid or does anyone else have constant issues settings up projects in the hadoop ecosystem","hadoop-ecosystem hive training"],["data processing","data processing","creating indexes in hive","hive"],["data ingestion streaming","data ingestion streaming","stream data from web site using nifi","nifi windows"],["data ingestion streaming","data ingestion streaming","how to add content type application json to invokehttp","invokehttp"],["data ingestion streaming","data ingestion streaming","kafka spark streaming unable to get any messages","kafka spark-streaming"],["data processing","ds analytics spark","how to use with spark","python spark"],["governance lifecycle","cloud operations","falcon doesn start up hdp ambari","falcon"],["ds analytics spark","data processing","submit spark applition to kerberised hdp but authentication failed","spark yarn"],["data ingestion streaming","data ingestion streaming","not able to read tweets format never seen such behaviour flume","flume"],["data processing","data processing","sequence number generation in hive","hive"],["hadoop core","cloud operations","status code received on get method for api api stacks hdp versions recommendations","ambari"],["data processing","data processing","classcastexception while loading decimal data from avro schema backed parquet file","avro hive parquet"],["security","security","ranger hive policy question","ranger"],["sandbox learning","cloud operations","what are all the best possible ways to install single node cluster setup in laptop appreciate your inputs on this","installation"],["sandbox learning","cloud operations","yum install doesn work couldn resolve host centos org couldn know the problem exactly but need to install hue","installation"],["data processing","data processing","genericudf not returning data","hive"],["design architecture","cloud operations","ambari dashboard logs","ambari dashboard logs"],["data processing","data processing","hbase replication identify and fix impacted records","hbase replication"],["cloud operations","cloud operations","while registering node with ambariserver am getting the following error can someone please help me to fix the issue","ambari"],["data ingestion streaming","data ingestion streaming","how to extract data from mysql to oracle using apache nifi are there some demos","nifi"],["security","security","how the client can access kafka through knox","knox-gateway"],["data processing","data ingestion streaming","options for decompressing hdfs data using pig","compression pig"],["cloud operations","cloud operations","ambari start service failed with error","ambari"],["cloud operations","cloud operations","ambari only install component through yum other method is have","ambari installer"],["ds analytics spark","ds analytics spark","when trying to run zeppelin for the first time get this error any help please","spark zeppelin-notebook"],["data processing","data processing","how to increment the hive external table","hive sqoop"],["solutions","data processing","hive hdfs for new records","hivecontext"],["data processing","data processing","hive insert getting failed","hiveserver"],["data processing","data processing","hive benchmarking error tpcds_kit zip issue","help hive"],["data processing","data processing","pig map reduce permission denied after history server enable","mapreduce pig"],["hadoop core","data processing","how to get the value in core site xml in oozie java action","oozie"],["sandbox learning","data processing","pig queries to analyse the data","tutorial"],["data ingestion streaming","data processing","hadoop exposing sql interface on collection of csv files with variable columns","hadoop-ecosystem hive sql"],["data processing","data processing","hive query performance via jdbc java","hive"],["data ingestion streaming","data ingestion streaming","putkafka getkafka nifi and avro format","avro kafka nifi"],["ds analytics spark","cloud operations","zeppelin offline installation","installation installer zeppelin"],["cloud operations","hadoop core","what are best practices for patching worker nodes","datanode nodemanager regionserver"],["ds analytics spark","data processing","create different schemas at run time for different data frames","dataframe java schema spark spark-sql"],["governance lifecycle","governance lifecycle","how to navigate within atlas how to create tags search etc","answerhub atlas hcc help"],["cloud operations","cloud operations","where can find the steps to modify configuration with ambari rest for cluster that is spinning several hosts","ambari"],["design architecture","cloud operations","hbase rootdir in ambari metrics configuration","ambari hdp"]]},"dateCreated":"2016-12-12T05:34:08-0800","dateStarted":"2016-12-13T19:25:32-0800","dateFinished":"2016-12-13T19:25:32-0800","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:530"},{"title":"Optional: Save model","text":"%pyspark\n\nmodel_path = \"/path/to/my/model\"\n\n#model.save(model_path)","dateUpdated":"2016-12-12T21:31:03-0800","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1481592848028_922459334","id":"20161115-235419_286947947","dateCreated":"2016-12-12T05:34:08-0800","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:531"}],"name":"HCC Track Prediction","id":"2C5MSKN7C","angularObjects":{"2C17KRZ89:shared_process":[],"2BYY922YJ:shared_process":[],"2BZFM3RS5:shared_process":[],"2BWWMQD15:shared_process":[],"2BX9JK9RG:shared_process":[],"2BYS6EPN8:shared_process":[],"2BXM16U2W:shared_process":[],"2BZKN45ZE:shared_process":[],"2BYJTTQ3C:shared_process":[],"2BY4FUMA6:shared_process":[],"2BX12UTS8:shared_process":[],"2BXFZDPB4:shared_process":[],"2BXPTJA1N:shared_process":[],"2BYHZF59F:shared_process":[],"2BZ33SWBS:shared_process":[],"2BZ233MCE:shared_process":[],"2BXTNQ9NR:shared_process":[],"2BZB9FFTE:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}