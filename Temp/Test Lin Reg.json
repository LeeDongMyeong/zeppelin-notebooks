{"paragraphs":[{"text":"%sh\n\nwget https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_linear_regression_data.txt -O /tmp/lin_reg_data.txt","dateUpdated":"2016-10-19T12:00:31+0200","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476871231093_1328756091","id":"20161018-192055_78948884","result":{"code":"SUCCESS","type":"TEXT","msg":"--2016-10-18 19:24:07--  https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_linear_regression_data.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.36.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.36.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 119069 (116K) [text/plain]\nSaving to: ‘/tmp/lin_reg_data.txt’\n\n     0K .......... .......... .......... .......... .......... 43%  254K 0s\n    50K .......... .......... .......... .......... .......... 86% 1,06M 0s\n   100K .......... ......                                     100% 3,38M=0,2s\n\n2016-10-18 19:24:08 (470 KB/s) - ‘/tmp/lin_reg_data.txt’ saved [119069/119069]\n\n"},"dateCreated":"2016-10-19T12:00:31+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2879"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476900823505_-1217286375","id":"20161019-201343_654113105","dateCreated":"2016-10-19T08:13:43+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3493","text":"%md\n\n## Train one feature\n#### Scatter plot x,y","dateUpdated":"2016-10-19T08:15:01+0200","dateFinished":"2016-10-19T08:14:58+0200","dateStarted":"2016-10-19T08:14:58+0200","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Train one feature</h2>\n<h4>Scatter plot x,y</h4>\n"}},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476903799173_530885101","id":"20161019-210319_1007000956","dateCreated":"2016-10-19T21:03:19+0200","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3858","text":"%md\n\nScatter Data Points\n\nTODO: Create a table\n\n-12, -4.9\n-6, -4.5\n-7.2, -4.1\n-5, -3.2\n-2, -3.0\n-3.1, -2.1\n-4, -1.5\n-2.2, -1.2\n-2, -0.7\n1,  -0.5\n-0.7, -0.2\n1.2, 0.1\n2.2, 0.3\n6.5, 0.52\n4.2, 0.72\n8.6, 1.1\n9.5, 2.3\n14.52, 3.4\n12.9, 3.61\n16.3, 3.8","dateUpdated":"2016-10-19T21:03:50+0200"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476900926386_-102330035","id":"20161019-201526_2057433338","dateCreated":"2016-10-19T08:15:26+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3584","text":"import org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.ml.linalg.Vectors\n\nval scatterData = spark.createDataFrame(Seq(\n(-12.0,  Vectors.dense(-4.9)),\n( -6.0,  Vectors.dense(-4.5)),\n( -7.2,  Vectors.dense(-4.1)),\n( -5.0,  Vectors.dense(-3.2)),\n( -2.0,  Vectors.dense(-3.0)),\n( -3.1,  Vectors.dense(-2.1)),\n( -4.0,  Vectors.dense(-1.5)),\n( -2.2,  Vectors.dense(-1.2)),\n( -2.0,  Vectors.dense(-0.7)),\n( 1.0,   Vectors.dense(-0.5)),\n( -0.7,  Vectors.dense(-0.2)),\n( 1.2,   Vectors.dense(0.1)),\n( 2.2,   Vectors.dense(0.3)),  \n( 6.5,   Vectors.dense(0.52)),\n( 4.2,   Vectors.dense(0.72)),\n( 8.6,   Vectors.dense(1.1)),\n( 9.5,   Vectors.dense(2.3)),\n( 14.52, Vectors.dense(3.4)),\n( 12.9,  Vectors.dense(3.61)), \n( 16.3,  Vectors.dense(3.8))\n)).toDF(\"label\", \"features\")\n\nval lr1 = new LinearRegression()\n  .setMaxIter(10)\n  .setRegParam(0.3)\n  .setElasticNetParam(0.8)\n\n// Fit the model\nval lrModel1 = lr1.fit(scatterData)\n\n// Print the coefficients and intercept for linear regression\nprintln(s\"Coefficients: ${lrModel1.coefficients} Intercept: ${lrModel1.intercept}\")","dateUpdated":"2016-10-19T08:59:57+0200","dateFinished":"2016-10-19T09:00:01+0200","dateStarted":"2016-10-19T08:59:57+0200","result":{"code":"SUCCESS","type":"TEXT","msg":"\nimport org.apache.spark.ml.regression.LinearRegression\n\nimport org.apache.spark.ml.linalg.Vectors\n\nscatterData: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n\nlr1: org.apache.spark.ml.regression.LinearRegression = linReg_bb1a6154746b\n\nlrModel1: org.apache.spark.ml.regression.LinearRegressionModel = linReg_bb1a6154746b\nCoefficients: [2.6872513028109535] Intercept: 2.9863437796625045\n"}},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476900941922_-2092620771","id":"20161019-201541_1596990071","dateCreated":"2016-10-19T08:15:41+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3654","text":"%md\n## Train one feature\n#### Ideal function: y = -3x + 4","dateUpdated":"2016-10-19T08:16:21+0200","dateFinished":"2016-10-19T08:16:16+0200","dateStarted":"2016-10-19T08:16:16+0200","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Train one feature</h2>\n<h4>Ideal function: y = -3x + 4</h4>\n"}},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476900987169_-1948496464","id":"20161019-201627_1680508917","dateCreated":"2016-10-19T08:16:27+0200","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3733","text":"","dateUpdated":"2016-10-19T08:16:37+0200"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476900661795_1030787769","id":"20161019-201101_1604210745","dateCreated":"2016-10-19T08:11:01+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3393","text":"%md\n\n## Train two features\n#### Ideal Plane: y = 2x + 4y + 3","dateUpdated":"2016-10-19T08:14:46+0200","dateFinished":"2016-10-19T08:14:45+0200","dateStarted":"2016-10-19T08:14:44+0200","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Train two features</h2>\n<h4>Ideal Plane: y = 2x + 4y + 3</h4>\n"}},{"text":"import org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.ml.linalg.Vectors\n\nval dataDF = spark.createDataFrame(Seq(\n  (3.0, Vectors.dense(0.0, 0.0)),\n  (5.0, Vectors.dense(1.0, 0.0)),\n  (7.0, Vectors.dense(0.0, 1.0)),\n  (17.0, Vectors.dense(3.0, 2.0)),\n  (27.0, Vectors.dense(4.0, 4.0))\n)).toDF(\"label\", \"features\")\n\n// Load training data\n//val training = spark.read.format(\"libsvm\").load(\"file:///tmp/lin_reg_data.txt\")\n\nval lr = new LinearRegression()\n  .setMaxIter(10)\n  .setRegParam(0.3)\n  .setElasticNetParam(0.8)\n\n// Fit the model\nval lrModel = lr.fit(dataDF)\n\n// Print the coefficients and intercept for linear regression\nprintln(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")\n\n// Summarize the model over the training set and print out some metrics\nval trainingSummary = lrModel.summary\nprintln(s\"numIterations: ${trainingSummary.totalIterations}\")\nprintln(s\"objectiveHistory: ${trainingSummary.objectiveHistory.toList}\")\n\n\n\n// TOOD: Make as a seperate paragraph\n//trainingSummary.residuals.show()\nprintln(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")\nprintln(s\"r2: ${trainingSummary.r2}\")","dateUpdated":"2016-10-19T08:23:24+0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476872430180_-1978649435","id":"20161019-122030_199790599","result":{"code":"SUCCESS","type":"TEXT","msg":"\nimport org.apache.spark.ml.regression.LinearRegression\n\nimport org.apache.spark.ml.linalg.Vectors\n\ndataDF: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n\nlr: org.apache.spark.ml.regression.LinearRegression = linReg_4eecb32299b8\n\nlrModel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_4eecb32299b8\nCoefficients: [1.9543766985791324,3.836109869301599] Intercept: 3.30244346525115\n\ntrainingSummary: org.apache.spark.ml.regression.LinearRegressionTrainingSummary = org.apache.spark.ml.regression.LinearRegressionTrainingSummary@a24b9a5\nnumIterations: 6\nobjectiveHistory: List(0.3999999999999999, 0.2996804732481419, 0.027207606211397152, 0.02683848656579744, 0.025712347684262244, 0.02571234768287102)\nRMSE: 0.31299042811846567\nr2: 0.9987899826075373\n"},"dateCreated":"2016-10-19T12:20:30+0200","dateStarted":"2016-10-19T08:12:12+0200","dateFinished":"2016-10-19T08:12:18+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2880","focus":true},{"text":"trainingSummary.residuals.agg(sum(\"residuals\")).first.get(0)","dateUpdated":"2016-10-19T12:50:34+0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476873963173_1584888824","id":"20161019-124603_1704953562","result":{"code":"SUCCESS","type":"TEXT","msg":"\nres20: Any = -4.440892098500626E-15\n"},"dateCreated":"2016-10-19T12:46:03+0200","dateStarted":"2016-10-19T12:50:34+0200","dateFinished":"2016-10-19T12:50:35+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2881"},{"text":"// Ideal: z = 2x + 4y + 3\n\nval dataDF2 = spark.createDataFrame(Seq(\n  (9.0, Vectors.dense(1.0,1.0)),\n  (5.0, Vectors.dense(-1.0,1.0)),\n  (9.0, Vectors.dense(2.0,0.5)),\n  (5.0, Vectors.dense(-3.0,2.0)),\n  (21.0, Vectors.dense(3.0,3.0))\n)).toDF(\"expected\", \"features\")\n\nval predictions = lrModel.transform(dataDF2)\n\npredictions.select($\"expected\", $\"features\", $\"prediction\" as \"predicted\").show()","dateUpdated":"2016-10-19T08:15:36+0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476875209189_1084537941","id":"20161019-130649_1383501907","result":{"code":"SUCCESS","type":"TEXT","msg":"\ndataDF2: org.apache.spark.sql.DataFrame = [expected: double, features: vector]\n\npredictions: org.apache.spark.sql.DataFrame = [expected: double, features: vector ... 1 more field]\n+--------+----------+------------------+\n|expected|  features|         predicted|\n+--------+----------+------------------+\n|     9.0| [1.0,1.0]| 9.092930033131882|\n|     5.0|[-1.0,1.0]| 5.184176635973617|\n|     9.0| [2.0,0.5]| 9.129251797060213|\n|     5.0|[-3.0,2.0]| 5.111533108116951|\n|    21.0| [3.0,3.0]|20.673903168893347|\n+--------+----------+------------------+\n\n"},"dateCreated":"2016-10-19T01:06:49+0200","dateStarted":"2016-10-19T08:13:04+0200","dateFinished":"2016-10-19T08:13:05+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2882","title":"Predict two features and intercept","focus":true},{"text":"import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n\n// Make predictions.                                                                                                     \nval predictions = lrModel.transform(trainingData)\n\n// Select example rows to display.                                                                                       \nval subset = predictions.select(\"prediction\", \"label\", \"features\")\n\nval evaluator = new MulticlassClassificationEvaluator()\n      .setLabelCol(\"label\")                                                                                           \n      .setPredictionCol(\"prediction\")\n      .setMetricName(\"accuracy\")\n      \nval accuracy = evaluator.evaluate(predictions)                                                                                      \nprintln(\"Accuracy: \" + accuracy)  \n\npredictions.show(20)\n\npredictions.createOrReplaceTempView(\"predictions\")\n\n//subset.createOrReplaceTempView(\"predictions\")","dateUpdated":"2016-10-19T12:19:36+0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476871231094_1329910337","id":"20161018-192414_292089231","result":{"code":"ERROR","type":"TEXT","msg":"\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n\npredictions: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 3 more fields]\n\nsubset: org.apache.spark.sql.DataFrame = [prediction: double, label: double ... 1 more field]\n\nevaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_e200d6f0d523\n\naccuracy: Double = 0.0\nAccuracy: 0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 669.0 failed 1 times, most recent failure: Lost task 0.0 in stage 669.0 (TID 1062, localhost): org.apache.spark.SparkException: Unseen index: -3.030992964896073 ??\n\tat org.apache.spark.ml.feature.IndexToString$$anonfun$6.apply(StringIndexer.scala:321)\n\tat org.apache.spark.ml.feature.IndexToString$$anonfun$6.apply(StringIndexer.scala:316)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:246)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:347)\n  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:39)\n  at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2183)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n  at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2532)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2182)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2189)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1925)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1924)\n  at org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2562)\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:1924)\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:2139)\n  at org.apache.spark.sql.Dataset.showString(Dataset.scala:239)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:526)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:486)\n  ... 46 elided\nCaused by: org.apache.spark.SparkException: Unseen index: -3.030992964896073 ??\n  at org.apache.spark.ml.feature.IndexToString$$anonfun$6.apply(StringIndexer.scala:321)\n  at org.apache.spark.ml.feature.IndexToString$$anonfun$6.apply(StringIndexer.scala:316)\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:246)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240)\n  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)\n  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n  at org.apache.spark.scheduler.Task.run(Task.scala:85)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n  ... 3 more\n"},"dateCreated":"2016-10-19T12:00:31+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2883"},{"text":"%sql\n\nselect label, predictedLabel from predictions limit 20","dateUpdated":"2016-10-19T12:00:31+0200","config":{"colWidth":12,"editorMode":"ace/mode/sql","tableHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"label","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"label","index":0,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476871231094_1329910337","id":"20161018-192721_987200724","result":{"code":"SUCCESS","type":"TABLE","msg":"label\tpredictedLabel\n-28.571478869743427\t-28.046018037776633\n-28.046018037776633\t-28.046018037776633\n-23.487440120936512\t-28.046018037776633\n-22.949825936196074\t-28.046018037776633\n-21.432387764165806\t-28.046018037776633\n-20.212077258958672\t5.004048239623824\n-20.057482615789212\t-28.046018037776633\n-19.872991038068406\t5.004048239623824\n-19.66731861537172\t-28.046018037776633\n-19.402336030214553\t5.004048239623824\n-18.845922472898582\t5.004048239623824\n-17.494200356883344\t-28.046018037776633\n-17.428674570939506\t-28.046018037776633\n-17.32672073267595\t-28.046018037776633\n-17.065399625876015\t-28.046018037776633\n-17.026492264209548\t-28.046018037776633\n-16.71909683360509\t-28.046018037776633\n-16.692207021311106\t-28.046018037776633\n-16.26143027545273\t5.004048239623824\n-16.151349351277112\t-28.046018037776633\n","comment":"","msgTable":[[{"key":"predictedLabel","value":"-28.571478869743427"},{"key":"predictedLabel","value":"-28.046018037776633"}],[{"value":"-28.046018037776633"},{"value":"-28.046018037776633"}],[{"value":"-23.487440120936512"},{"value":"-28.046018037776633"}],[{"value":"-22.949825936196074"},{"value":"-28.046018037776633"}],[{"value":"-21.432387764165806"},{"value":"-28.046018037776633"}],[{"value":"-20.212077258958672"},{"value":"5.004048239623824"}],[{"value":"-20.057482615789212"},{"value":"-28.046018037776633"}],[{"value":"-19.872991038068406"},{"value":"5.004048239623824"}],[{"value":"-19.66731861537172"},{"value":"-28.046018037776633"}],[{"value":"-19.402336030214553"},{"value":"5.004048239623824"}],[{"value":"-18.845922472898582"},{"value":"5.004048239623824"}],[{"value":"-17.494200356883344"},{"value":"-28.046018037776633"}],[{"value":"-17.428674570939506"},{"value":"-28.046018037776633"}],[{"value":"-17.32672073267595"},{"value":"-28.046018037776633"}],[{"value":"-17.065399625876015"},{"value":"-28.046018037776633"}],[{"value":"-17.026492264209548"},{"value":"-28.046018037776633"}],[{"value":"-16.71909683360509"},{"value":"-28.046018037776633"}],[{"value":"-16.692207021311106"},{"value":"-28.046018037776633"}],[{"value":"-16.26143027545273"},{"value":"5.004048239623824"}],[{"value":"-16.151349351277112"},{"value":"-28.046018037776633"}]],"columnNames":[{"name":"label","index":0,"aggr":"sum"},{"name":"predictedLabel","index":1,"aggr":"sum"}],"rows":[["-28.571478869743427","-28.046018037776633"],["-28.046018037776633","-28.046018037776633"],["-23.487440120936512","-28.046018037776633"],["-22.949825936196074","-28.046018037776633"],["-21.432387764165806","-28.046018037776633"],["-20.212077258958672","5.004048239623824"],["-20.057482615789212","-28.046018037776633"],["-19.872991038068406","5.004048239623824"],["-19.66731861537172","-28.046018037776633"],["-19.402336030214553","5.004048239623824"],["-18.845922472898582","5.004048239623824"],["-17.494200356883344","-28.046018037776633"],["-17.428674570939506","-28.046018037776633"],["-17.32672073267595","-28.046018037776633"],["-17.065399625876015","-28.046018037776633"],["-17.026492264209548","-28.046018037776633"],["-16.71909683360509","-28.046018037776633"],["-16.692207021311106","-28.046018037776633"],["-16.26143027545273","5.004048239623824"],["-16.151349351277112","-28.046018037776633"]]},"dateCreated":"2016-10-19T12:00:31+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2884"},{"text":"","dateUpdated":"2016-10-19T12:00:31+0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476871231094_1329910337","id":"20161019-115552_606892688","dateCreated":"2016-10-19T12:00:31+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2885"}],"name":"Test Lin Reg 4","id":"2C1CT873E","angularObjects":{"2C17KRZ89:shared_process":[],"2BYY922YJ:shared_process":[],"2BZFM3RS5:shared_process":[],"2BWWMQD15:shared_process":[],"2BX9JK9RG:shared_process":[],"2BYS6EPN8:shared_process":[],"2BXM16U2W:shared_process":[],"2BZKN45ZE:shared_process":[],"2BYJTTQ3C:shared_process":[],"2BY4FUMA6:shared_process":[],"2BX12UTS8:shared_process":[],"2BXFZDPB4:shared_process":[],"2BXPTJA1N:shared_process":[],"2BYHZF59F:shared_process":[],"2BZ33SWBS:shared_process":[],"2BZ233MCE:shared_process":[],"2BXTNQ9NR:shared_process":[],"2BZB9FFTE:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}