{"paragraphs":[{"text":"%md\n\n## Exploring Common Machine Learning Algorithms\n##### K-Means Clustering, Decision Trees, Random Forests\n\n**Level**: Beginner\n**Language**: Scala\n**Requirements**: \n- [HDP 2.5](http://hortonworks.com/products/sandbox/) (or later) or [HDCloud](https://hortonworks.github.io/hdp-aws/)\n- Spark 2.x\n\n**Author**: Robert Hryniewicz\n**Follow** [@RobH8z](https://twitter.com/RobH8z)\n","dateUpdated":"2016-11-02T09:35:27+0100","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":false,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476368135408_-866086959","id":"20160531-234527_1318957937","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Exploring Common Machine Learning Algorithms</h2>\n<h5>K-Means Clustering, Decision Trees, Random Forests</h5>\n<p><strong>Level</strong>: Beginner\n<br  /><strong>Language</strong>: Scala\n<br  /><strong>Requirements</strong>:</p>\n<ul>\n<li><a href=\"http://hortonworks.com/products/sandbox/\">HDP 2.5</a> (or later) or <a href=\"https://hortonworks.github.io/hdp-aws/\">HDCloud</a></li>\n<li>Spark 2.x</li>\n</ul>\n<p><strong>Author</strong>: Robert Hryniewicz\n<br  /><strong>Follow</strong> <a href=\"https://twitter.com/RobH8z\">@RobH8z</a></p>\n"},"dateCreated":"2016-10-13T04:15:35+0200","dateStarted":"2016-11-02T09:35:27+0100","dateFinished":"2016-11-02T09:35:27+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:313"},{"title":"Introduction","text":"%md\n\nIn this lab you will run a few examples covering both *unsupervised* learning, such as K-Means clustering, as well as *supervised* learning, such as Decision Trees and Random Forests. The purpose of this lab is to get you started exploring machine learning algorithms without going into mathematical details of what goes on behind the scenes.\n#\nOnce you're done, you should have a better feel for the powerful Machine Learning libraries that are part of Apache Spark.\n#\nFor a complete documentation checkout the official Apache Spark [Machine Learning Library (MLlib) Guide](http://spark.apache.org/docs/latest/mllib-guide.html).\n#","dateUpdated":"2016-10-18T02:09:19+0200","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476368135408_-866086959","id":"20160531-234527_2012845753","result":{"code":"SUCCESS","type":"HTML","msg":"<p>In this lab you will run a few examples covering both <em>unsupervised</em> learning, such as K-Means clustering, as well as <em>supervised</em> learning, such as Decision Trees and Random Forests. The purpose of this lab is to get you started exploring machine learning algorithms without going into mathematical details of what goes on behind the scenes.</p>\n<h1></h1>\n<p>Once you're done, you should have a better feel for the powerful Machine Learning libraries that are part of Apache Spark.</p>\n<h1></h1>\n<p>For a complete documentation checkout the official Apache Spark <a href=\"http://spark.apache.org/docs/latest/mllib-guide.html\">Machine Learning Library (MLlib) Guide</a>.</p>\n<h1></h1>\n"},"dateCreated":"2016-10-13T04:15:35+0200","dateStarted":"2016-10-18T02:09:19+0200","dateFinished":"2016-10-18T02:09:19+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:314"},{"title":"New to Scala?","text":"%md\n\nThroughout this lab we will use basic Scala syntax. If you would like to learn more about Scala, here's an excellent [Scala Tutorial](http://www.dhgarrette.com/nlpclass/scala/basics.html).","dateUpdated":"2016-10-21T05:54:23+0200","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476368135408_-866086959","id":"20160531-234527_588679480","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Throughout this lab we will use basic Scala syntax. If you would like to learn more about Scala, here's an excellent <a href=\"http://www.dhgarrette.com/nlpclass/scala/basics.html\">Scala Tutorial</a>.</p>\n"},"dateCreated":"2016-10-13T04:15:35+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:315"},{"title":"How to run a paragraph?","text":"%md\nTo run a paragraph in a Zeppelin notebook you can either click the `play` button (blue triangle) on the right-hand side or simply press `Shift + Enter`.","dateUpdated":"2016-10-21T05:57:52+0200","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476368135408_-866086959","id":"20160531-234527_1555785908","result":{"code":"SUCCESS","type":"HTML","msg":"<p>To run a paragraph in a Zeppelin notebook you can either click the <code>play</code> button (blue triangle) on the right-hand side or simply press <code>Shift + Enter</code>.</p>\n"},"dateCreated":"2016-10-13T04:15:35+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:316"},{"title":"Unsupervised Learning: K-Means Clustering","text":"%md\n\n#### Unsupervised Learning\n\n\"Unsupervised learning is the machine learning task of inferring a function to describe hidden structure from unlabeled data. Since the examples given to the learner are unlabeled, there is no error or reward signal to evaluate a potential solution. This distinguishes unsupervised learning from supervised learning and reinforcement learning.\" - [wikipedia](https://en.wikipedia.org/wiki/Unsupervised_learning)\n#\n#### K-Means Clustering\n\nK-Means is one of the most commonly used clustering algorithms that clusters the data points into a predefined number of clusters. (See [Spark docs](http://spark.apache.org/docs/latest/ml-clustering.html) for more info.)\n#\nWe will use Spark ML API to generate a K-Means model using the Spark ML KMeans class. \n#\nKMeans is implemented as an Estimator and generates a KMeansModel as the base model.\n#\nNote that the data points for the training are hardcoded in the example below. Before you run the K-Means sample code, try to guess what the two cluster centers should be based on the training data.\n#","dateUpdated":"2016-10-18T02:10:12+0200","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476368135408_-866086959","id":"20160531-234527_332869884","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Unsupervised Learning</h4>\n<p>&ldquo;Unsupervised learning is the machine learning task of inferring a function to describe hidden structure from unlabeled data. Since the examples given to the learner are unlabeled, there is no error or reward signal to evaluate a potential solution. This distinguishes unsupervised learning from supervised learning and reinforcement learning.&rdquo; - <a href=\"https://en.wikipedia.org/wiki/Unsupervised_learning\">wikipedia</a></p>\n<h1></h1>\n<h4>K-Means Clustering</h4>\n<p>K-Means is one of the most commonly used clustering algorithms that clusters the data points into a predefined number of clusters. (See <a href=\"http://spark.apache.org/docs/latest/ml-clustering.html\">Spark docs</a> for more info.)</p>\n<h1></h1>\n<p>We will use Spark ML API to generate a K-Means model using the Spark ML KMeans class.</p>\n<h1></h1>\n<p>KMeans is implemented as an Estimator and generates a KMeansModel as the base model.</p>\n<h1></h1>\n<p>Note that the data points for the training are hardcoded in the example below. Before you run the K-Means sample code, try to guess what the two cluster centers should be based on the training data.</p>\n<h1></h1>\n"},"dateCreated":"2016-10-13T04:15:35+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:317"},{"title":"Interpreters","text":"%md\n\nIn the following paragraphs we are going to execute Spark code, run shell commands to download and move files, run sql queries etc. Each paragraph will start with `%` followed by an interpreter name, e.g. `%spark` for a Spark interpreter. Different interpreter names indicate what will be executed: code, markdown, html etc.  This allows you to perform data ingestion, munging, wrangling, visualization, analysis, processing and more, all in one place!\n\nThroughtout this notebook we will use the following interpreters:\n\n- `%spark` - Spark interpreter to run Spark code written in Scala\n- `%sh` - Shell interpreter to run shell commands\n- `%md` - Markdown for displaying formatted text, links, and images\n\nNote: The **default interpreter**, if none is specified, is the `%spark` interpreter.\n\nTo learn more about Zeppelin interpreters check out this [link](https://zeppelin.apache.org/docs/0.5.6-incubating/manual/interpreters.html).","dateUpdated":"2016-10-21T06:03:00+0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true,"title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476792809590_-1054984478","id":"20161018-141329_1642441071","result":{"code":"SUCCESS","type":"HTML","msg":"<p>In the following paragraphs we are going to execute Spark code, run shell commands to download and move files, run sql queries etc. Each paragraph will start with <code>%</code> followed by an interpreter name, e.g. <code>%spark</code> for a Spark interpreter. Different interpreter names indicate what will be executed: code, markdown, html etc.  This allows you to perform data ingestion, munging, wrangling, visualization, analysis, processing and more, all in one place!</p>\n<p>Throughtout this notebook we will use the following interpreters:</p>\n<ul>\n<li><code>%spark</code> - Spark interpreter to run Spark code written in Scala</li>\n<li><code>%sh</code> - Shell interpreter to run shell commands</li>\n<li><code>%md</code> - Markdown for displaying formatted text, links, and images</li>\n</ul>\n<p>Note: The <strong>default interpreter</strong>, if none is specified, is the <code>%spark</code> interpreter.</p>\n<p>To learn more about Zeppelin interpreters check out this <a href=\"https://zeppelin.apache.org/docs/0.5.6-incubating/manual/interpreters.html\">link</a>.</p>\n"},"dateCreated":"2016-10-18T02:13:29+0200","dateStarted":"2016-10-18T02:15:30+0200","dateFinished":"2016-10-18T02:15:30+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:318"},{"text":"%spark\n\nimport org.apache.spark.ml.clustering.KMeans\nimport org.apache.spark.mllib.linalg.Vectors\n\nimport org.apache.spark.sql.{DataFrame, SQLContext} \n\n// Crates a DataFrame\nval dataset: DataFrame = spark.createDataFrame(Seq(\n  (1, Vectors.dense(0.0, 0.0, 0.0)),\n  (2, Vectors.dense(0.1, 0.1, 0.1)),\n  (3, Vectors.dense(0.2, 0.2, 0.2)),\n  (4, Vectors.dense(3.0, 3.0, 3.0)),\n  (5, Vectors.dense(3.1, 3.1, 3.1)),\n  (6, Vectors.dense(3.2, 3.2, 3.2))\n)).toDF(\"id\", \"features\")\n\n// Trains a k-means model\nval kmeans = new KMeans()\n  .setK(2)                              // set number of clusters\n  .setFeaturesCol(\"features\")\n  .setPredictionCol(\"prediction\")\nval model = kmeans.fit(dataset)\n\n// Shows the result\nprintln(\"Final Centers: \")\nmodel.clusterCenters.foreach(println)","dateUpdated":"2016-11-02T09:36:35+0100","config":{"editorMode":"ace/mode/scala","colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476368135408_-866086959","id":"20160531-234527_349239953","result":{"code":"ERROR","type":"TEXT","msg":"\nimport org.apache.spark.ml.clustering.KMeans\n\nimport org.apache.spark.mllib.linalg.Vectors\n\nimport org.apache.spark.sql.{DataFrame, SQLContext}\n\ndataset: org.apache.spark.sql.DataFrame = [id: int, features: vector]\n\nkmeans: org.apache.spark.ml.clustering.KMeans = kmeans_3d5bf126d12c\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 84.0 failed 1 times, most recent failure: Lost task 0.0 in stage 84.0 (TID 2068, localhost): scala.MatchError: [[0.0,0.0,0.0]] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.clustering.KMeans$$anonfun$9.apply(KMeans.scala:307)\n\tat org.apache.spark.ml.clustering.KMeans$$anonfun$9.apply(KMeans.scala:307)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:214)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:919)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:910)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:910)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:668)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:281)\n\tat org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1911)\n  at org.apache.spark.rdd.RDD.count(RDD.scala:1115)\n  at org.apache.spark.rdd.RDD$$anonfun$takeSample$1.apply(RDD.scala:545)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n  at org.apache.spark.rdd.RDD.takeSample(RDD.scala:534)\n  at org.apache.spark.mllib.clustering.KMeans.initKMeansParallel(KMeans.scala:386)\n  at org.apache.spark.mllib.clustering.KMeans.runAlgorithm(KMeans.scala:256)\n  at org.apache.spark.mllib.clustering.KMeans.run(KMeans.scala:219)\n  at org.apache.spark.ml.clustering.KMeans.fit(KMeans.scala:321)\n  ... 48 elided\nCaused by: scala.MatchError: [[0.0,0.0,0.0]] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n  at org.apache.spark.ml.clustering.KMeans$$anonfun$9.apply(KMeans.scala:307)\n  at org.apache.spark.ml.clustering.KMeans$$anonfun$9.apply(KMeans.scala:307)\n  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n  at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:214)\n  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:919)\n  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:910)\n  at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)\n  at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:910)\n  at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:668)\n  at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:281)\n  at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n  at org.apache.spark.scheduler.Task.run(Task.scala:85)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n  ... 3 more\n"},"dateCreated":"2016-10-13T04:15:35+0200","dateStarted":"2016-11-02T09:36:35+0100","dateFinished":"2016-11-02T09:36:36+0100","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:319"},{"text":"%md\n\nDid you guess the cluster centers correctly?\n#\nAlthough this is a very simple exmaple, it should provide you with an intuitive feel for K-Means clustering.\n#\nBelow we've provided you with a visualization of training data points and computed cluster centers.\n#","dateUpdated":"2016-10-18T02:14:58+0200","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476368135408_-866086959","id":"20160531-234527_603082820","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Did you guess the cluster centers correctly?</p>\n<h1></h1>\n<p>Although this is a very simple exmaple, it should provide you with an intuitive feel for K-Means clustering.</p>\n<h1></h1>\n<p>Below we've provided you with a visualization of training data points and computed cluster centers.</p>\n<h1></h1>\n"},"dateCreated":"2016-10-13T04:15:35+0200","dateStarted":"2016-10-18T02:14:58+0200","dateFinished":"2016-10-18T02:14:58+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:320"},{"title":"Visualized Result of K-Means Clustering","text":"%md\n\n**TODO** Replace with Matplotlib\n\nThe input data is marked with a blue **+** and the two K-Means cluser centers are marked with a red **x**.\n#\n![scatter-plot](https://raw.githubusercontent.com/roberthryniewicz/images/master/lab201-plt-3d-scatter.png)","dateUpdated":"2016-11-02T09:37:14+0100","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":false,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476368135408_-866086959","id":"20160531-234527_2037625547","result":{"code":"SUCCESS","type":"HTML","msg":"<p><strong>TODO</strong> Replace with Matplotlib</p>\n<p>The input data is marked with a blue <strong>+</strong> and the two K-Means cluser centers are marked with a red <strong>x</strong>.</p>\n<h1></h1>\n<p><img src=\"https://raw.githubusercontent.com/roberthryniewicz/images/master/lab201-plt-3d-scatter.png\" alt=\"scatter-plot\" /></p>\n"},"dateCreated":"2016-10-13T04:15:35+0200","dateStarted":"2016-11-02T09:37:14+0100","dateFinished":"2016-11-02T09:37:14+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:321"},{"title":"Supervised Learning: Decision Trees and Random Forests","text":"%md\n\n### Supervised Learning\n\n\"Supervised learning is the machine learning task of inferring a function from labeled training data. The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a 'reasonable' way.\" - [wikipedia](https://en.wikipedia.org/wiki/Supervised_learning)","dateUpdated":"2016-10-18T02:15:44+0200","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476368135408_-866086959","id":"20160531-234527_450849720","result":{"code":"SUCCESS","type":"HTML","msg":"<h3>Supervised Learning</h3>\n<p>&ldquo;Supervised learning is the machine learning task of inferring a function from labeled training data. The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a 'reasonable' way.&rdquo; - <a href=\"https://en.wikipedia.org/wiki/Supervised_learning\">wikipedia</a></p>\n"},"dateCreated":"2016-10-13T04:15:35+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:322"},{"title":"Training Dataset","text":"%md\n\nFor Decision Tree and Random Forest examples we will use a diabetes dataset that has been cleansed, scaled, and sanitized to remove any personally identifying information. The diabetes dataset contains a distribution for 70 sets of data recorded on diabetes patients (several weeks' to months' worth of glucose, insulin, and lifestyle data per patient + a description of the problem domain).\n#\nKeep in mind that we are not particularly concerned what specific features represent, rather we will train our Decision Trees and Random Forest models to learn how the underlying features \"predict\" either negative or positive result based on the labeled training data set.\n#\n","dateUpdated":"2016-10-18T02:15:48+0200","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476368135408_-866086959","id":"20160531-234527_1108937424","result":{"code":"SUCCESS","type":"HTML","msg":"<p>For Decision Tree and Random Forest examples we will use a diabetes dataset that has been cleansed, scaled, and sanitized to remove any personally identifying information. The diabetes dataset contains a distribution for 70 sets of data recorded on diabetes patients (several weeks' to months' worth of glucose, insulin, and lifestyle data per patient + a description of the problem domain).</p>\n<h1></h1>\n<p>Keep in mind that we are not particularly concerned what specific features represent, rather we will train our Decision Trees and Random Forest models to learn how the underlying features &ldquo;predict&rdquo; either negative or positive result based on the labeled training data set.</p>\n<h1></h1>\n"},"dateCreated":"2016-10-13T04:15:35+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:323"},{"title":"Download diabetes dataset","text":"%sh\n\nwget --no-check-certificate http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/diabetes_scale -O /tmp/diabetes_scaled_data.txt","dateUpdated":"2016-11-02T09:37:23+0100","config":{"editorMode":"ace/mode/sh","colWidth":12,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476368135408_-866086959","id":"20160531-234527_2048196502","result":{"code":"SUCCESS","type":"TEXT","msg":"--2016-11-02 21:37:24--  http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/diabetes_scale\nResolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.28\nConnecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.28|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 68645 (67K)\nSaving to: ‘/tmp/diabetes_scaled_data.txt’\n\n     0K .......... .......... .......... .......... .......... 74% 15,3M 0s\n    50K .......... .......                                    100%  736M=0,003s\n\n2016-11-02 21:37:24 (20,4 MB/s) - ‘/tmp/diabetes_scaled_data.txt’ saved [68645/68645]\n\n"},"dateCreated":"2016-10-13T04:15:35+0200","dateStarted":"2016-11-02T09:37:23+0100","dateFinished":"2016-11-02T09:37:24+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:324"},{"title":"Preview dataset","text":"%sh\n\n#TODO: Test on HDP Sandbox\n\nhead /tmp/diabetes_scaled_data","dateUpdated":"2016-11-02T09:37:48+0100","config":{"editorMode":"ace/mode/sh","colWidth":12,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476368135408_-866086959","id":"20160531-234527_2023231351","result":{"code":"ERROR","type":"TEXT","msg":"head: cannot open '/tmp/diabetes_scaled_data' for reading: No such file or directory\nhead: cannot open '/tmp/diabetes_scaled_data' for reading: No such file or directory\nExitValue: 1"},"dateCreated":"2016-10-13T04:15:35+0200","dateStarted":"2016-11-02T09:37:28+0100","dateFinished":"2016-11-02T09:37:28+0100","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:325"},{"text":"import org.apache.spark.ml.classification.DecisionTreeClassificationModel\nimport org.apache.spark.ml.classification.DecisionTreeClassifier\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n\n// Load training data\nval data = spark.read.format(\"libsvm\")\n  .load(\"/tmp/diabetes_scaled_data.txt\")\n\n// Split the data into training and test sets (30% held out for testing).\nval Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))\n\n/*\n// Index labels, adding metadata to the label column.                                                                    \n// Fit on whole dataset to include all labels in index.                                                                  \nval labelIndexer = new StringIndexer()                                                                                   \n      .setInputCol(\"label\")                                                                                                  \n      .setOutputCol(\"indexedLabel\")                                                                                          \n      .fit(data)\n    \n// Automatically identify categorical features, and index them.                                                          \nval featureIndexer = new VectorIndexer()                                                                                 \n      .setInputCol(\"features\")                                                                                               \n      .setOutputCol(\"indexedFeatures\")                                                                                       \n      .setMaxCategories(2) // features with > 4 distinct values are treated as continuous                                    \n      .fit(data)  \n*/\n\nval dt = new DecisionTreeClassifier()\n\nval model = dt.fit(trainingData)","dateUpdated":"2016-11-02T09:37:41+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476798425689_-1334569246","id":"20161018-154705_499706276","result":{"code":"ERROR","type":"TEXT","msg":"\nimport org.apache.spark.ml.classification.DecisionTreeClassificationModel\n\nimport org.apache.spark.ml.classification.DecisionTreeClassifier\n\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n\ndata: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n\n\ntrainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\ntestData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n\ndt: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_68f7d2a548f8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 88.0 failed 1 times, most recent failure: Lost task 0.0 in stage 88.0 (TID 2072, localhost): java.lang.IllegalArgumentException: requirement failed: Classifier was given dataset with invalid label -1.0.  Labels must be integers in range [0, 1, ..., 2), where numClasses=2.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.classification.Classifier$$anonfun$extractLabeledPoints$2.apply(Classifier.scala:84)\n\tat org.apache.spark.ml.classification.Classifier$$anonfun$extractLabeledPoints$2.apply(Classifier.scala:82)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:393)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1305)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1305)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n  at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1305)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n  at org.apache.spark.rdd.RDD.take(RDD.scala:1279)\n  at org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)\n  at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:105)\n  at org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:93)\n  at org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:45)\n  at org.apache.spark.ml.Predictor.fit(Predictor.scala:90)\n  ... 48 elided\nCaused by: java.lang.IllegalArgumentException: requirement failed: Classifier was given dataset with invalid label -1.0.  Labels must be integers in range [0, 1, ..., 2), where numClasses=2.\n  at scala.Predef$.require(Predef.scala:224)\n  at org.apache.spark.ml.classification.Classifier$$anonfun$extractLabeledPoints$2.apply(Classifier.scala:84)\n  at org.apache.spark.ml.classification.Classifier$$anonfun$extractLabeledPoints$2.apply(Classifier.scala:82)\n  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n  at scala.collection.Iterator$$anon$10.next(Iterator.scala:393)\n  at scala.collection.Iterator$class.foreach(Iterator.scala:893)\n  at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n  at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n  at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n  at scala.collection.AbstractIterator.to(Iterator.scala:1336)\n  at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n  at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\n  at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n  at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\n  at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1305)\n  at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$29.apply(RDD.scala:1305)\n  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\n  at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n  at org.apache.spark.scheduler.Task.run(Task.scala:85)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n  ... 3 more\n"},"dateCreated":"2016-10-18T03:47:05+0200","dateStarted":"2016-11-02T09:37:41+0100","dateFinished":"2016-11-02T09:37:44+0100","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:326"},{"text":"import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n\n// Make predictions.                                                                                                     \nval predictions = model.transform(testData)\n\nval evaluator = new MulticlassClassificationEvaluator()                                                                  \n\nval accuracy = evaluator.evaluate(predictions)                                                                                      \nprintln(\"Test Error = \" + (1.0 - accuracy))\n\npredictions.select(\"prediction\", \"label\", \"features\").show()","dateUpdated":"2016-11-02T09:37:52+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476799914991_1002324409","id":"20161018-161154_281830760","result":{"code":"ERROR","type":"TEXT","msg":"\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n\n\n\n<console>:48: error: not found: value model\n       val predictions = model.transform(testData)\n                         ^\n"},"dateCreated":"2016-10-18T04:11:54+0200","dateStarted":"2016-11-02T09:37:52+0100","dateFinished":"2016-11-02T09:37:52+0100","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:327"},{"text":"%sh\n\nwget https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_linear_regression_data.txt -O /tmp/lin_reg_data.txt","dateUpdated":"2016-11-02T09:38:20+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476800468672_1380898714","id":"20161018-162108_66550990","result":{"code":"SUCCESS","type":"TEXT","msg":"--2016-11-02 21:38:20--  https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_linear_regression_data.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.52.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.52.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 119069 (116K) [text/plain]\nSaving to: ‘/tmp/lin_reg_data.txt’\n\n     0K .......... .......... .......... .......... .......... 43%  755K 0s\n    50K .......... .......... .......... .......... .......... 86% 1,56M 0s\n   100K .......... ......                                     100% 12,1M=0,1s\n\n2016-11-02 21:38:20 (1,15 MB/s) - ‘/tmp/lin_reg_data.txt’ saved [119069/119069]\n\n"},"dateCreated":"2016-10-18T04:21:08+0200","dateStarted":"2016-11-02T09:38:20+0100","dateFinished":"2016-11-02T09:38:20+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:328"},{"text":"import org.apache.spark.ml.regression.LinearRegression\n\n// Load training data\nval data = spark.read.format(\"libsvm\")\n  .load(\"file:///tmp/lin_reg_data.txt\")     // TODO: change to HDFS\n\nval lr = new LinearRegression()\n  .setMaxIter(10)\n  .setRegParam(0.3)\n  .setElasticNetParam(0.8)\n\n// Split the data into training and test sets (30% held out for testing).\nval Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))\n\n// Fit the model\nval lrModel = lr.fit(trainingData)\n\n// Print the coefficients and intercept for linear regression\nprintln(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")\n\n// Summarize the model over the training set and print out some metrics\n//val trainingSummary = lrModel.summary\n\n//println(s\"numIterations: ${trainingSummary.totalIterations}\")\n//println(s\"objectiveHistory: ${trainingSummary.objectiveHistory.toList}\")\n//trainingSummary.residuals.show()\n//println(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\")\n//println(s\"r2: ${trainingSummary.r2}\")","dateUpdated":"2016-11-02T09:38:23+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476795859882_-588330632","id":"20161018-150419_1884298670","result":{"code":"SUCCESS","type":"TEXT","msg":"\nimport org.apache.spark.ml.regression.LinearRegression\n\ndata: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n\nlr: org.apache.spark.ml.regression.LinearRegression = linReg_3fc260d566e5\n\n\ntrainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\ntestData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n\nlrModel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_3fc260d566e5\nCoefficients: [-0.27940206476566953,0.6859423755107964,0.0,1.827465242155485,0.20789366784617974,1.0512221376214135,0.0,-0.23519259457756797,0.0,0.0] Intercept: 0.26838655321361143\n"},"dateCreated":"2016-10-18T03:04:19+0200","dateStarted":"2016-11-02T09:38:23+0100","dateFinished":"2016-11-02T09:38:25+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:329"},{"text":"import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n\n// Make predictions.                                                                                                     \nval predictions = lrModel.transform(trainingData)\n\n// Select example rows to display.                                                                                       \nval subset = predictions.select(\"prediction\", \"label\", \"features\")\n\nval evaluator = new MulticlassClassificationEvaluator()\n      .setLabelCol(\"label\")                                                                                           \n      .setPredictionCol(\"prediction\")                                                                                        \n      .setMetricName(\"accuracy\")\n      \nval accuracy = evaluator.evaluate(predictions)                                                                                      \nprintln(\"Test Error = \" + (1.0 - accuracy))\n\npredictions.show()","dateUpdated":"2016-10-19T10:44:15+0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476796268473_-655477800","id":"20161018-151108_677366666","result":{"code":"SUCCESS","type":"TEXT","msg":"\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n\npredictions: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 1 more field]\n\nsubset: org.apache.spark.sql.DataFrame = [prediction: double, label: double ... 1 more field]\n\nevaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_52e87b164931\n\naccuracy: Double = 0.0\nTest Error = 1.0\n+-------------------+--------------------+-------------------+\n|              label|            features|         prediction|\n+-------------------+--------------------+-------------------+\n|-26.805483428483072|(10,[0,1,2,3,4,5,...| 1.0745658541618976|\n|-26.736207182601724|(10,[0,1,2,3,4,5,...| -2.397147883107923|\n| -23.51088409032297|(10,[0,1,2,3,4,5,...| -1.839857690924632|\n|-23.487440120936512|(10,[0,1,2,3,4,5,...|0.28971892409608085|\n|-22.949825936196074|(10,[0,1,2,3,4,5,...|   2.92617066259918|\n|-22.837460416919342|(10,[0,1,2,3,4,5,...|-3.0971328108689478|\n|-21.432387764165806|(10,[0,1,2,3,4,5,...| 0.7261334884396136|\n|-20.212077258958672|(10,[0,1,2,3,4,5,...| 1.1053782859118588|\n|-20.057482615789212|(10,[0,1,2,3,4,5,...| 0.6935186253228984|\n|-19.872991038068406|(10,[0,1,2,3,4,5,...|-0.8118149035345368|\n|-19.782762789614537|(10,[0,1,2,3,4,5,...| 0.3661233636809672|\n| -19.66731861537172|(10,[0,1,2,3,4,5,...|0.02465486584694182|\n|-19.402336030214553|(10,[0,1,2,3,4,5,...|-0.7988338892405433|\n|-18.845922472898582|(10,[0,1,2,3,4,5,...| 1.2833711139147275|\n| -18.27521356600463|(10,[0,1,2,3,4,5,...|  0.713659477351759|\n|-17.803626188664516|(10,[0,1,2,3,4,5,...| 0.3376370929458461|\n|-17.494200356883344|(10,[0,1,2,3,4,5,...|-1.8558640699049485|\n| -17.32672073267595|(10,[0,1,2,3,4,5,...|-0.6486665305633337|\n| -16.71909683360509|(10,[0,1,2,3,4,5,...| -0.716382931851115|\n|-16.692207021311106|(10,[0,1,2,3,4,5,...| 0.8281509315645663|\n+-------------------+--------------------+-------------------+\nonly showing top 20 rows\n\n"},"dateCreated":"2016-10-18T03:11:08+0200","dateStarted":"2016-10-19T10:43:31+0200","dateFinished":"2016-10-19T10:43:32+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:330"},{"text":"\nsubset.createOrReplaceTempView(\"predictions\")","dateUpdated":"2016-11-02T09:38:44+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476800699437_-28899522","id":"20161018-162459_1109892736","result":{"code":"ERROR","type":"TEXT","msg":"\n\n\n<console>:49: error: not found: value subset\n       subset.createOrReplaceTempView(\"predictions\")\n       ^\n"},"dateCreated":"2016-10-18T04:24:59+0200","dateStarted":"2016-11-02T09:38:44+0100","dateFinished":"2016-11-02T09:38:44+0100","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:331"},{"text":"%sql\n\nSELECT prediction, label from predictions","dateUpdated":"2016-11-02T09:38:45+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":true,"keys":[],"values":[{"name":"label","index":1,"aggr":"sum"},{"name":"prediction","index":0,"aggr":"sum"}],"groups":[],"scatter":{"yAxis":{"name":"label","index":1,"aggr":"sum"},"xAxis":{"name":"prediction","index":0,"aggr":"sum"}},"forceY":true,"lineWithFocus":false},"enabled":true,"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476800784181_392354551","id":"20161018-162624_1800628735","result":{"code":"ERROR","type":"TEXT","msg":"Table or view not found: predictions; line 1 pos 30\nset zeppelin.spark.sql.stacktrace = true to see full stacktrace"},"dateCreated":"2016-10-18T04:26:24+0200","dateStarted":"2016-11-02T09:38:45+0100","dateFinished":"2016-11-02T09:38:45+0100","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:332"},{"title":"Count the number of training examples in the file","text":"%sh\n\nwc -l < /tmp/diabetes_scaled_data","dateUpdated":"2016-11-02T09:38:48+0100","config":{"editorMode":"ace/mode/sh","colWidth":12,"editorHide":false,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476368135408_-866086959","id":"20160531-234527_1193249659","result":{"code":"ERROR","type":"TEXT","msg":"bash: /tmp/diabetes_scaled_data: No such file or directory\nbash: /tmp/diabetes_scaled_data: No such file or directory\nExitValue: 1"},"dateCreated":"2016-10-13T04:15:35+0200","dateStarted":"2016-11-02T09:38:48+0100","dateFinished":"2016-11-02T09:38:48+0100","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:333"},{"title":"Move File to HDFS","text":"%sh\n\n# remove existing copies of dataset from HDFS\nhadoop fs -rm -r -f /tmp/diabetes_scaled_data\n\n# create directory on HDFS\nhadoop fs -mkdir /tmp/libsvm-data\n\n# put data into HDFS\nhadoop fs -put /tmp/diabetes_scaled_data /tmp/libsvm-data/diabetes_scaled_data","dateUpdated":"2016-11-02T09:38:50+0100","config":{"editorMode":"ace/mode/sh","colWidth":12,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476368135408_-866086959","id":"20161005-214739_393008546","result":{"code":"ERROR","type":"TEXT","msg":"bash: line 1: hadoop: command not found\nbash: line 4: hadoop: command not found\nbash: line 7: hadoop: command not found\nbash: line 1: hadoop: command not found\nbash: line 4: hadoop: command not found\nbash: line 7: hadoop: command not found\nExitValue: 127"},"dateCreated":"2016-10-13T04:15:35+0200","dateStarted":"2016-11-02T09:38:50+0100","dateFinished":"2016-11-02T09:38:50+0100","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:334"},{"title":"Decision Trees","text":"%md\n\nDecision trees and their ensembles are popular methods for the machine learning tasks of classification and regression. Decision trees are widely used since they are easy to interpret, handle categorical features, extend to the multiclass classification setting, do not require feature scaling, and are able to capture non-linearities and feature interactions. Tree ensemble algorithms such as random forests and boosting are among the top performers for classification and regression tasks.\n#\nThe spark.ml implementation supports decision trees for binary and multiclass classification and for regression, using both continuous and categorical features. The implementation partitions data by rows, allowing distributed training with millions or even billions of instances. ([See docs](http://spark.apache.org/docs/latest/ml-classification-regression.html#decision-trees) for more info.)\n#\nMake sure to checkout **[this](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)** great introduction to Visual Machine Learning to get an intuitive feel for the *ideas* behind Decision Tree classification.","dateUpdated":"2016-10-18T02:16:55+0200","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476368135408_-866086959","id":"20160531-234527_1744723127","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Decision trees and their ensembles are popular methods for the machine learning tasks of classification and regression. Decision trees are widely used since they are easy to interpret, handle categorical features, extend to the multiclass classification setting, do not require feature scaling, and are able to capture non-linearities and feature interactions. Tree ensemble algorithms such as random forests and boosting are among the top performers for classification and regression tasks.</p>\n<h1></h1>\n<p>The spark.ml implementation supports decision trees for binary and multiclass classification and for regression, using both continuous and categorical features. The implementation partitions data by rows, allowing distributed training with millions or even billions of instances. (<a href=\"http://spark.apache.org/docs/latest/ml-classification-regression.html#decision-trees\">See docs</a> for more info.)</p>\n<h1></h1>\n<p>Make sure to checkout <strong><a href=\"http://www.r2d3.us/visual-intro-to-machine-learning-part-1/\">this</a></strong> great introduction to Visual Machine Learning to get an intuitive feel for the <em>ideas</em> behind Decision Tree classification.</p>\n"},"dateCreated":"2016-10-13T04:15:35+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:335"},{"title":"Decision Tree","text":"import org.apache.spark.sql.SQLContext                                                                                       \n\nimport org.apache.spark.ml.Pipeline                                                                                          \nimport org.apache.spark.ml.classification.DecisionTreeClassifier                                                             \nimport org.apache.spark.ml.classification.DecisionTreeClassificationModel                                                    \nimport org.apache.spark.ml.feature.{StringIndexer, IndexToString, VectorIndexer}                                             \nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator \nimport org.apache.spark.sql.Row\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}\n\nval sqlContext = new SQLContext(sc)                                                                                              \n    \n// Load the data stored in LIBSVM format as a DataFrame.                                                                 \n//val data = sqlContext.read.format(\"libsvm\").load(\"/tmp/libsvm-data/diabetes_scaled_data\")            // TODO: Uncomment                        \nval data = sqlContext.read.format(\"libsvm\").load(\"file:///tmp/diabetes_scaled_data.txt\")\n                                                                                            \n// Index labels, adding metadata to the label column.                                                                    \n// Fit on whole dataset to include all labels in index.                                                                  \nval labelIndexer = new StringIndexer()                                                                                   \n      .setInputCol(\"label\")                                                                                                  \n      .setOutputCol(\"indexedLabel\")                                                                                          \n      .fit(data)\n","dateUpdated":"2016-11-02T09:38:54+0100","config":{"editorMode":"ace/mode/scala","colWidth":12,"editorHide":false,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476368135408_-866086959","id":"20160531-234527_903900070","result":{"code":"SUCCESS","type":"TEXT","msg":"\nimport org.apache.spark.sql.SQLContext\n\nimport org.apache.spark.ml.Pipeline\n\nimport org.apache.spark.ml.classification.DecisionTreeClassifier\n\nimport org.apache.spark.ml.classification.DecisionTreeClassificationModel\n\nimport org.apache.spark.ml.feature.{StringIndexer, IndexToString, VectorIndexer}\n\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n\nimport org.apache.spark.sql.Row\n\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nsqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@737bd454\n\ndata: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n\nlabelIndexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_badb15d7d321\n"},"dateCreated":"2016-10-13T04:15:35+0200","dateStarted":"2016-11-02T09:38:54+0100","dateFinished":"2016-11-02T09:38:56+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:336"},{"text":"// Automatically identify categorical features, and index them.                                                          \nval featureIndexer = new VectorIndexer()                                                                                 \n      .setInputCol(\"features\")                                                                                               \n      .setOutputCol(\"indexedFeatures\")                                                                                       \n      .setMaxCategories(2) // features with > 4 distinct values are treated as continuous                                    \n      .fit(data)                                                                                                             \n                                                                                                                             \n// Split the data into training and test sets (30% held out for testing)                                                 \nval Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))                                                    \n                                                                                                                             \n// Train a DecisionTree model.                                                                                           \nval dt = new DecisionTreeClassifier()                                                                                    \n      .setLabelCol(\"indexedLabel\")                                                                                           \n      .setFeaturesCol(\"indexedFeatures\")\n      .setMaxDepth(5)\n                                                                                                                             \n// Convert indexed labels back to original labels.                                                                       \nval labelConverter = new IndexToString()                                                                                 \n      .setInputCol(\"prediction\")                                                                                             \n      .setOutputCol(\"predictedLabel\")                                                                                        \n      .setLabels(labelIndexer.labels)                                                                                        \n                                                                                                                             \n// Chain indexers and tree in a Pipeline                                                                                 \nval pipeline = new Pipeline()                                                                                            \n      .setStages(Array(labelIndexer, featureIndexer, dt, labelConverter))                                                    \n                                                                                                                             \n// Train model.  This also runs the indexers.                                                                            \nval model = pipeline.fit(trainingData)                                                                                   \n                                                                                                                             \n// Make predictions.                                                                                                     \nval predictions = model.transform(testData)                                                                              \n                                                                                                                             \n// Select example rows to display.                                                                                       \npredictions.select(\"predictedLabel\", \"label\", \"features\").show(5)                                                        \n                                                                                                                             \n// Select (prediction, true label) and compute test error                                                                \nval evaluator = new MulticlassClassificationEvaluator()                                                                  \n      .setLabelCol(\"indexedLabel\")                                                                                           \n      .setPredictionCol(\"prediction\")                                                                                        \n      .setMetricName(\"precision\")                                                                                            \n    \nval accuracy = evaluator.evaluate(predictions)                                                                           \nprintln(\"Test Error = \" + (1.0 - accuracy))                                                                              \n                                                                                                                             \nval treeModel = model.stages(2).asInstanceOf[DecisionTreeClassificationModel]                                            \nprintln(\"Learned classification tree model:\\n\" + treeModel.toDebugString)","dateUpdated":"2016-11-02T09:39:00+0100","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476810829028_1618641691","id":"20161018-191349_888985205","result":{"code":"ERROR","type":"TEXT","msg":"\nfeatureIndexer: org.apache.spark.ml.feature.VectorIndexerModel = vecIdx_40a42db56414\n\n\ntrainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\ntestData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n\ndt: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_7dde8a184e15\n\nlabelConverter: org.apache.spark.ml.feature.IndexToString = idxToStr_1920af41243e\n\npipeline: org.apache.spark.ml.Pipeline = pipeline_66bc6275ce5f\n\nmodel: org.apache.spark.ml.PipelineModel = pipeline_66bc6275ce5f\n\npredictions: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 6 more fields]\n+--------------+-----+--------------------+\n|predictedLabel|label|            features|\n+--------------+-----+--------------------+\n|           1.0| -1.0|(8,[0,1,2,3,4,5,6...|\n|          -1.0| -1.0|(8,[0,1,2,3,4,5,6...|\n|          -1.0| -1.0|(8,[0,1,2,3,4,5,6...|\n|          -1.0| -1.0|(8,[0,1,2,3,4,5,6...|\n|          -1.0| -1.0|(8,[0,1,2,3,4,5,6...|\n+--------------+-----+--------------------+\nonly showing top 5 rows\n\n\n\n\n\n\n\n\n\njava.lang.IllegalArgumentException: mcEval_89e1b5a2ecee parameter metricName given invalid value precision.\n  at org.apache.spark.ml.param.Param.validate(params.scala:77)\n  at org.apache.spark.ml.param.ParamPair.<init>(params.scala:521)\n  at org.apache.spark.ml.param.Param.$minus$greater(params.scala:87)\n  at org.apache.spark.ml.param.Params$class.set(params.scala:617)\n  at org.apache.spark.ml.evaluation.Evaluator.set(Evaluator.scala:30)\n  at org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator.setMetricName(MulticlassClassificationEvaluator.scala:60)\n  ... 48 elided\n"},"dateCreated":"2016-10-18T07:13:49+0200","dateStarted":"2016-11-02T09:39:00+0100","dateFinished":"2016-11-02T09:39:05+0100","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:337"},{"title":"Random Forests","text":"%md\n\nNow let's see if we can achieve a better performance with an ensemble of trees known as random forests. \n#\nRandom forests combine many decision trees in order to reduce the risk of overfitting. The spark.ml implementation supports random forests for binary and multiclass classification and for regression, using both continuous and categorical features. ([See docs](http://spark.apache.org/docs/latest/ml-classification-regression.html#random-forests) for more info.)\n#\nIn the example below we will combine five (5) trees to create a forest of trees.\n#\n","dateUpdated":"2016-10-18T02:17:06+0200","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476368135409_-866471707","id":"20160531-234527_1954753725","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now let's see if we can achieve a better performance with an ensemble of trees known as random forests.</p>\n<h1></h1>\n<p>Random forests combine many decision trees in order to reduce the risk of overfitting. The spark.ml implementation supports random forests for binary and multiclass classification and for regression, using both continuous and categorical features. (<a href=\"http://spark.apache.org/docs/latest/ml-classification-regression.html#random-forests\">See docs</a> for more info.)</p>\n<h1></h1>\n<p>In the example below we will combine five (5) trees to create a forest of trees.</p>\n<h1></h1>\n"},"dateCreated":"2016-10-13T04:15:35+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:338"},{"title":"Random Forest","text":"import org.apache.spark.sql.SQLContext                                                                                                  \n\nimport org.apache.spark.ml.Pipeline                                                                                                     \nimport org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}                                     \nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator                                                                 \nimport org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}                                                        \n\nval sqlContext = new SQLContext(sc)                                                                                                 \n                                                                                                                                        \n// Load and parse the LIBSVM data file, converting it to a DataFrame.\nval data = sqlContext.read.format(\"libsvm\").load(\"/tmp/libsvm-data/diabetes_scaled_data\") \n                                                                                                                                        \n// Index labels, adding metadata to the label column.                                                                               \n// Fit on whole dataset to include all labels in index.                                                                             \nval labelIndexer = new StringIndexer()                                                                                              \n      .setInputCol(\"label\")                                                                                                             \n      .setOutputCol(\"indexedLabel\")                                                                                                     \n      .fit(data)                                                                                                                        \n\n// Automatically identify categorical features, and index them.                                                                     \nval featureIndexer = new VectorIndexer()                                                                                            \n      .setInputCol(\"features\")                                                                                                          \n      .setOutputCol(\"indexedFeatures\")                                                                                                  \n      .setMaxCategories(2)                                                                                                              \n      .fit(data)                                                                                                                        \n                                                                                                                                        \n// Split the data into training and test sets (30% held out for testing)                                                            \nval Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))                                                               \n                                                                                                                                        \n// Train a RandomForest model.                                                                                                      \nval rf = new RandomForestClassifier()                                                                                               \n      .setLabelCol(\"indexedLabel\")                                                                                                      \n      .setFeaturesCol(\"indexedFeatures\")                                                                                                \n      .setNumTrees(5)                                                                                                                  \n                                                                                                                                        \n// Convert indexed labels back to original labels.                                                                                  \nval labelConverter = new IndexToString()                                                                                            \n      .setInputCol(\"prediction\")                                                                                                        \n      .setOutputCol(\"predictedLabel\")                                                                                                   \n      .setLabels(labelIndexer.labels)                                                                                                   \n                                                                                                                                        \n// Chain indexers and forest in a Pipeline                                                                                          \nval pipeline = new Pipeline()                                                                                                       \n      .setStages(Array(labelIndexer, featureIndexer, rf, labelConverter))                                                               \n                                                                                                                                        \n// Train model.  This also runs the indexers.                                                                                       \nval model = pipeline.fit(trainingData)                                                                                              \n                                                                                                                                        \n// Make predictions.                                                                                                                \nval predictions = model.transform(testData)                                                                                         \n                                                                                                                                        \n// Select example rows to display.                                                                                                  \npredictions.select(\"predictedLabel\", \"label\", \"features\").show(5)                                                                   \n                                                                                                                                        \n// Select (prediction, true label) and compute test error                                                                           \nval evaluator = new MulticlassClassificationEvaluator()                                                                             \n      .setLabelCol(\"indexedLabel\")                                                                                                      \n      .setPredictionCol(\"prediction\")                                                                                                   \n      .setMetricName(\"precision\")                                                                                                       \n    \nval accuracy = evaluator.evaluate(predictions)                                                                                      \nprintln(\"Test Error = \" + (1.0 - accuracy))                                                                                         \n                                                                                                                                        \nval rfModel = model.stages(2).asInstanceOf[RandomForestClassificationModel]                                                         \nprintln(\"Learned classification forest model:\\n\" + rfModel.toDebugString)  ","dateUpdated":"2016-10-18T02:06:11+0200","config":{"editorMode":"ace/mode/scala","colWidth":12,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476368135409_-866471707","id":"20160531-234527_1341024269","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.sql.SQLContext\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\nsqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@1d7b3923\ndata: org.apache.spark.sql.DataFrame = [label: double, features: vector]\nlabelIndexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_02eb790d2906\nfeatureIndexer: org.apache.spark.ml.feature.VectorIndexerModel = vecIdx_21e6abcca78d\ntrainingData: org.apache.spark.sql.DataFrame = [label: double, features: vector]\ntestData: org.apache.spark.sql.DataFrame = [label: double, features: vector]\nrf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_e9b24b4bf33a\nlabelConverter: org.apache.spark.ml.feature.IndexToString = idxToStr_1d254989def5\npipeline: org.apache.spark.ml.Pipeline = pipeline_f78b1d228c0f\nmodel: org.apache.spark.ml.PipelineModel = pipeline_f78b1d228c0f\npredictions: org.apache.spark.sql.DataFrame = [label: double, features: vector, indexedLabel: double, indexedFeatures: vector, rawPrediction: vector, probability: vector, prediction: double, predictedLabel: string]\n+--------------+-----+--------------------+\n|predictedLabel|label|            features|\n+--------------+-----+--------------------+\n|          -1.0| -1.0|(8,[0,1,2,3,4,5,6...|\n|          -1.0| -1.0|(8,[0,1,2,3,4,5,6...|\n|           1.0| -1.0|(8,[0,1,2,3,4,5,6...|\n|          -1.0| -1.0|(8,[0,1,2,3,4,5,6...|\n|           1.0| -1.0|(8,[0,1,2,3,4,5,6...|\n+--------------+-----+--------------------+\nonly showing top 5 rows\n\nevaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_e11d55da1218\naccuracy: Double = 0.75\nTest Error = 0.25\nrfModel: org.apache.spark.ml.classification.RandomForestClassificationModel = RandomForestClassificationModel (uid=rfc_b522745ef610) with 5 trees\nLearned classification forest model:\nRandomForestClassificationModel (uid=rfc_b522745ef610) with 5 trees\n  Tree 0 (weight 1.0):\n    If (feature 5 <= -0.105812)\n     If (feature 7 <= -0.833333)\n      Predict: 0.0\n     Else (feature 7 > -0.833333)\n      If (feature 5 <= -0.308495)\n       If (feature 0 <= 0.0588235)\n        Predict: 0.0\n       Else (feature 0 > 0.0588235)\n        If (feature 3 <= -1.0)\n         Predict: 1.0\n        Else (feature 3 > -1.0)\n         Predict: 0.0\n      Else (feature 5 > -0.308495)\n       If (feature 5 <= -0.168405)\n        If (feature 3 <= -0.59596)\n         Predict: 0.0\n        Else (feature 3 > -0.59596)\n         Predict: 1.0\n       Else (feature 5 > -0.168405)\n        If (feature 4 <= -0.716312)\n         Predict: 0.0\n        Else (feature 4 > -0.716312)\n         Predict: 1.0\n    Else (feature 5 > -0.105812)\n     If (feature 6 <= -0.63877)\n      If (feature 1 <= 0.326633)\n       If (feature 2 <= -0.147541)\n        If (feature 1 <= 0.165829)\n         Predict: 0.0\n        Else (feature 1 > 0.165829)\n         Predict: 1.0\n       Else (feature 2 > -0.147541)\n        If (feature 4 <= -0.621749)\n         Predict: 0.0\n        Else (feature 4 > -0.621749)\n         Predict: 0.0\n      Else (feature 1 > 0.326633)\n       If (feature 3 <= -1.0)\n        If (feature 1 <= 0.447236)\n         Predict: 0.0\n        Else (feature 1 > 0.447236)\n         Predict: 1.0\n       Else (feature 3 > -1.0)\n        If (feature 6 <= -0.798463)\n         Predict: 0.0\n        Else (feature 6 > -0.798463)\n         Predict: 1.0\n     Else (feature 6 > -0.63877)\n      If (feature 7 <= -0.533333)\n       If (feature 2 <= 0.278689)\n        If (feature 6 <= -0.432963)\n         Predict: 1.0\n        Else (feature 6 > -0.432963)\n         Predict: 1.0\n       Else (feature 2 > 0.278689)\n        If (feature 0 <= -0.882353)\n         Predict: 0.0\n        Else (feature 0 > -0.882353)\n         Predict: 1.0\n      Else (feature 7 > -0.533333)\n       If (feature 1 <= -0.0753769)\n        Predict: 0.0\n       Else (feature 1 > -0.0753769)\n        If (feature 1 <= 0.366834)\n         Predict: 1.0\n        Else (feature 1 > 0.366834)\n         Predict: 1.0\n  Tree 1 (weight 1.0):\n    If (feature 1 <= 0.537688)\n     If (feature 7 <= -0.766667)\n      If (feature 5 <= -0.0938897)\n       If (feature 3 <= -0.434343)\n        If (feature 5 <= -0.120715)\n         Predict: 0.0\n        Else (feature 5 > -0.120715)\n         Predict: 0.0\n       Else (feature 3 > -0.434343)\n        If (feature 7 <= -0.8)\n         Predict: 0.0\n        Else (feature 7 > -0.8)\n         Predict: 1.0\n      Else (feature 5 > -0.0938897)\n       If (feature 6 <= -0.63877)\n        If (feature 5 <= 0.257824)\n         Predict: 0.0\n        Else (feature 5 > 0.257824)\n         Predict: 1.0\n       Else (feature 6 > -0.63877)\n        If (feature 3 <= -0.393939)\n         Predict: 0.0\n        Else (feature 3 > -0.393939)\n         Predict: 1.0\n     Else (feature 7 > -0.766667)\n      If (feature 1 <= 0.326633)\n       If (feature 5 <= -0.213115)\n        If (feature 5 <= -0.406855)\n         Predict: 1.0\n        Else (feature 5 > -0.406855)\n         Predict: 0.0\n       Else (feature 5 > -0.213115)\n        If (feature 3 <= -0.59596)\n         Predict: 0.0\n        Else (feature 3 > -0.59596)\n         Predict: 1.0\n      Else (feature 1 > 0.326633)\n       If (feature 5 <= 0.052161)\n        If (feature 6 <= -0.515798)\n         Predict: 0.0\n        Else (feature 6 > -0.515798)\n         Predict: 1.0\n       Else (feature 5 > 0.052161)\n        If (feature 6 <= -0.855679)\n         Predict: 0.0\n        Else (feature 6 > -0.855679)\n         Predict: 1.0\n    Else (feature 1 > 0.537688)\n     If (feature 5 <= -0.347243)\n      Predict: 0.0\n     Else (feature 5 > -0.347243)\n      If (feature 4 <= -0.966903)\n       If (feature 5 <= -0.14456)\n        If (feature 2 <= 0.409836)\n         Predict: 1.0\n        Else (feature 2 > 0.409836)\n         Predict: 0.0\n       Else (feature 5 > -0.14456)\n        Predict: 1.0\n      Else (feature 4 > -0.966903)\n       If (feature 6 <= -0.945346)\n        Predict: 0.0\n       Else (feature 6 > -0.945346)\n        If (feature 4 <= -0.65721)\n         Predict: 1.0\n        Else (feature 4 > -0.65721)\n         Predict: 1.0\n  Tree 2 (weight 1.0):\n    If (feature 7 <= -0.7)\n     If (feature 5 <= -0.0938897)\n      If (feature 7 <= -0.8)\n       If (feature 1 <= 0.366834)\n        If (feature 6 <= -0.515798)\n         Predict: 0.0\n        Else (feature 6 > -0.515798)\n         Predict: 0.0\n       Else (feature 1 > 0.366834)\n        If (feature 4 <= -0.77305)\n         Predict: 1.0\n        Else (feature 4 > -0.77305)\n         Predict: 0.0\n      Else (feature 7 > -0.8)\n       If (feature 3 <= -0.616162)\n        If (feature 6 <= -0.899231)\n         Predict: 0.0\n        Else (feature 6 > -0.899231)\n         Predict: 0.0\n       Else (feature 3 > -0.616162)\n        If (feature 7 <= -0.733333)\n         Predict: 1.0\n        Else (feature 7 > -0.733333)\n         Predict: 0.0\n     Else (feature 5 > -0.0938897)\n      If (feature 6 <= -0.63877)\n       If (feature 5 <= 0.257824)\n        If (feature 5 <= -0.0581222)\n         Predict: 1.0\n        Else (feature 5 > -0.0581222)\n         Predict: 0.0\n       Else (feature 5 > 0.257824)\n        If (feature 2 <= 0.0491803)\n         Predict: 0.0\n        Else (feature 2 > 0.0491803)\n         Predict: 1.0\n      Else (feature 6 > -0.63877)\n       If (feature 1 <= 0.58794)\n        If (feature 1 <= 0.0653266)\n         Predict: 0.0\n        Else (feature 1 > 0.0653266)\n         Predict: 0.0\n       Else (feature 1 > 0.58794)\n        Predict: 1.0\n    Else (feature 7 > -0.7)\n     If (feature 5 <= -0.183308)\n      If (feature 2 <= 0.344262)\n       If (feature 6 <= -0.778822)\n        If (feature 1 <= 0.738693)\n         Predict: 0.0\n        Else (feature 1 > 0.738693)\n         Predict: 1.0\n       Else (feature 6 > -0.778822)\n        Predict: 0.0\n      Else (feature 2 > 0.344262)\n       Predict: 1.0\n     Else (feature 5 > -0.183308)\n      If (feature 6 <= -0.610589)\n       If (feature 4 <= -0.687943)\n        If (feature 4 <= -1.0)\n         Predict: 0.0\n        Else (feature 4 > -1.0)\n         Predict: 0.0\n       Else (feature 4 > -0.687943)\n        If (feature 6 <= -0.70538)\n         Predict: 1.0\n        Else (feature 6 > -0.70538)\n         Predict: 0.0\n      Else (feature 6 > -0.610589)\n       If (feature 2 <= 0.606557)\n        If (feature 4 <= -0.328605)\n         Predict: 1.0\n        Else (feature 4 > -0.328605)\n         Predict: 0.0\n       Else (feature 2 > 0.606557)\n        Predict: 0.0\n  Tree 3 (weight 1.0):\n    If (feature 5 <= -0.14456)\n     If (feature 1 <= 0.447236)\n      If (feature 3 <= -0.494949)\n       If (feature 5 <= -0.213115)\n        If (feature 5 <= -0.406855)\n         Predict: 0.0\n        Else (feature 5 > -0.406855)\n         Predict: 0.0\n       Else (feature 5 > -0.213115)\n        If (feature 3 <= -0.858586)\n         Predict: 0.0\n        Else (feature 3 > -0.858586)\n         Predict: 0.0\n      Else (feature 3 > -0.494949)\n       If (feature 7 <= -0.8)\n        Predict: 0.0\n       Else (feature 7 > -0.8)\n        If (feature 7 <= -0.666667)\n         Predict: 1.0\n        Else (feature 7 > -0.666667)\n         Predict: 0.0\n     Else (feature 1 > 0.447236)\n      If (feature 5 <= -0.347243)\n       Predict: 0.0\n      Else (feature 5 > -0.347243)\n       If (feature 0 <= -0.0588235)\n        If (feature 7 <= 0.366667)\n         Predict: 1.0\n        Else (feature 7 > 0.366667)\n         Predict: 0.0\n       Else (feature 0 > -0.0588235)\n        Predict: 1.0\n    Else (feature 5 > -0.14456)\n     If (feature 1 <= 0.326633)\n      If (feature 6 <= -0.211785)\n       If (feature 2 <= 0.0819672)\n        If (feature 0 <= -0.764706)\n         Predict: 0.0\n        Else (feature 0 > -0.764706)\n         Predict: 1.0\n       Else (feature 2 > 0.0819672)\n        If (feature 4 <= -0.574468)\n         Predict: 0.0\n        Else (feature 4 > -0.574468)\n         Predict: 1.0\n      Else (feature 6 > -0.211785)\n       If (feature 5 <= 0.290611)\n        Predict: 1.0\n       Else (feature 5 > 0.290611)\n        Predict: 0.0\n     Else (feature 1 > 0.326633)\n      If (feature 0 <= -0.294118)\n       If (feature 1 <= 0.58794)\n        If (feature 5 <= -0.0581222)\n         Predict: 0.0\n        Else (feature 5 > -0.0581222)\n         Predict: 1.0\n       Else (feature 1 > 0.58794)\n        If (feature 6 <= -0.211785)\n         Predict: 1.0\n        Else (feature 6 > -0.211785)\n         Predict: 1.0\n      Else (feature 0 > -0.294118)\n       If (feature 3 <= -0.272727)\n        If (feature 5 <= 0.0938898)\n         Predict: 1.0\n        Else (feature 5 > 0.0938898)\n         Predict: 1.0\n       Else (feature 3 > -0.272727)\n        If (feature 3 <= -0.131313)\n         Predict: 0.0\n        Else (feature 3 > -0.131313)\n         Predict: 1.0\n  Tree 4 (weight 1.0):\n    If (feature 7 <= -0.833333)\n     If (feature 0 <= -1.0)\n      If (feature 5 <= -0.0760059)\n       If (feature 4 <= -0.621749)\n        Predict: 0.0\n       Else (feature 4 > -0.621749)\n        If (feature 7 <= -0.9)\n         Predict: 0.0\n        Else (feature 7 > -0.9)\n         Predict: 1.0\n      Else (feature 5 > -0.0760059)\n       If (feature 1 <= 0.286432)\n        If (feature 6 <= -0.855679)\n         Predict: 1.0\n        Else (feature 6 > -0.855679)\n         Predict: 0.0\n       Else (feature 1 > 0.286432)\n        If (feature 5 <= -0.0104321)\n         Predict: 0.0\n        Else (feature 5 > -0.0104321)\n         Predict: 1.0\n     Else (feature 0 > -1.0)\n      If (feature 5 <= 0.120715)\n       If (feature 5 <= 0.052161)\n        If (feature 1 <= 0.326633)\n         Predict: 0.0\n        Else (feature 1 > 0.326633)\n         Predict: 0.0\n       Else (feature 5 > 0.052161)\n        If (feature 0 <= -0.764706)\n         Predict: 0.0\n        Else (feature 0 > -0.764706)\n         Predict: 1.0\n      Else (feature 5 > 0.120715)\n       If (feature 6 <= -0.587532)\n        If (feature 5 <= 0.290611)\n         Predict: 0.0\n        Else (feature 5 > 0.290611)\n         Predict: 1.0\n       Else (feature 6 > -0.587532)\n        If (feature 1 <= 0.18593)\n         Predict: 0.0\n        Else (feature 1 > 0.18593)\n         Predict: 1.0\n    Else (feature 7 > -0.833333)\n     If (feature 7 <= -0.566667)\n      If (feature 4 <= -0.65721)\n       If (feature 2 <= 0.508197)\n        If (feature 1 <= 0.396985)\n         Predict: 0.0\n        Else (feature 1 > 0.396985)\n         Predict: 1.0\n       Else (feature 2 > 0.508197)\n        If (feature 0 <= -0.882353)\n         Predict: 1.0\n        Else (feature 0 > -0.882353)\n         Predict: 0.0\n      Else (feature 4 > -0.65721)\n       If (feature 5 <= -0.168405)\n        Predict: 0.0\n       Else (feature 5 > -0.168405)\n        If (feature 2 <= 0.0819672)\n         Predict: 1.0\n        Else (feature 2 > 0.0819672)\n         Predict: 1.0\n     Else (feature 7 > -0.566667)\n      If (feature 1 <= 0.0653266)\n       If (feature 2 <= 0.278689)\n        If (feature 5 <= 0.171386)\n         Predict: 0.0\n        Else (feature 5 > 0.171386)\n         Predict: 0.0\n       Else (feature 2 > 0.278689)\n        If (feature 5 <= 0.207154)\n         Predict: 0.0\n        Else (feature 5 > 0.207154)\n         Predict: 1.0\n      Else (feature 1 > 0.0653266)\n       If (feature 7 <= 0.366667)\n        If (feature 7 <= -0.3)\n         Predict: 1.0\n        Else (feature 7 > -0.3)\n         Predict: 1.0\n       Else (feature 7 > 0.366667)\n        If (feature 5 <= -0.105812)\n         Predict: 0.0\n        Else (feature 5 > -0.105812)\n         Predict: 1.0\n\n"},"dateCreated":"2016-10-13T04:15:35+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:339"},{"title":"Followup questions","text":"%md\n#\nHas the accuracy improved after training the dataset using the Random Forest model? Does the accuracy improve if you increase the number of trees?\n#\nWhat did you find interesting in the output of a Random Forest classifier?\n#","dateUpdated":"2016-10-18T02:17:30+0200","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476368135409_-866471707","id":"20160531-234527_754013818","result":{"code":"SUCCESS","type":"HTML","msg":"<h1></h1>\n<p>Has the accuracy improved after training the dataset using the Random Forest model? Does the accuracy improve if you increase the number of trees?</p>\n<h1></h1>\n<p>What did you find interesting in the output of a Random Forest classifier?</p>\n<h1></h1>\n"},"dateCreated":"2016-10-13T04:15:35+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:340"},{"title":"The End","text":"%md\n\nThis concludes our lab. Hopefully by now you have a better understanding of how to run some of the Unsupervised and Supervised Machine Learning algorithms in Apache Spark. ","dateUpdated":"2016-10-18T02:32:20+0200","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476368135409_-866471707","id":"20160531-234527_409238179","result":{"code":"SUCCESS","type":"HTML","msg":"<p>This concludes our lab. Hopefully by now you have a better understanding of how to run some of the Unsupervised and Supervised Machine Learning algorithms in Apache Spark.</p>\n"},"dateCreated":"2016-10-13T04:15:35+0200","dateStarted":"2016-10-18T02:32:19+0200","dateFinished":"2016-10-18T02:32:19+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:341"},{"title":"Additional Resources","text":"%md\n\nWe hope you've enjoyed this introductory lab. Below are additional resources that you should find useful:\n\n1. [Hortonworks Apache Spark Tutorials](http://hortonworks.com/tutorials/#tuts-developers) are your natural next step where you can explore Spark in more depth.\n2. [Hortonworks Community Connection (HCC)](https://community.hortonworks.com/spaces/85/data-science.html?type=question) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Hadoop Summit: Slides & Videos](http://hadoopsummit.org/san-jose/agenda/) - a collection of presentations on Spark, Hadoop, and other Big Data related topics.\n4. [Hortonworks Apache Spark Docs](http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.0/bk_spark-component-guide/content/ch_developing-spark-apps.html) - official Spark documentation.\n5. [Hortonworks Apache Zeppelin Docs](http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.0/bk_zeppelin-component-guide/content/ch_using_zeppelin.html) - official Zeppelin documentation.","dateUpdated":"2016-10-21T06:26:10+0200","config":{"editorMode":"ace/mode/markdown","colWidth":10,"editorHide":true,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476368135409_-866471707","id":"20160531-234527_1823436759","result":{"code":"SUCCESS","type":"HTML","msg":"<p>We hope you've enjoyed this introductory lab. Below are additional resources that you should find useful:</p>\n<ol>\n<li><a href=\"http://hortonworks.com/tutorials/#tuts-developers\">Hortonworks Apache Spark Tutorials</a> are your natural next step where you can explore Spark in more depth.</li>\n<li><a href=\"https://community.hortonworks.com/spaces/85/data-science.html?type=question\">Hortonworks Community Connection (HCC)</a> is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.</li>\n<li><a href=\"http://hadoopsummit.org/san-jose/agenda/\">Hadoop Summit: Slides &amp; Videos</a> - a collection of presentations on Spark, Hadoop, and other Big Data related topics.</li>\n<li><a href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.0/bk_spark-component-guide/content/ch_developing-spark-apps.html\">Hortonworks Apache Spark Docs</a> - official Spark documentation.</li>\n<li><a href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.0/bk_zeppelin-component-guide/content/ch_using_zeppelin.html\">Hortonworks Apache Zeppelin Docs</a> - official Zeppelin documentation.</li>\n</ol>\n"},"dateCreated":"2016-10-13T04:15:35+0200","dateStarted":"2016-10-18T02:22:38+0200","dateFinished":"2016-10-18T02:22:38+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:342"},{"text":"%angular\n</br>\n<center>\n<a href=\"https://community.hortonworks.com/spaces/85/data-science.html?type=question\" target='_blank'>\n  <img src=\"http://hortonworks.com/wp-content/uploads/2016/03/logo-hcc.png\" alt=\"HCC\" style=\"width:125px;height:125px;border:0;\" align=\"middle\">\n</a>\n</center>","dateUpdated":"2016-10-21T06:26:15+0200","config":{"editorMode":"ace/mode/scala","colWidth":2,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476368135409_-866471707","id":"20160531-234527_1909965823","result":{"code":"SUCCESS","type":"ANGULAR","msg":"</br>\n<center>\n<a href=\"https://community.hortonworks.com/spaces/85/data-science.html?type=question\" target='_blank'>\n  <img src=\"http://hortonworks.com/wp-content/uploads/2016/03/logo-hcc.png\" alt=\"HCC\" style=\"width:125px;height:125px;border:0;\" align=\"middle\">\n</a>\n</center>"},"dateCreated":"2016-10-13T04:15:35+0200","dateStarted":"2016-10-18T02:38:46+0200","dateFinished":"2016-10-18T02:38:46+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:343"},{"text":"","dateUpdated":"2016-10-18T02:22:57+0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1476793367511_-1780423334","id":"20161018-142247_538149892","dateCreated":"2016-10-18T02:22:47+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:344"}],"name":"Lab 102 DS Draft: Common Machine Learning Examples (Scala)","id":"2BXMBDAXS","angularObjects":{"2C17KRZ89:shared_process":[],"2BYY922YJ:shared_process":[],"2BZFM3RS5:shared_process":[],"2BWWMQD15:shared_process":[],"2BX9JK9RG:shared_process":[],"2BYS6EPN8:shared_process":[],"2BXM16U2W:shared_process":[],"2BZKN45ZE:shared_process":[],"2BYJTTQ3C:shared_process":[],"2BY4FUMA6:shared_process":[],"2BX12UTS8:shared_process":[],"2BXFZDPB4:shared_process":[],"2BXPTJA1N:shared_process":[],"2BYHZF59F:shared_process":[],"2BZ33SWBS:shared_process":[],"2BZ233MCE:shared_process":[],"2BXTNQ9NR:shared_process":[],"2BZB9FFTE:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}