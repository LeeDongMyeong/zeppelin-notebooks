{"paragraphs":[{"text":"%md\n\n# MLlib API Guide Overview\n\nThis notebook copies all the [Spark MLlib sample code](http://spark.apache.org/docs/latest/mllib-data-types.html) from the official Spark docs for you to try in Zeppelin environment. \n\nNote: Some code samples were updated to remove dependancies on NumPy and SciPy.\n\nLanguage: Mostly Python; Scala where Python API has not yet been implemented.\nLast updated: Aug 15, 2016\n\nAuthor: Robert Hryniewicz\nTwitter: @RobHryniewicz\n\nTopics:\n- Dimensionality Reduction","dateUpdated":"2016-08-15T15:14:49+0000","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471246266899_1458113280","id":"20160815-042153_1836979317","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>MLlib API Guide Overview</h1>\n<p>This notebook copies all the <a href=\"http://spark.apache.org/docs/latest/mllib-data-types.html\">Spark MLlib sample code</a> from the official Spark docs for you to try in Zeppelin environment.</p>\n<p>Note: Some code samples were updated to remove dependancies on NumPy and SciPy.</p>\n<p>Language: Mostly Python; Scala where Python API has not yet been implemented.\n<br  />Last updated: Aug 15, 2016</p>\n<p>Author: Robert Hryniewicz\n<br  />Twitter: @RobHryniewicz</p>\n<p>Topics:</p>\n<ul>\n<li>Dimensionality Reduction</li>\n</ul>\n"},"dateCreated":"2016-08-15T07:31:06+0000","dateStarted":"2016-08-15T15:14:45+0000","dateFinished":"2016-08-15T15:14:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:148"},{"text":"%md\n\nBefore we proceed, let's verify your Spark Version. You should be running at minimum Spark 1.6.\n#\n**Note**: The first time you run `sc.version` in the paragraph below, several services will initialize in the background. This may take **1~2 min** so please **be patient**. Afterwards, each paragraph should run much more quickly since all the services will already be running in the background.","dateUpdated":"2016-08-15T07:31:06+0000","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471246266900_1456189535","id":"20160815-042306_538669140","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Before we proceed, let's verify your Spark Version. You should be running at minimum Spark 1.6.</p>\n<h1></h1>\n<p><strong>Note</strong>: The first time you run <code>sc.version</code> in the paragraph below, several services will initialize in the background. This may take <strong>1~2 min</strong> so please <strong>be patient</strong>. Afterwards, each paragraph should run much more quickly since all the services will already be running in the background.</p>\n"},"dateCreated":"2016-08-15T07:31:06+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:149"},{"text":"%md\nTo run a paragraph in a Zeppelin notebook you can either click the `play` button (blue triangle) on the right-hand side or simply press `Shift + Enter`.","dateUpdated":"2016-08-15T07:31:06+0000","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471246266900_1456189535","id":"20160815-042329_914422611","result":{"code":"SUCCESS","type":"HTML","msg":"<p>To run a paragraph in a Zeppelin notebook you can either click the <code>play</code> button (blue triangle) on the right-hand side or simply press <code>Shift + Enter</code>.</p>\n"},"dateCreated":"2016-08-15T07:31:06+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:150"},{"text":"%pyspark\nprint sc.version","dateUpdated":"2016-08-15T07:31:06+0000","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471246266900_1456189535","id":"20160815-042346_1498879383","dateCreated":"2016-08-15T07:31:06+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:151"},{"text":"%md\n\n# Dimensionality Reduction","dateUpdated":"2016-08-15T15:01:02+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471246266900_1456189535","id":"20160815-042359_2059877669","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Dimensionality Reduction</h1>\n"},"dateCreated":"2016-08-15T07:31:06+0000","dateStarted":"2016-08-15T15:00:59+0000","dateFinished":"2016-08-15T15:00:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:152"},{"title":"Singular Value Decomposition (SVD)","text":"%spark\n\nimport org.apache.spark.mllib.linalg.Matrix\nimport org.apache.spark.mllib.linalg.SingularValueDecomposition\nimport org.apache.spark.mllib.linalg.Vector\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix\n\nval data = Array(\n  Vectors.sparse(5, Seq((1, 1.0), (3, 7.0))),\n  Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),\n  Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0))\n\nval dataRDD = sc.parallelize(data, 2)\n\nval mat: RowMatrix = new RowMatrix(dataRDD)\n\n// Compute the top 5 singular values and corresponding singular vectors.\nval svd: SingularValueDecomposition[RowMatrix, Matrix] = mat.computeSVD(5, computeU = true)\nval U: RowMatrix = svd.U  // The U factor is a RowMatrix.\nval s: Vector = svd.s  // The singular values are stored in a local dense vector.\nval V: Matrix = svd.V  // The V factor is a local dense matrix.","dateUpdated":"2016-08-15T15:01:49+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471273253863_1689344612","id":"20160815-150053_1916263820","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.mllib.linalg.Matrix\nimport org.apache.spark.mllib.linalg.SingularValueDecomposition\nimport org.apache.spark.mllib.linalg.Vector\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix\ndata: Array[org.apache.spark.mllib.linalg.Vector] = Array((5,[1,3],[1.0,7.0]), [2.0,0.0,3.0,4.0,5.0], [4.0,0.0,0.0,6.0,7.0])\ndataRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = ParallelCollectionRDD[595] at parallelize at <console>:92\nmat: org.apache.spark.mllib.linalg.distributed.RowMatrix = org.apache.spark.mllib.linalg.distributed.RowMatrix@47d785de\nsvd: org.apache.spark.mllib.linalg.SingularValueDecomposition[org.apache.spark.mllib.linalg.distributed.RowMatrix,org.apache.spark.mllib.linalg.Matrix] = \nSingularValueDecomposition(org.apache.spark.mllib.linalg.distributed.RowMatrix@57f830b6,[13.029275535600473,5.368578733451684,2.5330498218813755,6.323166049206486E-8,2.0226934557075942E-8],-0.31278534337232633   0.3116713569157832    ... (5 total)\n-0.029801450130953977  -0.17133211263608739  ...\n-0.12207248163673157   0.15256470925290191   ...\n-0.7184789931874109    -0.6809628499946365   ...\n-0.6084105917199364    0.6217072292290715    ...)\nU: org.apache.spark.mllib.linalg.distributed.RowMatrix = org.apache.spark.mllib.linalg.distributed.RowMatrix@57f830b6\ns: org.apache.spark.mllib.linalg.Vector = [13.029275535600473,5.368578733451684,2.5330498218813755,6.323166049206486E-8,2.0226934557075942E-8]\nV: org.apache.spark.mllib.linalg.Matrix = \n-0.31278534337232633   0.3116713569157832    ... (5 total)\n-0.029801450130953977  -0.17133211263608739  ...\n-0.12207248163673157   0.15256470925290191   ...\n-0.7184789931874109    -0.6809628499946365   ...\n-0.6084105917199364    0.6217072292290715    ...\n"},"dateCreated":"2016-08-15T15:00:53+0000","dateStarted":"2016-08-15T15:01:49+0000","dateFinished":"2016-08-15T15:01:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:153"},{"title":"Principal Component Analysis (PCA)","text":"%spark \n\nimport org.apache.spark.mllib.linalg.Matrix\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix\n\nval data = Array(\n  Vectors.sparse(5, Seq((1, 1.0), (3, 7.0))),\n  Vectors.dense(2.0, 0.0, 3.0, 4.0, 5.0),\n  Vectors.dense(4.0, 0.0, 0.0, 6.0, 7.0))\n\nval dataRDD = sc.parallelize(data, 2)\n\nval mat: RowMatrix = new RowMatrix(dataRDD)\n\n// Compute the top 4 principal components.\n// Principal components are stored in a local dense matrix.\nval pc: Matrix = mat.computePrincipalComponents(4)\n\n// Project the rows to the linear space spanned by the top 4 principal components.\nval projected: RowMatrix = mat.multiply(pc)","dateUpdated":"2016-08-15T15:04:09+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471273309678_-996309980","id":"20160815-150149_1211138413","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.mllib.linalg.Matrix\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix\ndata: Array[org.apache.spark.mllib.linalg.Vector] = Array((5,[1,3],[1.0,7.0]), [2.0,0.0,3.0,4.0,5.0], [4.0,0.0,0.0,6.0,7.0])\ndataRDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] = ParallelCollectionRDD[607] at parallelize at <console>:108\nmat: org.apache.spark.mllib.linalg.distributed.RowMatrix = org.apache.spark.mllib.linalg.distributed.RowMatrix@154d239b\npc: org.apache.spark.mllib.linalg.Matrix = \n-0.44859172075072673  -0.28423808214073987  0.08344545257592471   0.8364102009456849    \n0.13301985745398526   -0.05621155904253121  0.044239792581370035  0.17224337841622106   \n-0.1252315635978212   0.7636264774662965    -0.578071228563837    0.2554154886635869    \n0.21650756651919933   -0.5652958773533949   -0.7955405062786798   4.858121429822393E-5  \n-0.8476512931126826   -0.11560340501314653  -0.1550117891430013   -0.4533355491646027   \nprojected: org.apache.spark.mllib.linalg.distributed.RowMatrix = org.apache.spark.mllib.linalg.distributed.RowMatrix@4251b474\n"},"dateCreated":"2016-08-15T15:01:49+0000","dateStarted":"2016-08-15T15:04:09+0000","dateFinished":"2016-08-15T15:04:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:154"},{"text":"%spark \n\nimport org.apache.spark.mllib.feature.PCA\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.rdd.RDD\n\nval data: RDD[LabeledPoint] = sc.parallelize(Seq(\n  new LabeledPoint(0, Vectors.dense(1, 0, 0, 0, 1)),\n  new LabeledPoint(1, Vectors.dense(1, 1, 0, 1, 0)),\n  new LabeledPoint(1, Vectors.dense(1, 1, 0, 0, 0)),\n  new LabeledPoint(0, Vectors.dense(1, 0, 0, 0, 0)),\n  new LabeledPoint(1, Vectors.dense(1, 1, 0, 0, 0))))\n\n// Compute the top 5 principal components.\nval pca = new PCA(5).fit(data.map(_.features))\n\n// Project vectors to the linear space spanned by the top 5 principal\n// components, keeping the label\nval projected = data.map(p => p.copy(features = pca.transform(p.features)))\n\nprojected.collect","dateUpdated":"2016-08-15T15:12:19+0000","config":{"colWidth":12,"graph":{"mode":"table","height":195,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471273411869_-1713897327","id":"20160815-150331_820615807","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.mllib.feature.PCA\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.rdd.RDD\ndata: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = ParallelCollectionRDD[641] at parallelize at <console>:141\npca: org.apache.spark.mllib.feature.PCAModel = org.apache.spark.mllib.feature.PCAModel@302d0fae\nprojected: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[645] at map at <console>:145\nres49: Array[org.apache.spark.mllib.regression.LabeledPoint] = Array((0.0,[0.5206573684395938,-0.4271322870657463,-0.7392387395392245,-9.272192293572736E-17,1.0]), (1.0,[-1.1529018904707842,-0.7155024798795668,-0.39858930271029713,1.9520404828574198E-17,1.0]), (1.0,[-0.7557893406837773,0.1721478589408803,-0.6317812811178027,-7.93016446160826E-17,1.0]), (0.0,[0.0,0.0,0.0,0.0,1.0]), (1.0,[-0.7557893406837773,0.1721478589408803,-0.6317812811178027,-7.93016446160826E-17,1.0]))\n"},"dateCreated":"2016-08-15T15:03:31+0000","dateStarted":"2016-08-15T15:12:19+0000","dateFinished":"2016-08-15T15:12:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:155"},{"text":"%spark ","dateUpdated":"2016-08-15T15:09:27+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471273767806_433647319","id":"20160815-150927_776002649","dateCreated":"2016-08-15T15:09:27+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:156"}],"name":"Spark MLlib API - Part 3","id":"2BUP9SGD1","lastReplName":{"value":"md"},"angularObjects":{"2BRHYDE45:shared_process":[],"2BQVH8HWF:shared_process":[],"2BTZ65EZ1:shared_process":[],"2BSTTAPCN:shared_process":[],"2BU44EEGW:shared_process":[],"2BSJHPYU8:shared_process":[],"2BTEBDBMC:shared_process":[],"2BQNEXNZ9:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}