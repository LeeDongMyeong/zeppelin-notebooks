{"paragraphs":[{"text":"%md\n\n# MLlib API Guide Overview\n\nThis notebook copies all the [Spark MLlib sample code](http://spark.apache.org/docs/latest/mllib-data-types.html) from the official Spark docs for you to try in Zeppelin environment. \n\nNote: Some code samples were updated to remove dependancies on NumPy and SciPy.\n\nLanguage: Mostly Python; Scala where Python API has not yet been implemented.\nLast updated: Aug 15, 2016\n\nAuthor: Robert Hryniewicz\nTwitter: @RobHryniewicz\n\nTopics:\n- Frequent Pattern Mining\n- Evaluation Metrics","dateUpdated":"2016-08-15T04:23:08+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471234913864_-581335236","id":"20160815-042153_1836979317","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>MLlib API Guide Overview</h1>\n<p>This notebook copies all the <a href=\"http://spark.apache.org/docs/latest/mllib-data-types.html\">Spark MLlib sample code</a> from the official Spark docs for you to try in Zeppelin environment.</p>\n<p>Note: Some code samples were updated to remove dependancies on NumPy and SciPy.</p>\n<p>Language: Mostly Python; Scala where Python API has not yet been implemented.\n<br  />Last updated: Aug 15, 2016</p>\n<p>Author: Robert Hryniewicz\n<br  />Twitter: @RobHryniewicz</p>\n<p>Topics:</p>\n<ul>\n<li>Frequent Pattern Mining</li>\n<li>Evaluation Metrics</li>\n</ul>\n"},"dateCreated":"2016-08-15T04:21:53+0000","dateStarted":"2016-08-15T04:23:06+0000","dateFinished":"2016-08-15T04:23:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:53"},{"text":"%md\n\nBefore we proceed, let's verify your Spark Version. You should be running at minimum Spark 1.6.\n#\n**Note**: The first time you run `sc.version` in the paragraph below, several services will initialize in the background. This may take **1~2 min** so please **be patient**. Afterwards, each paragraph should run much more quickly since all the services will already be running in the background.","dateUpdated":"2016-08-15T04:23:31+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorHide":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471234986183_216320398","id":"20160815-042306_538669140","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Before we proceed, let's verify your Spark Version. You should be running at minimum Spark 1.6.</p>\n<h1></h1>\n<p><strong>Note</strong>: The first time you run <code>sc.version</code> in the paragraph below, several services will initialize in the background. This may take <strong>1~2 min</strong> so please <strong>be patient</strong>. Afterwards, each paragraph should run much more quickly since all the services will already be running in the background.</p>\n"},"dateCreated":"2016-08-15T04:23:06+0000","dateStarted":"2016-08-15T04:23:29+0000","dateFinished":"2016-08-15T04:23:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:54"},{"text":"%md\nTo run a paragraph in a Zeppelin notebook you can either click the `play` button (blue triangle) on the right-hand side or simply press `Shift + Enter`.","dateUpdated":"2016-08-15T04:23:48+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471235009834_-1770877023","id":"20160815-042329_914422611","result":{"code":"SUCCESS","type":"HTML","msg":"<p>To run a paragraph in a Zeppelin notebook you can either click the <code>play</code> button (blue triangle) on the right-hand side or simply press <code>Shift + Enter</code>.</p>\n"},"dateCreated":"2016-08-15T04:23:29+0000","dateStarted":"2016-08-15T04:23:46+0000","dateFinished":"2016-08-15T04:23:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:55"},{"text":"%pyspark\nprint sc.version","dateUpdated":"2016-08-15T04:23:59+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471235026449_240236127","id":"20160815-042346_1498879383","result":{"code":"SUCCESS","type":"TEXT","msg":"1.6.2\n"},"dateCreated":"2016-08-15T04:23:46+0000","dateStarted":"2016-08-15T04:23:59+0000","dateFinished":"2016-08-15T04:24:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56"},{"text":"%md\n\n# Frequent Pattern Mining","dateUpdated":"2016-08-15T07:30:33+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471246205240_1715666825","id":"20160815-073005_1139577598","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Frequent Pattern Mining</h1>\n"},"dateCreated":"2016-08-15T07:30:05+0000","dateStarted":"2016-08-15T07:30:33+0000","dateFinished":"2016-08-15T07:30:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:57"},{"title":"FP-growth","text":"%pyspark\n\nfrom pyspark.mllib.fpm import FPGrowth\n\ndata = sc.textFile(\"file:///usr/hdp/current/spark-client/data/mllib/sample_libsvm_data.txt\")\ntransactions = data.map(lambda line: line.strip().split(' '))\nmodel = FPGrowth.train(transactions, minSupport=0.2, numPartitions=10)\nresult = model.freqItemsets().collect()\nfor fi in result:\n    print(fi)","dateUpdated":"2016-08-15T04:29:18+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471235039792_1010734932","id":"20160815-042359_2059877669","result":{"code":"SUCCESS","type":"TEXT","msg":"FreqItemset(items=[u'214:253'], freq=22)\nFreqItemset(items=[u'1'], freq=57)\nFreqItemset(items=[u'352:253'], freq=21)\nFreqItemset(items=[u'352:253', u'1'], freq=21)\nFreqItemset(items=[u'0'], freq=43)\nFreqItemset(items=[u'599:253'], freq=28)\nFreqItemset(items=[u'600:253'], freq=21)\nFreqItemset(items=[u'379:253'], freq=21)\nFreqItemset(items=[u'379:253', u'1'], freq=20)\nFreqItemset(items=[u'627:253'], freq=27)\nFreqItemset(items=[u'627:253', u'599:253'], freq=20)\nFreqItemset(items=[u'269:253'], freq=20)\nFreqItemset(items=[u'435:253'], freq=26)\nFreqItemset(items=[u'435:253', u'1'], freq=26)\nFreqItemset(items=[u'212:253'], freq=23)\nFreqItemset(items=[u'351:253'], freq=20)\nFreqItemset(items=[u'655:253'], freq=22)\nFreqItemset(items=[u'655:253', u'627:253'], freq=22)\nFreqItemset(items=[u'407:253'], freq=22)\nFreqItemset(items=[u'407:253', u'1'], freq=22)\nFreqItemset(items=[u'184:253'], freq=22)\nFreqItemset(items=[u'240:253'], freq=22)\n"},"dateCreated":"2016-08-15T04:23:59+0000","dateStarted":"2016-08-15T04:27:55+0000","dateFinished":"2016-08-15T04:28:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:58"},{"text":"%spark\n\nimport org.apache.spark.mllib.fpm.AssociationRules\nimport org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n\nval freqItemsets = sc.parallelize(Seq(\n  new FreqItemset(Array(\"a\"), 15L),\n  new FreqItemset(Array(\"b\"), 35L),\n  new FreqItemset(Array(\"a\", \"b\"), 12L)\n))\n\nval ar = new AssociationRules()\n  .setMinConfidence(0.8)\nval results = ar.run(freqItemsets)\n\nresults.collect().foreach { rule =>\n  println(\"[\" + rule.antecedent.mkString(\",\")\n    + \"=>\"\n    + rule.consequent.mkString(\",\") + \"],\" + rule.confidence)\n}","dateUpdated":"2016-08-15T07:23:47+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471235187039_-1828880517","id":"20160815-042627_1764671559","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.mllib.fpm.AssociationRules\nimport org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\nfreqItemsets: org.apache.spark.rdd.RDD[org.apache.spark.mllib.fpm.FPGrowth.FreqItemset[String]] = ParallelCollectionRDD[33] at parallelize at <console>:37\nar: org.apache.spark.mllib.fpm.AssociationRules = org.apache.spark.mllib.fpm.AssociationRules@407f8641\nresults: org.apache.spark.rdd.RDD[org.apache.spark.mllib.fpm.AssociationRules.Rule[String]] = MapPartitionsRDD[40] at filter at AssociationRules.scala:82\n[a=>b],0.8\n"},"dateCreated":"2016-08-15T04:26:27+0000","dateStarted":"2016-08-15T07:23:47+0000","dateFinished":"2016-08-15T07:23:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:59"},{"title":"Prefix Span","text":"%spark \n\nimport org.apache.spark.mllib.fpm.PrefixSpan\n\nval sequences = sc.parallelize(Seq(\n  Array(Array(1, 2), Array(3)),\n  Array(Array(1), Array(3, 2), Array(1, 2)),\n  Array(Array(1, 2), Array(5)),\n  Array(Array(6))\n), 2).cache()\nval prefixSpan = new PrefixSpan()\n  .setMinSupport(0.5)\n  .setMaxPatternLength(5)\nval model = prefixSpan.run(sequences)\nmodel.freqSequences.collect().foreach { freqSequence =>\n  println(\n    freqSequence.sequence.map(_.mkString(\"[\", \", \", \"]\")).mkString(\"[\", \", \", \"]\") +\n      \", \" + freqSequence.freq)\n}","dateUpdated":"2016-08-15T07:35:45+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471245733845_-890360069","id":"20160815-072213_2037316043","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.mllib.fpm.PrefixSpan\nsequences: org.apache.spark.rdd.RDD[Array[Array[Int]]] = ParallelCollectionRDD[41] at parallelize at <console>:38\nprefixSpan: org.apache.spark.mllib.fpm.PrefixSpan = org.apache.spark.mllib.fpm.PrefixSpan@4ef3832f\nmodel: org.apache.spark.mllib.fpm.PrefixSpanModel[Int] = org.apache.spark.mllib.fpm.PrefixSpanModel@25c9e80e\n[[2]], 3\n[[3]], 2\n[[1]], 3\n[[2, 1]], 3\n[[1], [3]], 2\n"},"dateCreated":"2016-08-15T07:22:13+0000","dateStarted":"2016-08-15T07:29:37+0000","dateFinished":"2016-08-15T07:29:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:60"},{"text":"%md \n\n# Evaluation Metrics","dateUpdated":"2016-08-15T07:30:36+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471246217995_-50715375","id":"20160815-073017_1601599542","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Evaluation Metrics</h1>\n"},"dateCreated":"2016-08-15T07:30:17+0000","dateStarted":"2016-08-15T07:30:33+0000","dateFinished":"2016-08-15T07:30:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:61"},{"title":"Binary Classification","text":"%spark\n\nimport org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.util.MLUtils\n\n// Load training data in LIBSVM format\nval data = MLUtils.loadLibSVMFile(sc, \"file:///usr/hdp/current/spark-client/data/mllib/sample_binary_classification_data.txt\")\n\n// Split data into training (60%) and test (40%)\nval Array(training, test) = data.randomSplit(Array(0.6, 0.4), seed = 11L)\ntraining.cache()\n\n// Run training algorithm to build the model\nval model = new LogisticRegressionWithLBFGS()\n  .setNumClasses(2)\n  .run(training)\n\n// Clear the prediction threshold so the model will return probabilities\nmodel.clearThreshold\n\n// Compute raw scores on the test set\nval predictionAndLabels = test.map { case LabeledPoint(label, features) =>\n  val prediction = model.predict(features)\n  (prediction, label)\n}\n\n// Instantiate metrics object\nval metrics = new BinaryClassificationMetrics(predictionAndLabels)\n\n// Precision by threshold\nval precision = metrics.precisionByThreshold\nprecision.foreach { case (t, p) =>\n  println(s\"Threshold: $t, Precision: $p\")\n}\n\n// Recall by threshold\nval recall = metrics.recallByThreshold\nrecall.foreach { case (t, r) =>\n  println(s\"Threshold: $t, Recall: $r\")\n}\n\n// Precision-Recall Curve\nval PRC = metrics.pr\n\n// F-measure\nval f1Score = metrics.fMeasureByThreshold\nf1Score.foreach { case (t, f) =>\n  println(s\"Threshold: $t, F-score: $f, Beta = 1\")\n}\n\nval beta = 0.5\nval fScore = metrics.fMeasureByThreshold(beta)\nf1Score.foreach { case (t, f) =>\n  println(s\"Threshold: $t, F-score: $f, Beta = 0.5\")\n}\n\n// AUPRC\nval auPRC = metrics.areaUnderPR\nprintln(\"Area under precision-recall curve = \" + auPRC)\n\n// Compute thresholds used in ROC and PR curves\nval thresholds = precision.map(_._1)\n\n// ROC Curve\nval roc = metrics.roc\n\n// AUROC\nval auROC = metrics.areaUnderROC\nprintln(\"Area under ROC = \" + auROC)","dateUpdated":"2016-08-15T14:01:44+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471246177085_-1479734087","id":"20160815-072937_1081804186","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.util.MLUtils\ndata: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[68] at map at MLUtils.scala:108\ntraining: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[69] at randomSplit at <console>:49\ntest: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[70] at randomSplit at <console>:49\nres8: training.type = MapPartitionsRDD[69] at randomSplit at <console>:49\nmodel: org.apache.spark.mllib.classification.LogisticRegressionModel = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 692, numClasses = 2, threshold = 0.5\nres9: model.type = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 692, numClasses = 2, threshold = None\npredictionAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[98] at map at <console>:51\nmetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@668a0a75\nprecision: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[107] at map at BinaryClassificationMetrics.scala:213\nrecall: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[108] at map at BinaryClassificationMetrics.scala:213\nPRC: org.apache.spark.rdd.RDD[(Double, Double)] = UnionRDD[111] at union at BinaryClassificationMetrics.scala:108\nf1Score: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[112] at map at BinaryClassificationMetrics.scala:213\nbeta: Double = 0.5\nfScore: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[113] at map at BinaryClassificationMetrics.scala:213\nauPRC: Double = 1.0\nArea under precision-recall curve = 1.0\nthresholds: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[119] at map at <console>:57\nroc: org.apache.spark.rdd.RDD[(Double, Double)] = UnionRDD[123] at UnionRDD at BinaryClassificationMetrics.scala:89\nauROC: Double = 1.0\nArea under ROC = 1.0\n"},"dateCreated":"2016-08-15T07:29:37+0000","dateStarted":"2016-08-15T14:01:44+0000","dateFinished":"2016-08-15T14:01:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:62"},{"title":"Multiclass Classification","text":"%spark\n\nimport org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS\nimport org.apache.spark.mllib.evaluation.MulticlassMetrics\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.util.MLUtils\n\n// Load training data in LIBSVM format\nval data = MLUtils.loadLibSVMFile(sc, \"file:///usr/hdp/current/spark-client/data/mllib/sample_multiclass_classification_data.txt\")\n\n// Split data into training (60%) and test (40%)\nval Array(training, test) = data.randomSplit(Array(0.6, 0.4), seed = 11L)\ntraining.cache()\n\n// Run training algorithm to build the model\nval model = new LogisticRegressionWithLBFGS()\n  .setNumClasses(3)\n  .run(training)\n\n// Compute raw scores on the test set\nval predictionAndLabels = test.map { case LabeledPoint(label, features) =>\n  val prediction = model.predict(features)\n  (prediction, label)\n}\n\n// Instantiate metrics object\nval metrics = new MulticlassMetrics(predictionAndLabels)\n\n// Confusion matrix\nprintln(\"Confusion matrix:\")\nprintln(metrics.confusionMatrix)\n\n// Overall Statistics\nval accuracy = metrics.accuracy\nprintln(\"Summary Statistics\")\nprintln(s\"Accuracy = $accuracy\")\n\n// Precision by label\nval labels = metrics.labels\nlabels.foreach { l =>\n  println(s\"Precision($l) = \" + metrics.precision(l))\n}\n\n// Recall by label\nlabels.foreach { l =>\n  println(s\"Recall($l) = \" + metrics.recall(l))\n}\n\n// False positive rate by label\nlabels.foreach { l =>\n  println(s\"FPR($l) = \" + metrics.falsePositiveRate(l))\n}\n\n// F-measure by label\nlabels.foreach { l =>\n  println(s\"F1-Score($l) = \" + metrics.fMeasure(l))\n}\n\n// Weighted stats\nprintln(s\"Weighted precision: ${metrics.weightedPrecision}\")\nprintln(s\"Weighted recall: ${metrics.weightedRecall}\")\nprintln(s\"Weighted F1 score: ${metrics.weightedFMeasure}\")\nprintln(s\"Weighted false positive rate: ${metrics.weightedFalsePositiveRate}\")","dateUpdated":"2016-08-15T14:09:49+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471269487691_367986670","id":"20160815-135807_1448346163","result":{"code":"ERROR","type":"TEXT","msg":"import org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS\nimport org.apache.spark.mllib.evaluation.MulticlassMetrics\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.util.MLUtils\ndata: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[142] at map at MLUtils.scala:108\ntraining: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[143] at randomSplit at <console>:63\ntest: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[144] at randomSplit at <console>:63\nres17: training.type = MapPartitionsRDD[143] at randomSplit at <console>:63\nmodel: org.apache.spark.mllib.classification.LogisticRegressionModel = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 8, numClasses = 3, threshold = 0.5\npredictionAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[187] at map at <console>:61\nmetrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@2d0f75df\nConfusion matrix:\n23.0  0.0   2.0   \n0.0   22.0  0.0   \n3.0   0.0   11.0  \n<console>:65: error: value accuracy is not a member of org.apache.spark.mllib.evaluation.MulticlassMetrics\n         val accuracy = metrics.accuracy\n                                ^\n"},"dateCreated":"2016-08-15T13:58:07+0000","dateStarted":"2016-08-15T14:09:49+0000","dateFinished":"2016-08-15T14:09:53+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:63"},{"title":"Multilabel Classification","text":"%spark \n\nimport org.apache.spark.mllib.evaluation.MultilabelMetrics\nimport org.apache.spark.rdd.RDD\n\nval scoreAndLabels: RDD[(Array[Double], Array[Double])] = sc.parallelize(\n  Seq((Array(0.0, 1.0), Array(0.0, 2.0)),\n    (Array(0.0, 2.0), Array(0.0, 1.0)),\n    (Array.empty[Double], Array(0.0)),\n    (Array(2.0), Array(2.0)),\n    (Array(2.0, 0.0), Array(2.0, 0.0)),\n    (Array(0.0, 1.0, 2.0), Array(0.0, 1.0)),\n    (Array(1.0), Array(1.0, 2.0))), 2)\n\n// Instantiate metrics object\nval metrics = new MultilabelMetrics(scoreAndLabels)\n\n// Summary stats\nprintln(s\"Recall = ${metrics.recall}\")\nprintln(s\"Precision = ${metrics.precision}\")\nprintln(s\"F1 measure = ${metrics.f1Measure}\")\nprintln(s\"Accuracy = ${metrics.accuracy}\")\n\n// Individual label stats\nmetrics.labels.foreach(label =>\n  println(s\"Class $label precision = ${metrics.precision(label)}\"))\nmetrics.labels.foreach(label => println(s\"Class $label recall = ${metrics.recall(label)}\"))\nmetrics.labels.foreach(label => println(s\"Class $label F1-score = ${metrics.f1Measure(label)}\"))\n\n// Micro stats\nprintln(s\"Micro recall = ${metrics.microRecall}\")\nprintln(s\"Micro precision = ${metrics.microPrecision}\")\nprintln(s\"Micro F1 measure = ${metrics.microF1Measure}\")\n\n// Hamming loss\nprintln(s\"Hamming loss = ${metrics.hammingLoss}\")\n\n// Subset accuracy\nprintln(s\"Subset accuracy = ${metrics.subsetAccuracy}\")","dateUpdated":"2016-08-15T14:10:21+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471270163775_-1303625828","id":"20160815-140923_685975012","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.mllib.evaluation.MultilabelMetrics\nimport org.apache.spark.rdd.RDD\nscoreAndLabels: org.apache.spark.rdd.RDD[(Array[Double], Array[Double])] = ParallelCollectionRDD[192] at parallelize at <console>:59\nmetrics: org.apache.spark.mllib.evaluation.MultilabelMetrics = org.apache.spark.mllib.evaluation.MultilabelMetrics@72677746\nRecall = 0.6428571428571429\nPrecision = 0.6666666666666666\nF1 measure = 0.6380952380952382\nAccuracy = 0.5476190476190476\nClass 0.0 precision = 1.0\nClass 1.0 precision = 0.6666666666666666\nClass 2.0 precision = 0.5\nClass 0.0 recall = 0.8\nClass 1.0 recall = 0.6666666666666666\nClass 2.0 recall = 0.5\nClass 0.0 F1-score = 0.888888888888889\nClass 1.0 F1-score = 0.6666666666666666\nClass 2.0 F1-score = 0.5\nMicro recall = 0.6666666666666666\nMicro precision = 0.7272727272727273\nMicro F1 measure = 0.6956521739130435\nHamming loss = 0.3333333333333333\nSubset accuracy = 0.2857142857142857\n"},"dateCreated":"2016-08-15T14:09:23+0000","dateStarted":"2016-08-15T14:10:21+0000","dateFinished":"2016-08-15T14:10:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:64"},{"title":"Ranking Systems","text":"%spark \n\nimport org.apache.spark.mllib.evaluation.{RankingMetrics, RegressionMetrics}\nimport org.apache.spark.mllib.recommendation.{ALS, Rating}\n\n// Read in the ratings data\nval ratings = sc.textFile(\"file:///usr/hdp/current/spark-client/data/mllib/sample_movielens_data.txt\").map { line =>\n  val fields = line.split(\"::\")\n  Rating(fields(0).toInt, fields(1).toInt, fields(2).toDouble - 2.5)\n}.cache()\n\n// Map ratings to 1 or 0, 1 indicating a movie that should be recommended\nval binarizedRatings = ratings.map(r => Rating(r.user, r.product,\n  if (r.rating > 0) 1.0 else 0.0)).cache()\n\n// Summarize ratings\nval numRatings = ratings.count()\nval numUsers = ratings.map(_.user).distinct().count()\nval numMovies = ratings.map(_.product).distinct().count()\nprintln(s\"Got $numRatings ratings from $numUsers users on $numMovies movies.\")\n\n// Build the model\nval numIterations = 10\nval rank = 10\nval lambda = 0.01\nval model = ALS.train(ratings, rank, numIterations, lambda)\n\n// Define a function to scale ratings from 0 to 1\ndef scaledRating(r: Rating): Rating = {\n  val scaledRating = math.max(math.min(r.rating, 1.0), 0.0)\n  Rating(r.user, r.product, scaledRating)\n}\n\n// Get sorted top ten predictions for each user and then scale from [0, 1]\nval userRecommended = model.recommendProductsForUsers(10).map { case (user, recs) =>\n  (user, recs.map(scaledRating))\n}\n\n// Assume that any movie a user rated 3 or higher (which maps to a 1) is a relevant document\n// Compare with top ten most relevant documents\nval userMovies = binarizedRatings.groupBy(_.user)\nval relevantDocuments = userMovies.join(userRecommended).map { case (user, (actual,\npredictions)) =>\n  (predictions.map(_.product), actual.filter(_.rating > 0.0).map(_.product).toArray)\n}\n\n// Instantiate metrics object\nval metrics = new RankingMetrics(relevantDocuments)\n\n// Precision at K\nArray(1, 3, 5).foreach { k =>\n  println(s\"Precision at $k = ${metrics.precisionAt(k)}\")\n}\n\n// Mean average precision\nprintln(s\"Mean average precision = ${metrics.meanAveragePrecision}\")\n\n// Normalized discounted cumulative gain\nArray(1, 3, 5).foreach { k =>\n  println(s\"NDCG at $k = ${metrics.ndcgAt(k)}\")\n}\n\n// Get predictions for each data point\nval allPredictions = model.predict(ratings.map(r => (r.user, r.product))).map(r => ((r.user,\n  r.product), r.rating))\nval allRatings = ratings.map(r => ((r.user, r.product), r.rating))\nval predictionsAndLabels = allPredictions.join(allRatings).map { case ((user, product),\n(predicted, actual)) =>\n  (predicted, actual)\n}\n\n// Get the RMSE using regression metrics\nval regressionMetrics = new RegressionMetrics(predictionsAndLabels)\nprintln(s\"RMSE = ${regressionMetrics.rootMeanSquaredError}\")\n\n// R-squared\nprintln(s\"R-squared = ${regressionMetrics.r2}\")","dateUpdated":"2016-08-15T14:13:48+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471270221865_-813763858","id":"20160815-141021_2147173329","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.mllib.evaluation.{RankingMetrics, RegressionMetrics}\nimport org.apache.spark.mllib.recommendation.{ALS, Rating}\nratings: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[218] at map at <console>:70\nbinarizedRatings: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] = MapPartitionsRDD[219] at map at <console>:72\nnumRatings: Long = 1501\nnumUsers: Long = 30\nnumMovies: Long = 100\nGot 1501 ratings from 30 users on 100 movies.\nnumIterations: Int = 10\nrank: Int = 10\nlambda: Double = 0.01\nmodel: org.apache.spark.mllib.recommendation.MatrixFactorizationModel = org.apache.spark.mllib.recommendation.MatrixFactorizationModel@27c14ebb\nscaledRating: (r: org.apache.spark.mllib.recommendation.Rating)org.apache.spark.mllib.recommendation.Rating\nuserRecommended: org.apache.spark.rdd.RDD[(Int, Array[org.apache.spark.mllib.recommendation.Rating])] = MapPartitionsRDD[442] at map at <console>:82\nuserMovies: org.apache.spark.rdd.RDD[(Int, Iterable[org.apache.spark.mllib.recommendation.Rating])] = ShuffledRDD[444] at groupBy at <console>:74\nrelevantDocuments: org.apache.spark.rdd.RDD[(Array[Int], Array[Int])] = MapPartitionsRDD[448] at map at <console>:88\nmetrics: org.apache.spark.mllib.evaluation.RankingMetrics[Int] = org.apache.spark.mllib.evaluation.RankingMetrics@156d2502\nPrecision at 1 = 0.5\nPrecision at 3 = 0.4555555555555555\nPrecision at 5 = 0.52\nMean average precision = 0.2745729809941534\nNDCG at 1 = 0.5\nNDCG at 3 = 0.45437815707576923\nNDCG at 5 = 0.49912872714487394\nallPredictions: org.apache.spark.rdd.RDD[((Int, Int), Double)] = MapPartitionsRDD[472] at map at <console>:80\nallRatings: org.apache.spark.rdd.RDD[((Int, Int), Double)] = MapPartitionsRDD[473] at map at <console>:72\npredictionsAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[477] at map at <console>:84\nregressionMetrics: org.apache.spark.mllib.evaluation.RegressionMetrics = org.apache.spark.mllib.evaluation.RegressionMetrics@1987da4f\nRMSE = 0.2476241337942452\nR-squared = 0.9564716833177382\n"},"dateCreated":"2016-08-15T14:10:21+0000","dateStarted":"2016-08-15T14:13:48+0000","dateFinished":"2016-08-15T14:13:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:65"},{"title":"Regression Model Evaluation","text":"%pyspark \n\nfrom pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD\nfrom pyspark.mllib.evaluation import RegressionMetrics\nfrom pyspark.mllib.linalg import DenseVector\n\n# Load and parse the data\ndef parsePoint(line):\n    values = line.split()\n    return LabeledPoint(float(values[0]),\n                        DenseVector([float(x.split(':')[1]) for x in values[1:]]))\n\ndata = sc.textFile(\"file:///usr/hdp/current/spark-client/data/mllib/sample_linear_regression_data.txt\")\nparsedData = data.map(parsePoint)\n\n# Build the model\nmodel = LinearRegressionWithSGD.train(parsedData)\n\n# Get predictions\nvaluesAndPreds = parsedData.map(lambda p: (float(model.predict(p.features)), p.label))\n\n# Instantiate metrics object\nmetrics = RegressionMetrics(valuesAndPreds)\n\n# Squared Error\nprint(\"MSE = %s\" % metrics.meanSquaredError)\nprint(\"RMSE = %s\" % metrics.rootMeanSquaredError)\n\n# R-squared\nprint(\"R-squared = %s\" % metrics.r2)\n\n# Mean absolute error\nprint(\"MAE = %s\" % metrics.meanAbsoluteError)\n\n# Explained variance\nprint(\"Explained variance = %s\" % metrics.explainedVariance)","dateUpdated":"2016-08-15T14:17:47+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471270252040_494644558","id":"20160815-141052_957261148","result":{"code":"SUCCESS","type":"TEXT","msg":"MSE = 103.309686818\nRMSE = 10.1641372884\nR-squared = 0.0276391109678\nMAE = 8.14869190795\nExplained variance = 2.88839520172\n"},"dateCreated":"2016-08-15T14:10:52+0000","dateStarted":"2016-08-15T14:17:47+0000","dateFinished":"2016-08-15T14:17:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:66"},{"text":"%spark ","dateUpdated":"2016-08-15T14:15:05+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471270505832_1829690949","id":"20160815-141505_1909011866","dateCreated":"2016-08-15T14:15:05+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:67"}],"name":"Spark MLlib API - Part 2","id":"2BW6F6XTU","lastReplName":{"value":"pyspark"},"angularObjects":{"2BRHYDE45:shared_process":[],"2BQVH8HWF:shared_process":[],"2BTZ65EZ1:shared_process":[],"2BSTTAPCN:shared_process":[],"2BU44EEGW:shared_process":[],"2BSJHPYU8:shared_process":[],"2BTEBDBMC:shared_process":[],"2BQNEXNZ9:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}