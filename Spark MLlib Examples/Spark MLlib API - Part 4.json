{"paragraphs":[{"text":"%md\n\n# MLlib API Guide Overview\n\nThis notebook copies all the [Spark MLlib sample code](http://spark.apache.org/docs/latest/mllib-data-types.html) from the official Spark docs for you to try in Zeppelin environment. \n\nNote: Some code samples were updated to remove dependancies on NumPy and SciPy.\n\nLanguage: Mostly Python; Scala where Python API has not yet been implemented.\nLast updated: Aug 15, 2016\n\nAuthor: Robert Hryniewicz\nTwitter: @RobHryniewicz\n\nTopics:\n- Feature Extraction","dateUpdated":"2016-08-15T15:15:31+0000","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471274115271_-186646173","id":"20160815-042153_1836979317","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>MLlib API Guide Overview</h1>\n<p>This notebook copies all the <a href=\"http://spark.apache.org/docs/latest/mllib-data-types.html\">Spark MLlib sample code</a> from the official Spark docs for you to try in Zeppelin environment.</p>\n<p>Note: Some code samples were updated to remove dependancies on NumPy and SciPy.</p>\n<p>Language: Mostly Python; Scala where Python API has not yet been implemented.\n<br  />Last updated: Aug 15, 2016</p>\n<p>Author: Robert Hryniewicz\n<br  />Twitter: @RobHryniewicz</p>\n<p>Topics:</p>\n<ul>\n<li>Feature Extraction</li>\n</ul>\n"},"dateCreated":"2016-08-15T15:15:15+0000","dateStarted":"2016-08-15T15:15:28+0000","dateFinished":"2016-08-15T15:15:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:53"},{"text":"%md\n\nBefore we proceed, let's verify your Spark Version. You should be running at minimum Spark 1.6.\n#\n**Note**: The first time you run `sc.version` in the paragraph below, several services will initialize in the background. This may take **1~2 min** so please **be patient**. Afterwards, each paragraph should run much more quickly since all the services will already be running in the background.","dateUpdated":"2016-08-15T15:15:15+0000","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471274115272_-188569918","id":"20160815-042306_538669140","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Before we proceed, let's verify your Spark Version. You should be running at minimum Spark 1.6.</p>\n<h1></h1>\n<p><strong>Note</strong>: The first time you run <code>sc.version</code> in the paragraph below, several services will initialize in the background. This may take <strong>1~2 min</strong> so please <strong>be patient</strong>. Afterwards, each paragraph should run much more quickly since all the services will already be running in the background.</p>\n"},"dateCreated":"2016-08-15T15:15:15+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:54"},{"text":"%md\nTo run a paragraph in a Zeppelin notebook you can either click the `play` button (blue triangle) on the right-hand side or simply press `Shift + Enter`.","dateUpdated":"2016-08-15T15:15:15+0000","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471274115272_-188569918","id":"20160815-042329_914422611","result":{"code":"SUCCESS","type":"HTML","msg":"<p>To run a paragraph in a Zeppelin notebook you can either click the <code>play</code> button (blue triangle) on the right-hand side or simply press <code>Shift + Enter</code>.</p>\n"},"dateCreated":"2016-08-15T15:15:15+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:55"},{"text":"%pyspark\nprint sc.version","dateUpdated":"2016-08-15T15:15:15+0000","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471274115272_-188569918","id":"20160815-042346_1498879383","dateCreated":"2016-08-15T15:15:15+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:56"},{"text":"%md\n\n# Feature Extraction","dateUpdated":"2016-08-15T15:16:50+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471274115272_-188569918","id":"20160815-042359_2059877669","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Feature Extraction</h1>\n"},"dateCreated":"2016-08-15T15:15:15+0000","dateStarted":"2016-08-15T15:16:36+0000","dateFinished":"2016-08-15T15:16:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:57"},{"title":"TF-IDF","text":"%pyspark\n\nfrom pyspark.mllib.feature import HashingTF, IDF\n\n# Load documents (one per line).\ndocuments = sc.textFile(\"file:///usr/hdp/current/spark-client/data/mllib/kmeans_data.txt\").map(lambda line: line.split(\" \"))\n\nhashingTF = HashingTF()\ntf = hashingTF.transform(documents)\n\n# While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n# First to compute the IDF vector and second to scale the term frequencies by IDF.\ntf.cache()\nidf = IDF().fit(tf)\ntfidf = idf.transform(tf)\n\n# spark.mllib's IDF implementation provides an option for ignoring terms\n# which occur in less than a minimum number of documents.\n# In such cases, the IDF for these terms is set to 0.\n# This feature can be used by passing the minDocFreq value to the IDF constructor.\nidfIgnore = IDF(minDocFreq=2).fit(tf)\ntfidfIgnore = idf.transform(tf)\n\nprint tfidf.collect()","dateUpdated":"2016-08-15T15:26:58+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471274196095_-1981976486","id":"20160815-151636_111135636","result":{"code":"SUCCESS","type":"TEXT","msg":"[SparseVector(1048576, {1046921: 3.7583}), SparseVector(1048576, {1046920: 3.7583}), SparseVector(1048576, {1046923: 3.7583}), SparseVector(1048576, {892732: 3.7583}), SparseVector(1048576, {892733: 3.7583}), SparseVector(1048576, {892734: 3.7583})]\n"},"dateCreated":"2016-08-15T15:16:36+0000","dateStarted":"2016-08-15T15:26:58+0000","dateFinished":"2016-08-15T15:26:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:58"},{"title":"Word2Vec","text":"%spark\n\nimport org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}\n\nval input = sc.textFile(\"file:///usr/hdp/current/spark-client/data/mllib/sample_lda_data.txt\").map(line => line.split(\" \").toSeq)\n\nval word2vec = new Word2Vec()\n\nval model = word2vec.fit(input)\n\nval synonyms = model.findSynonyms(\"1\", 5)\n\n// Preview input\ninput.take(20)\n\nfor((synonym, cosineSimilarity) <- synonyms) {\n  println(s\"$synonym $cosineSimilarity\")\n}\n\n// Save and load model\n//model.save(sc, \"myModelPath\")\nval sameModel = Word2VecModel.load(sc, \"myModelPath\")","dateUpdated":"2016-08-15T15:46:52+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471274475663_1777049271","id":"20160815-152115_1043669814","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.mllib.feature.{Word2Vec, Word2VecModel}\ninput: org.apache.spark.rdd.RDD[Seq[String]] = MapPartitionsRDD[1187] at map at <console>:163\nword2vec: org.apache.spark.mllib.feature.Word2Vec = org.apache.spark.mllib.feature.Word2Vec@311e01c2\nmodel: org.apache.spark.mllib.feature.Word2VecModel = org.apache.spark.mllib.feature.Word2VecModel@46a332fb\nsynonyms: Array[(String, Double)] = Array((3,0.008804913590946983), (2,0.008534942216095904), (0,0.007708196102618548), (4,0.006913486691132216), (9,7.303871222696393E-4))\nres63: Array[Seq[String]] = Array(WrappedArray(1, 2, 6, 0, 2, 3, 1, 1, 0, 0, 3), WrappedArray(1, 3, 0, 1, 3, 0, 0, 2, 0, 0, 1), WrappedArray(1, 4, 1, 0, 0, 4, 9, 0, 1, 2, 0), WrappedArray(2, 1, 0, 3, 0, 0, 5, 0, 2, 3, 9), WrappedArray(3, 1, 1, 9, 3, 0, 2, 0, 0, 1, 3), WrappedArray(4, 2, 0, 3, 4, 5, 1, 1, 1, 4, 0), WrappedArray(2, 1, 0, 3, 0, 0, 5, 0, 2, 2, 9), WrappedArray(1, 1, 1, 9, 2, 1, 2, 0, 0, 1, 3), WrappedArray(4, 4, 0, 3, 4, 2, 1, 3, 0, 0, 0), WrappedArray(2, 8, 2, 0, 3, 0, 2, 0, 2, 7, 2), WrappedArray(1, 1, 1, 9, 0, 2, 2, 0, 0, 3, 3), WrappedArray(4, 1, 0, 0, 4, 5, 1, 3, 0, 1, 0))\n3 0.008804913590946983\n2 0.008534942216095904\n0 0.007708196102618548\n4 0.006913486691132216\n9 7.303871222696393E-4\nsameModel: org.apache.spark.mllib.feature.Word2VecModel = org.apache.spark.mllib.feature.Word2VecModel@49fbf13\n"},"dateCreated":"2016-08-15T15:21:15+0000","dateStarted":"2016-08-15T15:46:52+0000","dateFinished":"2016-08-15T15:46:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:59"},{"title":"Standard Scaler","text":"%pyspark \n\nfrom pyspark.mllib.feature import StandardScaler, StandardScalerModel\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.mllib.util import MLUtils\n\ndata = MLUtils.loadLibSVMFile(sc, \"file:///usr/hdp/current/spark-client/data/mllib/sample_libsvm_data.txt\")\nlabel = data.map(lambda x: x.label)\nfeatures = data.map(lambda x: x.features)\n\nscaler1 = StandardScaler().fit(features)\nscaler2 = StandardScaler(withMean=True, withStd=True).fit(features)\n\n# data1 will be unit variance.\ndata1 = label.zip(scaler1.transform(features))\n\n# Without converting the features into dense vectors, transformation with zero mean will raise\n# exception on sparse vector.\n# data2 will be unit variance and zero mean.\ndata2 = label.zip(scaler2.transform(features.map(lambda x: Vectors.dense(x.toArray()))))\n\nprint data1.take(1)\nprint data2.take(1)","dateUpdated":"2016-08-15T15:31:39+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471274877861_-1916007636","id":"20160815-152757_1059964366","result":{"code":"SUCCESS","type":"TEXT","msg":"[(0.0, SparseVector(692, {127: 0.5468, 128: 1.5923, 129: 2.4354, 130: 1.7081, 131: 0.7335, 154: 0.4346, 155: 2.0985, 156: 2.2563, 157: 2.2368, 158: 2.2269, 159: 2.2555, 181: 0.4713, 182: 2.0575, 183: 2.3318, 184: 2.3761, 185: 2.1237, 186: 2.0452, 187: 2.2657, 188: 0.6339, 189: 0.1022, 207: 0.1056, 208: 0.5395, 209: 1.9268, 210: 2.2383, 211: 2.3018, 212: 2.3568, 213: 1.8002, 214: 0.7116, 215: 2.2256, 216: 2.4032, 217: 1.5931, 235: 1.5394, 236: 2.188, 237: 2.1493, 238: 2.2924, 239: 2.3889, 240: 2.3155, 241: 2.2653, 242: 0.8445, 243: 1.7094, 244: 2.2496, 245: 1.8613, 262: 0.5062, 263: 2.0796, 264: 2.2201, 265: 2.199, 266: 1.7299, 267: 1.083, 268: 2.1786, 269: 2.0345, 270: 0.4392, 271: 0.7218, 272: 2.2177, 273: 1.6764, 289: 0.4794, 290: 2.214, 291: 2.3569, 292: 2.2283, 293: 1.6322, 294: 0.1087, 295: 0.6833, 296: 1.0411, 297: 0.1941, 300: 2.277, 301: 2.3083, 302: 0.5395, 316: 0.3967, 317: 1.6059, 318: 2.3539, 319: 2.1535, 320: 1.9834, 321: 0.8017, 328: 2.4941, 329: 2.3661, 330: 1.7473, 343: 0.0763, 344: 1.7606, 345: 2.3044, 346: 2.2526, 347: 0.7221, 348: 0.2063, 349: 0.2749, 356: 2.5746, 357: 2.4037, 358: 1.9606, 371: 0.591, 372: 2.4489, 373: 2.3315, 374: 0.6414, 384: 2.5939, 385: 2.4196, 386: 1.8986, 399: 2.0016, 400: 2.3333, 401: 1.898, 412: 2.6198, 413: 2.4525, 414: 1.9856, 426: 0.783, 427: 2.3838, 428: 2.2811, 429: 1.1435, 440: 2.5745, 441: 2.4723, 442: 1.5716, 454: 0.8453, 455: 2.3533, 456: 2.1201, 457: 0.2628, 466: 0.0965, 467: 1.4473, 468: 2.5738, 469: 1.8242, 470: 0.1367, 482: 0.828, 483: 2.3418, 484: 2.0701, 493: 0.1335, 494: 1.4911, 495: 2.4922, 496: 2.224, 497: 0.7444, 510: 0.8429, 511: 2.314, 512: 1.3481, 520: 0.6477, 521: 1.9663, 522: 2.3879, 523: 1.6775, 538: 0.8901, 539: 2.3049, 540: 2.0206, 547: 1.1411, 548: 2.5098, 549: 2.3552, 550: 1.5115, 566: 1.0072, 567: 2.4162, 568: 2.2029, 569: 1.2616, 570: 0.4172, 571: 0.248, 572: 0.8054, 573: 1.6634, 574: 2.0511, 575: 2.349, 576: 2.0094, 577: 1.4659, 578: 0.5412, 594: 1.4063, 595: 2.5988, 596: 2.2365, 597: 2.175, 598: 1.9974, 599: 1.9218, 600: 2.3287, 601: 2.2914, 602: 2.2011, 603: 1.6439, 604: 1.1151, 622: 0.6799, 623: 2.7625, 624: 2.4617, 625: 2.2485, 626: 2.208, 627: 2.323, 628: 2.306, 629: 2.1166, 630: 1.2687, 651: 0.5922, 652: 1.7772, 653: 2.3485, 654: 2.1973, 655: 2.25, 656: 1.2522, 657: 0.3419}))]\n[(0.0, DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1, -0.1, -0.1357, -0.1287, -0.1584, -0.1689, -0.1934, -0.1068, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1, -0.1, -0.1338, -0.136, -0.2777, -0.4448, -0.5367, -0.6242, -0.1413, 0.8975, 1.6834, 1.0786, 0.2778, -0.2784, -0.1424, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1, -0.1, -0.1238, -0.1741, -0.2875, -0.436, -0.6436, -0.804, -0.5563, 0.9701, 1.0756, 1.0633, 1.2936, 1.5017, -0.4409, -0.2643, -0.1677, -0.1, -0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1, -0.1508, -0.1988, -0.3025, -0.4146, -0.5285, -0.7426, -0.4867, 0.8363, 0.9642, 0.8776, 0.7981, 0.8759, 1.3475, 0.0532, -0.2638, -0.2351, -0.1, -0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1473, -0.2123, -0.293, -0.4025, -0.4646, -0.5713, -0.3357, 0.8337, 0.95, 0.9619, 0.8119, 0.3863, -0.4976, 1.1607, 1.6697, 1.1153, -0.2902, -0.1147, -0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1, -0.1737, -0.2422, -0.3911, -0.4552, -0.6045, 0.7703, 1.2783, 1.0811, 1.0242, 1.0347, 0.9802, 0.9655, -0.316, 0.6902, 1.5056, 1.2792, -0.3696, -0.1914, -0.1051, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.138, -0.2088, -0.3229, -0.4268, -0.5353, -0.2181, 1.2681, 1.352, 1.2355, 0.511, -0.174, 1.091, 1.0527, -0.497, -0.1119, 1.4806, 1.0511, -0.4797, -0.2665, -0.139, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1815, -0.2766, -0.3899, -0.4813, -0.1714, 1.4931, 1.6222, 1.465, 0.6979, -1.1195, -0.5165, 0.1072, -0.5779, -0.7185, -0.6714, 1.5629, 1.6563, 0.0125, -0.372, -0.1704, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2219, -0.3518, -0.4713, -0.1571, 0.923, 1.6792, 1.5087, 1.3118, -0.1217, -1.1901, -1.1254, -0.818, -0.6479, -0.5726, -0.5227, 1.8384, 1.7465, 1.1504, -0.4304, -0.2483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2843, -0.4402, -0.4486, 1.1265, 1.6053, 1.6179, 0.1574, -0.3971, -0.6615, -1.1778, -1.0609, -0.7651, -0.5743, -0.4355, -0.439, 1.9493, 1.7853, 1.3685, -0.4806, -0.2961, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.3259, -0.481, 0.027, 1.7985, 1.6685, 0.0692, -0.4706, -0.6015, -1.0855, -1.1359, -1.0485, -0.693, -0.4395, -0.3876, -0.4275, 1.9906, 1.7809, 1.2915, -0.4895, -0.3128, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1, -0.3648, -0.52, 1.3905, 1.6753, 1.2655, -0.4698, -0.4063, -0.6375, -1.0602, -1.1191, -1.034, -0.558, -0.3725, -0.3325, -0.455, 2.033, 1.825, 1.4226, -0.4758, -0.3462, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1157, -0.4181, 0.2275, 1.734, 1.595, 0.5263, -0.434, -0.4101, -0.7107, -1.0713, -1.1114, -0.9221, -0.4328, -0.2785, -0.3406, -0.4726, 1.9588, 1.8771, 1.0205, -0.4533, -0.3306, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1415, -0.448, 0.2602, 1.6644, 1.4284, -0.3049, -0.4354, -0.507, -0.7449, -1.0441, -1.0922, -0.7898, -0.3971, -0.239, -0.3188, 0.9086, 1.9452, 1.2092, -0.3608, -0.4134, -0.2729, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1694, -0.456, 0.2275, 1.6294, 1.3892, -0.5733, -0.5166, -0.5899, -0.7526, -1.0396, -1.026, -0.6517, -0.4169, -0.2134, 0.9901, 1.8713, 1.5363, 0.1588, -0.4628, -0.376, -0.2433, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.2164, -0.4189, 0.2681, 1.5761, 0.6285, -0.6115, -0.6262, -0.6558, -0.813, -1.0641, -0.8976, -0.6301, 0.177, 1.4175, 1.7782, 0.9774, -0.6336, -0.494, -0.412, -0.3218, -0.2188, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1784, -0.3798, 0.3457, 1.5867, 1.2237, -0.7649, -0.8005, -0.804, -0.9076, -1.0231, -0.9076, 0.4239, 1.8329, 1.684, 0.7991, -0.7001, -0.5941, -0.4748, -0.3513, -0.2637, -0.1811, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1344, -0.3229, 0.5249, 1.7598, 1.3422, 0.3071, -0.6148, -0.8446, -0.4383, 0.4311, 0.9389, 1.414, 1.1825, 0.6781, -0.2101, -0.6059, -0.5058, -0.4191, -0.3035, -0.207, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1, -0.1862, 0.9773, 1.9703, 1.404, 1.0334, 0.7065, 0.5383, 0.9214, 0.951, 0.9818, 0.5838, 0.1799, -0.7994, -0.668, -0.5031, -0.4184, -0.3189, -0.2282, -0.1715, 0.0, 0.0, -0.1, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1, -0.1277, 0.4065, 2.2617, 1.7001, 1.1174, 0.8526, 0.7972, 0.9024, 0.6975, -0.0193, -1.0764, -0.8589, -0.6716, -0.5148, -0.4466, -0.3258, -0.2422, -0.1195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.1, -0.156, 0.2961, 1.2624, 1.5267, 1.1794, 1.1322, 0.24, -0.6698, -0.9531, -0.7473, -0.5558, -0.4777, -0.3928, -0.3003, -0.1981, -0.1433, -0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.142, -0.1421, -0.1612, -0.3261, -0.3703, -0.3936, -0.3809, -0.3466, -0.4038, -0.3488, -0.2193, -0.1792, -0.1559, -0.1345]))]\n"},"dateCreated":"2016-08-15T15:27:57+0000","dateStarted":"2016-08-15T15:31:39+0000","dateFinished":"2016-08-15T15:31:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:60"},{"title":"Normalizer","text":"%pyspark \n\nfrom pyspark.mllib.feature import Normalizer\nfrom pyspark.mllib.util import MLUtils\n\ndata = MLUtils.loadLibSVMFile(sc, \"file:///usr/hdp/current/spark-client/data/mllib/sample_libsvm_data.txt\")\nlabels = data.map(lambda x: x.label)\nfeatures = data.map(lambda x: x.features)\n\nnormalizer1 = Normalizer()\nnormalizer2 = Normalizer(p=float(\"inf\"))\n\n# Each sample in data1 will be normalized using $L^2$ norm.\ndata1 = labels.zip(normalizer1.transform(features))\n\n# Each sample in data2 will be normalized using $L^\\infty$ norm.\ndata2 = labels.zip(normalizer2.transform(features))\n\nprint data1.take(1)\nprint data2.take(1)","dateUpdated":"2016-08-15T15:33:15+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471275021853_-833108048","id":"20160815-153021_1752682621","result":{"code":"SUCCESS","type":"TEXT","msg":"[(0.0, SparseVector(692, {127: 0.0196, 128: 0.0612, 129: 0.0974, 130: 0.0612, 131: 0.0192, 154: 0.0185, 155: 0.0916, 156: 0.097, 157: 0.097, 158: 0.097, 159: 0.0912, 181: 0.0208, 182: 0.0874, 183: 0.0974, 184: 0.097, 185: 0.092, 186: 0.0897, 187: 0.097, 188: 0.0219, 189: 0.0023, 207: 0.0038, 208: 0.0231, 209: 0.0862, 210: 0.097, 211: 0.0974, 212: 0.097, 213: 0.0777, 214: 0.0323, 215: 0.097, 216: 0.0974, 217: 0.047, 235: 0.0627, 236: 0.097, 237: 0.097, 238: 0.097, 239: 0.0974, 240: 0.097, 241: 0.097, 242: 0.0369, 243: 0.0727, 244: 0.0974, 245: 0.0643, 262: 0.0196, 263: 0.0916, 264: 0.0974, 265: 0.0974, 266: 0.0731, 267: 0.0439, 268: 0.0974, 269: 0.0878, 270: 0.0181, 271: 0.0304, 272: 0.0981, 273: 0.0647, 289: 0.0185, 290: 0.0916, 291: 0.097, 292: 0.097, 293: 0.0689, 294: 0.0046, 295: 0.0289, 296: 0.0466, 297: 0.0081, 300: 0.0974, 301: 0.0935, 302: 0.0192, 316: 0.0146, 317: 0.0635, 318: 0.0974, 319: 0.0897, 320: 0.0801, 321: 0.0323, 328: 0.0974, 329: 0.097, 330: 0.0635, 343: 0.0027, 344: 0.0685, 345: 0.097, 346: 0.0924, 347: 0.0273, 348: 0.0073, 349: 0.0108, 356: 0.0974, 357: 0.097, 358: 0.0751, 371: 0.0219, 372: 0.097, 373: 0.097, 374: 0.0242, 384: 0.0974, 385: 0.097, 386: 0.0751, 399: 0.0762, 400: 0.0974, 401: 0.0731, 412: 0.0981, 413: 0.0974, 414: 0.0754, 426: 0.0293, 427: 0.0947, 428: 0.097, 429: 0.0431, 440: 0.0974, 441: 0.097, 442: 0.057, 454: 0.0327, 455: 0.097, 456: 0.0885, 457: 0.0096, 466: 0.0027, 467: 0.052, 468: 0.0974, 469: 0.0716, 470: 0.0046, 482: 0.0327, 483: 0.097, 484: 0.0858, 493: 0.0027, 494: 0.0504, 495: 0.097, 496: 0.0866, 497: 0.0273, 510: 0.0327, 511: 0.097, 512: 0.0558, 520: 0.0185, 521: 0.0635, 522: 0.097, 523: 0.0666, 538: 0.0331, 539: 0.0974, 540: 0.0866, 547: 0.0439, 548: 0.0916, 549: 0.0974, 550: 0.0624, 566: 0.0327, 567: 0.097, 568: 0.0958, 569: 0.0562, 570: 0.0185, 571: 0.0112, 572: 0.0327, 573: 0.0685, 574: 0.0866, 575: 0.0974, 576: 0.0858, 577: 0.0643, 578: 0.0216, 594: 0.0327, 595: 0.097, 596: 0.097, 597: 0.097, 598: 0.0881, 599: 0.0828, 600: 0.097, 601: 0.097, 602: 0.097, 603: 0.0754, 604: 0.05, 622: 0.0108, 623: 0.0766, 624: 0.097, 625: 0.097, 626: 0.0974, 627: 0.097, 628: 0.097, 629: 0.0897, 630: 0.0558, 651: 0.0096, 652: 0.0493, 653: 0.097, 654: 0.0974, 655: 0.097, 656: 0.0543, 657: 0.0142}))]\n[(0.0, SparseVector(692, {127: 0.2, 128: 0.6235, 129: 0.9922, 130: 0.6235, 131: 0.1961, 154: 0.1882, 155: 0.9333, 156: 0.9882, 157: 0.9882, 158: 0.9882, 159: 0.9294, 181: 0.2118, 182: 0.8902, 183: 0.9922, 184: 0.9882, 185: 0.9373, 186: 0.9137, 187: 0.9882, 188: 0.2235, 189: 0.0235, 207: 0.0392, 208: 0.2353, 209: 0.8784, 210: 0.9882, 211: 0.9922, 212: 0.9882, 213: 0.7922, 214: 0.3294, 215: 0.9882, 216: 0.9922, 217: 0.4784, 235: 0.6392, 236: 0.9882, 237: 0.9882, 238: 0.9882, 239: 0.9922, 240: 0.9882, 241: 0.9882, 242: 0.3765, 243: 0.7412, 244: 0.9922, 245: 0.6549, 262: 0.2, 263: 0.9333, 264: 0.9922, 265: 0.9922, 266: 0.7451, 267: 0.4471, 268: 0.9922, 269: 0.8941, 270: 0.1843, 271: 0.3098, 272: 1.0, 273: 0.6588, 289: 0.1882, 290: 0.9333, 291: 0.9882, 292: 0.9882, 293: 0.702, 294: 0.0471, 295: 0.2941, 296: 0.4745, 297: 0.0824, 300: 0.9922, 301: 0.9529, 302: 0.1961, 316: 0.149, 317: 0.6471, 318: 0.9922, 319: 0.9137, 320: 0.8157, 321: 0.3294, 328: 0.9922, 329: 0.9882, 330: 0.6471, 343: 0.0275, 344: 0.698, 345: 0.9882, 346: 0.9412, 347: 0.2784, 348: 0.0745, 349: 0.1098, 356: 0.9922, 357: 0.9882, 358: 0.7647, 371: 0.2235, 372: 0.9882, 373: 0.9882, 374: 0.2471, 384: 0.9922, 385: 0.9882, 386: 0.7647, 399: 0.7765, 400: 0.9922, 401: 0.7451, 412: 1.0, 413: 0.9922, 414: 0.7686, 426: 0.298, 427: 0.9647, 428: 0.9882, 429: 0.4392, 440: 0.9922, 441: 0.9882, 442: 0.5804, 454: 0.3333, 455: 0.9882, 456: 0.902, 457: 0.098, 466: 0.0275, 467: 0.5294, 468: 0.9922, 469: 0.7294, 470: 0.0471, 482: 0.3333, 483: 0.9882, 484: 0.8745, 493: 0.0275, 494: 0.5137, 495: 0.9882, 496: 0.8824, 497: 0.2784, 510: 0.3333, 511: 0.9882, 512: 0.5686, 520: 0.1882, 521: 0.6471, 522: 0.9882, 523: 0.6784, 538: 0.3373, 539: 0.9922, 540: 0.8824, 547: 0.4471, 548: 0.9333, 549: 0.9922, 550: 0.6353, 566: 0.3333, 567: 0.9882, 568: 0.9765, 569: 0.5725, 570: 0.1882, 571: 0.1137, 572: 0.3333, 573: 0.698, 574: 0.8824, 575: 0.9922, 576: 0.8745, 577: 0.6549, 578: 0.2196, 594: 0.3333, 595: 0.9882, 596: 0.9882, 597: 0.9882, 598: 0.898, 599: 0.8431, 600: 0.9882, 601: 0.9882, 602: 0.9882, 603: 0.7686, 604: 0.5098, 622: 0.1098, 623: 0.7804, 624: 0.9882, 625: 0.9882, 626: 0.9922, 627: 0.9882, 628: 0.9882, 629: 0.9137, 630: 0.5686, 651: 0.098, 652: 0.502, 653: 0.9882, 654: 0.9922, 655: 0.9882, 656: 0.5529, 657: 0.1451}))]\n"},"dateCreated":"2016-08-15T15:30:21+0000","dateStarted":"2016-08-15T15:33:15+0000","dateFinished":"2016-08-15T15:33:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:61"},{"title":"Chi Sq Selector","text":"%spark\n\nimport org.apache.spark.mllib.feature.ChiSqSelector\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.util.MLUtils\n\n// Load some data in libsvm format\nval data = MLUtils.loadLibSVMFile(sc, \"file:///usr/hdp/current/spark-client/data/mllib/sample_libsvm_data.txt\")\n// Discretize data in 16 equal bins since ChiSqSelector requires categorical features\n// Even though features are doubles, the ChiSqSelector treats each unique value as a category\nval discretizedData = data.map { lp =>\n  LabeledPoint(lp.label, Vectors.dense(lp.features.toArray.map { x => (x / 16).floor }))\n}\n// Create ChiSqSelector that will select top 50 of 692 features\nval selector = new ChiSqSelector(50)\n// Create ChiSqSelector model (selecting features)\nval transformer = selector.fit(discretizedData)\n// Filter the top 50 features from each feature vector\nval filteredData = discretizedData.map { lp =>\n  LabeledPoint(lp.label, transformer.transform(lp.features))\n}\n\nfilteredData.collect","dateUpdated":"2016-08-15T15:45:21+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471275162297_564851627","id":"20160815-153242_1180820365","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.mllib.feature.ChiSqSelector\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.util.MLUtils\ndata: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[1092] at map at MLUtils.scala:108\ndiscretizedData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[1093] at map at <console>:160\nselector: org.apache.spark.mllib.feature.ChiSqSelector = org.apache.spark.mllib.feature.ChiSqSelector@1f5ba210\ntransformer: org.apache.spark.mllib.feature.ChiSqSelectorModel = org.apache.spark.mllib.feature.ChiSqSelectorModel@419dfe11\nfilteredData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[1098] at map at <console>:166\nres55: Array[org.apache.spark.mllib.regression.LabeledPoint] = Array((0.0,[11.0,15.0,3.0,14.0,4.0,15.0,14.0,15.0,15.0,10.0,0.0,15.0,15.0,0.0,0.0,15.0,0.0,0.0,0.0,15.0,15.0,0.0,0.0,0.0,15.0,15.0,0.0,0.0,0.0,15.0,14.0,0.0,0.0,0.0,15.0,15.0,13.0,0.0,0.0,14.0,15.0,9.0,0.0,0.0,0.0,15.0,14.0,0.0,15.0,3.0]), (1.0,[13.0,1.0,0.0,0.0,5.0,0.0,0.0,0.0,0.0,0.0,15.0,0.0,0.0,12.0,15.0,0.0,6.0,15.0,15.0,0.0,0.0,15.0,15.0,12.0,0.0,0.0,15.0,15.0,9.0,0.0,0.0,15.0,15.0,2.0,0.0,0.0,0.0,15.0,10.0,0.0,0.0,0.0,12.0,0.0,0.0,0.0,0.0,5.0,0.0,0.0]), (1.0,[0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,15.0,0.0,0.0,15.0,15.0,0.0,15.0,15.0,15.0,0.0,0.0,15.0,15.0,15.0,0.0,0.0,15.0,15.0,15.0,0.0,0.0,15.0,15.0,15.0,0.0,0.0,0.0,15.0,15.0,0.0,0.0,0.0,9.0,15.0,0.0,0.0,0.0,6.0,0.0,2.0]), (1.0,[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,..."},"dateCreated":"2016-08-15T15:32:42+0000","dateStarted":"2016-08-15T15:36:35+0000","dateFinished":"2016-08-15T15:36:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:62"},{"title":"Elementwise Product","text":"%pyspark\n\nfrom pyspark.mllib.feature import ElementwiseProduct\nfrom pyspark.mllib.linalg import Vectors\n\ndata = sc.textFile(\"file:///usr/hdp/current/spark-client/data/mllib/kmeans_data.txt\")\nparsedData = data.map(lambda x: [float(t) for t in x.split(\" \")])\n\n# Create weight vector.\ntransformingVector = Vectors.dense([0.0, 1.0, 2.0])\ntransformer = ElementwiseProduct(transformingVector)\n\n# Batch transform\ntransformedData = transformer.transform(parsedData)\n# Single-row transform\ntransformedData2 = transformer.transform(parsedData.first())\n\nprint transformedData.collect()","dateUpdated":"2016-08-15T15:41:34+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471275258324_-215465115","id":"20160815-153418_131312622","result":{"code":"SUCCESS","type":"TEXT","msg":"[DenseVector([0.0, 0.0, 0.0]), DenseVector([0.0, 0.1, 0.2]), DenseVector([0.0, 0.2, 0.4]), DenseVector([0.0, 9.0, 18.0]), DenseVector([0.0, 9.1, 18.2]), DenseVector([0.0, 9.2, 18.4])]\n"},"dateCreated":"2016-08-15T15:34:18+0000","dateStarted":"2016-08-15T15:41:13+0000","dateFinished":"2016-08-15T15:41:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:63"},{"title":"PCA","text":"%scala\n\nimport org.apache.spark.mllib.feature.PCA\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.{LabeledPoint, LinearRegressionWithSGD}\n\nval data = sc.textFile(\"data/mllib/ridge-data/lpsa.data\").map { line =>\n  val parts = line.split(',')\n  LabeledPoint(parts(0).toDouble, Vectors.dense(parts(1).split(' ').map(_.toDouble)))\n}.cache()\n\nval splits = data.randomSplit(Array(0.6, 0.4), seed = 11L)\nval training = splits(0).cache()\nval test = splits(1)\n\nval pca = new PCA(training.first().features.size / 2).fit(data.map(_.features))\nval training_pca = training.map(p => p.copy(features = pca.transform(p.features)))\nval test_pca = test.map(p => p.copy(features = pca.transform(p.features)))\n\nval numIterations = 100\nval model = LinearRegressionWithSGD.train(training, numIterations)\nval model_pca = LinearRegressionWithSGD.train(training_pca, numIterations)\n\nval valuesAndPreds = test.map { point =>\n  val score = model.predict(point.features)\n  (score, point.label)\n}\n\nval valuesAndPreds_pca = test_pca.map { point =>\n  val score = model_pca.predict(point.features)\n  (score, point.label)\n}\n\nval MSE = valuesAndPreds.map { case (v, p) => math.pow((v - p), 2) }.mean()\nval MSE_pca = valuesAndPreds_pca.map { case (v, p) => math.pow((v - p), 2) }.mean()\n\nprintln(\"Mean Squared Error = \" + MSE)\nprintln(\"PCA Mean Squared Error = \" + MSE_pca)\n","dateUpdated":"2016-08-15T15:42:17+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471275604783_-979903171","id":"20160815-154004_670922487","result":{"code":"ERROR","type":"TEXT","msg":"Prefix not found."},"dateCreated":"2016-08-15T15:40:04+0000","dateStarted":"2016-08-15T15:42:17+0000","dateFinished":"2016-08-15T15:42:21+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:64"},{"text":"%scala ","dateUpdated":"2016-08-15T15:42:17+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1471275737449_1157608476","id":"20160815-154217_297496268","dateCreated":"2016-08-15T15:42:17+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:65"}],"name":"Spark MLlib API - Part 4","id":"2BSDRUCHV","lastReplName":{"value":"spark"},"angularObjects":{"2BRHYDE45:shared_process":[],"2BQVH8HWF:shared_process":[],"2BTZ65EZ1:shared_process":[],"2BSTTAPCN:shared_process":[],"2BU44EEGW:shared_process":[],"2BSJHPYU8:shared_process":[],"2BTEBDBMC:shared_process":[],"2BQNEXNZ9:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}