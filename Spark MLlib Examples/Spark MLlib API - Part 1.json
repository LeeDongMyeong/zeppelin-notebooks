{"paragraphs":[{"text":"%md\n\n# MLlib API Guide Overview\n\nThis notebook copies all the [Spark MLlib sample code](http://spark.apache.org/docs/latest/mllib-data-types.html) from the official Spark docs for you to try in Zeppelin environment. \n\nNote: Some code samples were updated to remove dependancies on NumPy and SciPy.\n\nLanguage: Mostly Python; Scala where Python API has not yet been implemented.\nLast updated: Aug 10, 2016\n\nAuthor: Robert Hryniewicz\nTwitter: @RobHryniewicz\n\nTopics:\n- Data Types\n- Basic Statistics","dateUpdated":"2016-08-15T04:21:30+0000","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":true,"colWidth":12,"title":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1470855246137_-1514147460","id":"20160531-234527_1318957937","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>MLlib API Guide Overview</h1>\n<p>This notebook copies all the <a href=\"http://spark.apache.org/docs/latest/mllib-data-types.html\">Spark MLlib sample code</a> from the official Spark docs for you to try in Zeppelin environment.</p>\n<p>Note: Some code samples were updated to remove dependancies on NumPy and SciPy.</p>\n<p>Language: Mostly Python; Scala where Python API has not yet been implemented.\n<br  />Last updated: Aug 10, 2016</p>\n<p>Author: Robert Hryniewicz\n<br  />Twitter: @RobHryniewicz</p>\n<p>Topics:</p>\n<ul>\n<li>Data Types</li>\n<li>Basic Statistics</li>\n</ul>\n"},"dateCreated":"2016-08-10T06:54:06+0000","dateStarted":"2016-08-15T04:21:05+0000","dateFinished":"2016-08-15T04:21:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:53"},{"title":"Lab Pre-Check","text":"%md\n\nBefore we proceed, let's verify your Spark Version. You should be running at minimum Spark 1.6.\n#\n**Note**: The first time you run `sc.version` in the paragraph below, several services will initialize in the background. This may take **1~2 min** so please **be patient**. Afterwards, each paragraph should run much more quickly since all the services will already be running in the background.","dateUpdated":"2016-08-15T04:23:19+0000","config":{"enabled":true,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/scala","editorHide":true,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1470855246137_-1514147460","id":"20160531-234527_478858631","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Before we proceed, let's verify your Spark Version. You should be running at minimum Spark 1.6.</p>\n<h1></h1>\n<p><strong>Note</strong>: The first time you run <code>sc.version</code> in the paragraph below, several services will initialize in the background. This may take <strong>1~2 min</strong> so please <strong>be patient</strong>. Afterwards, each paragraph should run much more quickly since all the services will already be running in the background.</p>\n"},"dateCreated":"2016-08-10T06:54:06+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:54"},{"text":"%md\nTo run a paragraph in a Zeppelin notebook you can either click the `play` button (blue triangle) on the right-hand side or simply press `Shift + Enter`.","dateUpdated":"2016-08-15T04:23:37+0000","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/markdown","editorHide":false,"colWidth":12},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1470855246137_-1514147460","id":"20160531-234527_1555785908","result":{"code":"SUCCESS","type":"HTML","msg":"<p>To run a paragraph in a Zeppelin notebook you can either click the <code>play</code> button (blue triangle) on the right-hand side or simply press <code>Shift + Enter</code>.</p>\n"},"dateCreated":"2016-08-10T06:54:06+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:55"},{"title":"Check Spark Version","text":"%pyspark\nprint sc.version","dateUpdated":"2016-08-11T12:29:38+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1470875247106_-1503876294","id":"20160811-002727_159758674","result":{"code":"SUCCESS","type":"TEXT","msg":"1.6.2\n"},"dateCreated":"2016-08-11T12:27:27+0000","dateStarted":"2016-08-11T12:29:38+0000","dateFinished":"2016-08-11T12:29:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56"},{"text":"%md \n\n# Data Types","dateUpdated":"2016-08-10T07:19:49+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1470856775161_-152330111","id":"20160810-191935_1315335690","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Data Types</h1>\n"},"dateCreated":"2016-08-10T07:19:35+0000","dateStarted":"2016-08-10T07:19:47+0000","dateFinished":"2016-08-10T07:19:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:57"},{"title":"Local Vector","text":"%pyspark\n\nfrom pyspark.mllib.linalg import Vectors\n\n# Use a Python list as a dense vector.\ndv2 = [1.0, 0.0, 3.0]\n\n# Create a SparseVector.\nsv1 = Vectors.sparse(3, [0, 2], [1.0, 3.0])\n\nprint \"Dense:\" + str(dv2)\nprint \"Sparse:\" + str(sv1)","dateUpdated":"2016-08-10T07:20:59+0000","config":{"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"editorMode":"ace/mode/python","colWidth":12,"title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1470855246139_-1513377962","id":"20160531-234527_1909965823","result":{"code":"SUCCESS","type":"TEXT","msg":"Dense:[1.0, 0.0, 3.0]\nSparse:(3,[0,2],[1.0,3.0])\n"},"dateCreated":"2016-08-10T06:54:06+0000","dateStarted":"2016-08-10T07:15:43+0000","dateFinished":"2016-08-10T07:15:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:58"},{"title":"Labeled Point","text":"%pyspark \n\nfrom pyspark.mllib.linalg import SparseVector\nfrom pyspark.mllib.regression import LabeledPoint\n\n# Create a labeled point with a positive label and a dense feature vector.\npos = LabeledPoint(1.0, [1.0, 0.0, 3.0])\n\n# Create a labeled point with a negative label and a sparse feature vector.\nneg = LabeledPoint(0.0, SparseVector(3, [0, 2], [1.0, 3.0]))\n\nprint \"Positive Label & Dense Vector: \" + str(pos)\nprint \"Negative Label & Sparse Vector: \" + str(neg)","dateUpdated":"2016-08-10T07:33:24+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1470855600504_-1638101715","id":"20160810-190000_1588685908","result":{"code":"SUCCESS","type":"TEXT","msg":"Positive Label & Dense Vector: (1.0,[1.0,0.0,3.0])\nNegative Label & Sparse Vector: (0.0,(3,[0,2],[1.0,3.0]))\n"},"dateCreated":"2016-08-10T07:00:00+0000","dateStarted":"2016-08-10T07:29:41+0000","dateFinished":"2016-08-10T07:29:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:59"},{"title":"Load File in LIBSVM Format","text":"%pyspark \n\nfrom pyspark.mllib.util import MLUtils\n\nexamples = MLUtils.loadLibSVMFile(sc, \"file:///usr/hdp/current/spark-client/data/mllib/sample_libsvm_data.txt\")\n\nprint examples.take(1)","dateUpdated":"2016-08-10T07:42:31+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1470857108745_-437168535","id":"20160810-192508_1325471302","result":{"code":"SUCCESS","type":"TEXT","msg":"[LabeledPoint(0.0, (692,[127,128,129,130,131,154,155,156,157,158,159,181,182,183,184,185,186,187,188,189,207,208,209,210,211,212,213,214,215,216,217,235,236,237,238,239,240,241,242,243,244,245,262,263,264,265,266,267,268,269,270,271,272,273,289,290,291,292,293,294,295,296,297,300,301,302,316,317,318,319,320,321,328,329,330,343,344,345,346,347,348,349,356,357,358,371,372,373,374,384,385,386,399,400,401,412,413,414,426,427,428,429,440,441,442,454,455,456,457,466,467,468,469,470,482,483,484,493,494,495,496,497,510,511,512,520,521,522,523,538,539,540,547,548,549,550,566,567,568,569,570,571,572,573,574,575,576,577,578,594,595,596,597,598,599,600,601,602,603,604,622,623,624,625,626,627,628,629,630,651,652,653,654,655,656,657],[51.0,159.0,253.0,159.0,50.0,48.0,238.0,252.0,252.0,252.0,237.0,54.0,227.0,253.0,252.0,239.0,233.0,252.0,57.0,6.0,10.0,60.0,224.0,252.0,253.0,252.0,202.0,84.0,252.0,253.0,122.0,163.0,252.0,252.0,252.0,253.0,252.0,252.0,96.0,189.0,253.0,167.0,51.0,238.0,253.0,253.0,190.0,114.0,253.0,228.0,47.0,79.0,255.0,168.0,48.0,238.0,252.0,252.0,179.0,12.0,75.0,121.0,21.0,253.0,243.0,50.0,38.0,165.0,253.0,233.0,208.0,84.0,253.0,252.0,165.0,7.0,178.0,252.0,240.0,71.0,19.0,28.0,253.0,252.0,195.0,57.0,252.0,252.0,63.0,253.0,252.0,195.0,198.0,253.0,190.0,255.0,253.0,196.0,76.0,246.0,252.0,112.0,253.0,252.0,148.0,85.0,252.0,230.0,25.0,7.0,135.0,253.0,186.0,12.0,85.0,252.0,223.0,7.0,131.0,252.0,225.0,71.0,85.0,252.0,145.0,48.0,165.0,252.0,173.0,86.0,253.0,225.0,114.0,238.0,253.0,162.0,85.0,252.0,249.0,146.0,48.0,29.0,85.0,178.0,225.0,253.0,223.0,167.0,56.0,85.0,252.0,252.0,252.0,229.0,215.0,252.0,252.0,252.0,196.0,130.0,28.0,199.0,252.0,252.0,253.0,252.0,252.0,233.0,145.0,25.0,128.0,252.0,253.0,252.0,141.0,37.0]))]\n"},"dateCreated":"2016-08-10T07:25:08+0000","dateStarted":"2016-08-10T07:42:31+0000","dateFinished":"2016-08-10T07:42:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:60"},{"title":"Local Matrix","text":"%pyspark \n\nfrom pyspark.mllib.linalg import Matrix, Matrices\n\n# Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\ndm2 = Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])\n\n# Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))\nsm = Matrices.sparse(3, 2, [0, 1, 3], [0, 2, 1], [9, 6, 8])\n\nprint \"Dense Matrix\"\nprint \"-------------\"\nprint dm2\nprint \"Sparse Matrix\"\nprint \"-------------\"\nprint sm","dateUpdated":"2016-08-10T07:46:44+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1470857683419_-211265091","id":"20160810-193443_1121734899","result":{"code":"SUCCESS","type":"TEXT","msg":"Dense Matrix\n-------------\nDenseMatrix([[ 1.,  4.],\n             [ 2.,  5.],\n             [ 3.,  6.]])\nSparse Matrix\n-------------\n3 X 2 CSCMatrix\n(0,0) 9.0\n(2,1) 6.0\n(1,1) 8.0\n"},"dateCreated":"2016-08-10T07:34:43+0000","dateStarted":"2016-08-10T07:46:44+0000","dateFinished":"2016-08-10T07:46:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:61"},{"title":"Row Matrix","text":"%pyspark \n\nfrom pyspark.mllib.linalg.distributed import RowMatrix\n\n# Create an RDD of vectors.\nrows = sc.parallelize([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n\n# Create a RowMatrix from an RDD of vectors.\nmat = RowMatrix(rows)\n\n# Get its size.\nm = mat.numRows()  # 4\nn = mat.numCols()  # 3\n\n# Get the rows as an RDD of vectors again.\nrowsRDD = mat.rows\n\nprint \"Mat Size: \" + str(m) + \"x\" + str(n)\nprint \"Row Matrix: \" + str(rowsRDD.collect())","dateUpdated":"2016-08-10T10:20:16+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1470858235503_-903446975","id":"20160810-194355_1481927904","result":{"code":"SUCCESS","type":"TEXT","msg":"Mat Size: 4x3\nRow Matrix: [DenseVector([1.0, 2.0, 3.0]), DenseVector([4.0, 5.0, 6.0]), DenseVector([7.0, 8.0, 9.0]), DenseVector([10.0, 11.0, 12.0])]\n"},"dateCreated":"2016-08-10T07:43:55+0000","dateStarted":"2016-08-10T10:20:16+0000","dateFinished":"2016-08-10T10:20:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:62"},{"title":"Indexed Row Matrix","text":"%pyspark \n\nfrom pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n\n# Create an RDD of indexed rows.\n#   - This can be done explicitly with the IndexedRow class:\nindexedRows = sc.parallelize([IndexedRow(0, [1, 2, 3]), \n                              IndexedRow(1, [4, 5, 6]), \n                              IndexedRow(2, [7, 8, 9]), \n                              IndexedRow(3, [10, 11, 12])])\n#   - or by using (long, vector) tuples:\nindexedRows = sc.parallelize([(0, [1, 2, 3]), (1, [4, 5, 6]), \n                              (2, [7, 8, 9]), (3, [10, 11, 12])])\n\n# Create an IndexedRowMatrix from an RDD of IndexedRows.\nmat = IndexedRowMatrix(indexedRows)\n\n# Get its size.\nm = mat.numRows()  # 4\nn = mat.numCols()  # 3\n\n# Get the rows as an RDD of IndexedRows.\nrowsRDD = mat.rows\n\n# Convert to a RowMatrix by dropping the row indices.\nrowMat = mat.toRowMatrix()\n\nprint \"Indexed Rows: \" + str(rowsRDD.collect())\n#print \"Row Matrix: \" + str(rowMat.rows.collect())","dateUpdated":"2016-08-10T10:19:48+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1470860998387_2061046221","id":"20160810-202958_1926945446","result":{"code":"SUCCESS","type":"TEXT","msg":"Indexed Rows: [IndexedRow(0, [1.0,2.0,3.0]), IndexedRow(1, [4.0,5.0,6.0]), IndexedRow(2, [7.0,8.0,9.0]), IndexedRow(3, [10.0,11.0,12.0])]\n"},"dateCreated":"2016-08-10T08:29:58+0000","dateStarted":"2016-08-10T10:19:48+0000","dateFinished":"2016-08-10T10:19:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:63"},{"title":"Coordinate Matrix","text":"%pyspark \n\nfrom pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry\n\n# Create an RDD of coordinate entries.\n#   - This can be done explicitly with the MatrixEntry class:\nentries = sc.parallelize([MatrixEntry(0, 0, 1.2), MatrixEntry(1, 0, 2.1), MatrixEntry(6, 1, 3.7)])\n#   - or using (long, long, float) tuples:\nentries = sc.parallelize([(0, 0, 1.2), (1, 0, 2.1), (2, 1, 3.7)])\n\n# Create an CoordinateMatrix from an RDD of MatrixEntries.\nmat = CoordinateMatrix(entries)\n\n# Get its size.\nm = mat.numRows()  # 3\nn = mat.numCols()  # 2\n\n# Get the entries as an RDD of MatrixEntries.\nentriesRDD = mat.entries\n\n# Convert to a RowMatrix.\nrowMat = mat.toRowMatrix()\n\n# Convert to an IndexedRowMatrix.\nindexedRowMat = mat.toIndexedRowMatrix()\n\n# Convert to a BlockMatrix.\nblockMat = mat.toBlockMatrix()\n\nprint \"Coordinate Matrix Entries: \" + str(entriesRDD.collect())","dateUpdated":"2016-08-10T10:13:11+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1470861207314_1337423840","id":"20160810-203327_792497940","result":{"code":"SUCCESS","type":"TEXT","msg":"Coordinate Matrix Entries: [MatrixEntry(0, 0, 1.2), MatrixEntry(1, 0, 2.1), MatrixEntry(2, 1, 3.7)]\n"},"dateCreated":"2016-08-10T08:33:27+0000","dateStarted":"2016-08-10T10:13:11+0000","dateFinished":"2016-08-10T10:13:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:64"},{"title":"Block Matrix","text":"%pyspark \n\nfrom pyspark.mllib.linalg import Matrices\nfrom pyspark.mllib.linalg.distributed import BlockMatrix\n\n# Create an RDD of sub-matrix blocks.\nblocks = sc.parallelize([((0, 0), Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])), \n                         ((1, 0), Matrices.dense(3, 2, [7, 8, 9, 10, 11, 12]))])\n\n# Create a BlockMatrix from an RDD of sub-matrix blocks.\nmat = BlockMatrix(blocks, 3, 2)\n\n# Get its size.\nm = mat.numRows() # 6\nn = mat.numCols() # 2\n\n# Get the blocks as an RDD of sub-matrix blocks.\nblocksRDD = mat.blocks\n\n# Convert to a LocalMatrix.\nlocalMat = mat.toLocalMatrix()\n\n# Convert to an IndexedRowMatrix.\nindexedRowMat = mat.toIndexedRowMatrix()\n\n# Convert to a CoordinateMatrix.\ncoordinateMat = mat.toCoordinateMatrix()\n\nprint \"Block Matrix Blocks: \" + str(blocksRDD.collect())","dateUpdated":"2016-08-10T10:19:28+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1470867143050_-318260188","id":"20160810-221223_214324997","result":{"code":"SUCCESS","type":"TEXT","msg":"Block Matrix Blocks: [((0, 0), DenseMatrix(3, 2, [1.0, 2.0, 3.0, 4.0, 5.0, 6.0], 0)), ((1, 0), DenseMatrix(3, 2, [7.0, 8.0, 9.0, 10.0, 11.0, 12.0], 0))]\n"},"dateCreated":"2016-08-10T10:12:23+0000","dateStarted":"2016-08-10T10:19:28+0000","dateFinished":"2016-08-10T10:19:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:65"},{"text":"%md\n\n# Basic Statistics","dateUpdated":"2016-08-10T10:31:58+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1470867628886_-1968744313","id":"20160810-222028_1702376093","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Basic Statistics</h1>\n"},"dateCreated":"2016-08-10T10:20:28+0000","dateStarted":"2016-08-10T10:31:55+0000","dateFinished":"2016-08-10T10:31:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:66"},{"title":"Summary Statistics","text":"%pyspark \n\nfrom pyspark.mllib.stat import Statistics\n\nmat = sc.parallelize(\n    [[1.0, 10.0, 100.0], \n     [2.0, 20.0, 200.0], \n     [3.0, 30.0, 300.0]]\n)  # an RDD of Vectors\n\n# Compute column summary statistics.\nsummary = Statistics.colStats(mat)\nprint(summary.mean())  # a dense vector containing the mean value for each column\nprint(summary.variance())  # column-wise variance\nprint(summary.numNonzeros())  # number of nonzeros in each column","dateUpdated":"2016-08-10T10:43:40+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1470867355250_-1106905133","id":"20160810-221555_1751817973","result":{"code":"SUCCESS","type":"TEXT","msg":"[   2.   20.  200.]\n[  1.00000000e+00   1.00000000e+02   1.00000000e+04]\n[ 3.  3.  3.]\n"},"dateCreated":"2016-08-10T10:15:55+0000","dateStarted":"2016-08-10T10:43:40+0000","dateFinished":"2016-08-10T10:43:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:67"},{"title":"Correlations","text":"%pyspark \n\nfrom pyspark.mllib.stat import Statistics\n\nseriesX = sc.parallelize([1.0, 2.0, 3.0, 3.0, 5.0])  # a series\n# seriesY must have the same number of partitions and cardinality as seriesX\nseriesY = sc.parallelize([11.0, 22.0, 33.0, 33.0, 555.0])\n\n# Compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method.\n# If a method is not specified, Pearson's method will be used by default.\nprint(\"Pearson's Correlation is: \" + str(Statistics.corr(seriesX, seriesY, method=\"pearson\")))\nprint(\"Spearman's Correlation is: \" + str(Statistics.corr(seriesX, seriesY, method=\"spearman\")))\n\ndata = sc.parallelize(\n    [np.array([1.0, 10.0, 100.0]), np.array([2.0, 20.0, 200.0]), np.array([5.0, 33.0, 366.0])]\n)  # an RDD of Vectors\n\n# calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method.\n# If a method is not specified, Pearson's method will be used by default.\nprint \"Pearson's Correlation Matrix\"\nprint (Statistics.corr(data, method=\"pearson\"))\nprint \"Spearman's Correlation Matrix\"\nprint (Statistics.corr(data, method=\"spearman\"))","dateUpdated":"2016-08-11T12:30:51+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1470868946718_1953852278","id":"20160810-224226_876814551","result":{"code":"SUCCESS","type":"TEXT","msg":"Pearson's Correlation is: 0.850028676877\nSpearman's Correlation is: 1.0\nPearson's Correlation Matrix\n[[ 1.          0.97888347  0.99038957]\n [ 0.97888347  1.          0.99774832]\n [ 0.99038957  0.99774832  1.        ]]\nSpearman's Correlation Matrix\n[[ 1.  1.  1.]\n [ 1.  1.  1.]\n [ 1.  1.  1.]]\n"},"dateCreated":"2016-08-10T10:42:26+0000","dateStarted":"2016-08-10T10:47:52+0000","dateFinished":"2016-08-10T10:47:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:68"},{"title":"Stratified Sampling","text":"%pyspark \n\n# Note: sampleByKeyExact is currently not supported in python\n# Note: sampleByKeyExact requires significant more resources than the per-stratum simple random sampling used in sampleByKey, but will provide the exact sampling size with 99.99% confidence. \n\n# an RDD of any key value pairs\ndata = sc.parallelize([(1, 'a'), (1, 'b'), (2, 'c'), (2, 'd'), (2, 'e'), (3, 'f')])\n\n# specify the exact fraction desired from each key as a dictionary\nfractions = {1: 0.1, 2: 0.6, 3: 0.3}\n\napproxSample = data.sampleByKey(False, fractions)\n\nprint approxSample.collect()","dateUpdated":"2016-08-10T10:54:28+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1470869068835_506366142","id":"20160810-224428_464108188","result":{"code":"SUCCESS","type":"TEXT","msg":"[(1, 'b'), (2, 'c'), (2, 'd')]\n"},"dateCreated":"2016-08-10T10:44:28+0000","dateStarted":"2016-08-10T10:53:11+0000","dateFinished":"2016-08-10T10:53:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:69"},{"title":"Hypothesis Testing","text":"%pyspark \n\nfrom pyspark.mllib.linalg import Matrices, Vectors\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.stat import Statistics\n\nvec = Vectors.dense(0.1, 0.15, 0.2, 0.3, 0.25)  # a vector composed of the frequencies of events\n\n# compute the goodness of fit. If a second vector to test against\n# is not supplied as a parameter, the test runs against a uniform distribution.\ngoodnessOfFitTestResult = Statistics.chiSqTest(vec)\n\n# summary of the test including the p-value, degrees of freedom,\n# test statistic, the method used, and the null hypothesis.\nprint(\"%s\\n\" % goodnessOfFitTestResult)\n\nmat = Matrices.dense(3, 2, [1.0, 3.0, 5.0, 2.0, 4.0, 6.0])  # a contingency matrix\n\n# conduct Pearson's independence test on the input contingency matrix\nindependenceTestResult = Statistics.chiSqTest(mat)\n\n# summary of the test including the p-value, degrees of freedom,\n# test statistic, the method used, and the null hypothesis.\nprint(\"%s\\n\" % independenceTestResult)\n\nobs = sc.parallelize(\n    [LabeledPoint(1.0, [1.0, 0.0, 3.0]),\n     LabeledPoint(1.0, [1.0, 2.0, 0.0]),\n     LabeledPoint(1.0, [-1.0, 0.0, -0.5])]\n)  # LabeledPoint(feature, label)\n\n# The contingency table is constructed from an RDD of LabeledPoint and used to conduct\n# the independence test. Returns an array containing the ChiSquaredTestResult for every feature\n# against the label.\nfeatureTestResults = Statistics.chiSqTest(obs)\n\nfor i, result in enumerate(featureTestResults):\n    print(\"Column %d:\\n%s\" % (i + 1, result))","dateUpdated":"2016-08-10T11:04:08+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1470869567173_565922312","id":"20160810-225247_1470879811","result":{"code":"SUCCESS","type":"TEXT","msg":"Chi squared test summary:\nmethod: pearson\ndegrees of freedom = 4 \nstatistic = 0.12499999999999999 \npValue = 0.998126379239318 \nNo presumption against null hypothesis: observed follows the same distribution as expected..\n\nChi squared test summary:\nmethod: pearson\ndegrees of freedom = 2 \nstatistic = 0.14141414141414144 \npValue = 0.931734784568187 \nNo presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n\nColumn 1:\nChi squared test summary:\nmethod: pearson\ndegrees of freedom = 0 \nstatistic = 0.0 \npValue = 1.0 \nNo presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\nColumn 2:\nChi squared test summary:\nmethod: pearson\ndegrees of freedom = 0 \nstatistic = 0.0 \npValue = 1.0 \nNo presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\nColumn 3:\nChi squared test summary:\nmethod: pearson\ndegrees of freedom = 0 \nstatistic = 0.0 \npValue = 1.0 \nNo presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n"},"dateCreated":"2016-08-10T10:52:47+0000","dateStarted":"2016-08-10T11:03:44+0000","dateFinished":"2016-08-10T11:03:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:70"},{"title":"Kolmogorov-Smirnov (KS) Test","text":"%pyspark \n\nfrom pyspark.mllib.stat import Statistics\n\nparallelData = sc.parallelize([0.1, 0.15, 0.2, 0.3, 0.25])\n\n# run a KS test for the sample versus a standard normal distribution\ntestResult = Statistics.kolmogorovSmirnovTest(parallelData, \"norm\", 0, 1)\n# summary of the test including the p-value, test statistic, and null hypothesis\n# if our p-value indicates significance, we can reject the null hypothesis\n# Note that the Scala functionality of calling Statistics.kolmogorovSmirnovTest with\n# a lambda to calculate the CDF is not made available in the Python API\nprint(testResult)","dateUpdated":"2016-08-10T11:30:15+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1470870224634_-195147118","id":"20160810-230344_1405052648","result":{"code":"SUCCESS","type":"TEXT","msg":"Kolmogorov-Smirnov test summary:\ndegrees of freedom = 0 \nstatistic = 0.539827837277029 \npValue = 0.06821463111921133 \nLow presumption against null hypothesis: Sample follows theoretical distribution.\n"},"dateCreated":"2016-08-10T11:03:44+0000","dateStarted":"2016-08-10T11:16:07+0000","dateFinished":"2016-08-10T11:16:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:71"},{"title":"Streaming Significance Testing (Scala)","text":"%spark\n\n// Useful for A/B testing\n\n// Note: this code is incomplete for reference only. Running it has been disabled.\nval data = ssc.textFileStream(dataDir).map(line => line.split(\",\") match {\n  case Array(label, value) => BinarySample(label.toBoolean, value.toDouble)\n})\n\nval streamingTest = new StreamingTest()\n  .setPeacePeriod(0)\n  .setWindowSize(0)\n  .setTestMethod(\"welch\")\n\nval out = streamingTest.registerStream(data)\nout.print()","dateUpdated":"2016-08-11T12:39:01+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":false,"title":true,"editorMode":"ace/mode/scala","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1470870967178_1593222133","id":"20160810-231607_1498045638","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2016-08-10T11:16:07+0000","dateStarted":"2016-08-11T12:36:05+0000","dateFinished":"2016-08-11T12:36:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:72"},{"title":"Random Data Generator","text":"%pyspark\n\nfrom pyspark.mllib.random import RandomRDDs\n\n# Generate a random double RDD that contains 1 million i.i.d. values drawn from the\n# standard normal distribution `N(0, 1)`, evenly distributed in 10 partitions.\nu = RandomRDDs.normalRDD(sc, 1000000L, 10)\n\n# Apply a transform to get a random double RDD following `N(1, 4)`.\nv = u.map(lambda x: 1.0 + 2.0 * x)\n\nprint v.take(10)","dateUpdated":"2016-08-10T11:37:23+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1470871887331_-314055907","id":"20160810-233127_642712099","result":{"code":"SUCCESS","type":"TEXT","msg":"[0.87249533507287769, 1.5205541777390761, 2.1514881352838127, 3.6449335256351656, 1.3562778992383677, -0.70434550379635019, 0.54858730353707696, 2.6485665249064514, 0.22569709826581996, 5.1064061807617041]\n"},"dateCreated":"2016-08-10T11:31:27+0000","dateStarted":"2016-08-10T11:37:24+0000","dateFinished":"2016-08-10T11:37:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:73"},{"title":"Kernel Density Estimation","text":"%pyspark \n\nfrom pyspark.mllib.stat import KernelDensity\n\n# an RDD of sample data\ndata = sc.parallelize([1.0, 1.0, 1.0, 2.0, 3.0, 4.0, 5.0, 5.0, 6.0, 7.0, 8.0, 9.0, 9.0])\n\n# Construct the density estimator with the sample data and a standard deviation for the Gaussian\n# kernels\nkd = KernelDensity()\nkd.setSample(data)\nkd.setBandwidth(3.0)\n\n# Find density estimates for the given values\ndensities = kd.estimate([-1.0, 2.0, 5.0])\n\nprint densities\n","dateUpdated":"2016-08-10T11:45:44+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1470872206984_-487169365","id":"20160810-233646_1662034195","result":{"code":"SUCCESS","type":"TEXT","msg":"[ 0.04145944  0.07902017  0.0896292 ]\n"},"dateCreated":"2016-08-10T11:36:46+0000","dateStarted":"2016-08-10T11:45:44+0000","dateFinished":"2016-08-10T11:45:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:74"},{"text":"%pyspark ","dateUpdated":"2016-08-10T11:38:53+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1470872333121_1543366764","id":"20160810-233853_734057438","dateCreated":"2016-08-10T11:38:53+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:75"}],"name":"Spark MLlib API - Part 1","id":"2BTQNZGA3","lastReplName":{"value":"md"},"angularObjects":{"2BRHYDE45:shared_process":[],"2BQVH8HWF:shared_process":[],"2BTZ65EZ1:shared_process":[],"2BSTTAPCN:shared_process":[],"2BU44EEGW:shared_process":[],"2BSJHPYU8:shared_process":[],"2BTEBDBMC:shared_process":[],"2BQNEXNZ9:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}